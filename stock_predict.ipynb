{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "做未来做预测\n",
    "\n",
    "现在比较流行的方式是预测股价，我觉得不是很靠谱。\n",
    "\n",
    "## 我想到的一个是找最低点。\n",
    "这样的话，问题就由一个回归问题变为一个分类问题。  \n",
    "如以过去N天的(股价，成交量，时间（季度、月份），其它股票、指数、汇率、油价，等等)组成一个特征序列，来预测次日的股价是否是最低点。\n",
    "最低点的判断依据：未来T天的股价收盘价均不小于预测序列最后一日的收盘价，且未来T天的收盘价最高点大于预测序列最后一日的收盘价的Z%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tushare as ts\n",
    "\n",
    "ts.set_token('5fd1639100f8a22b7f86e882e03192009faa72bae1ae93803e1172d5')\n",
    "\n",
    "pro = ts.pro_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pro.index_daily(ts_code='000001.SH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_code</th>\n",
       "      <th>trade_date</th>\n",
       "      <th>close</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>pre_close</th>\n",
       "      <th>change</th>\n",
       "      <th>pct_chg</th>\n",
       "      <th>vol</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181128</td>\n",
       "      <td>2601.7365</td>\n",
       "      <td>2575.4541</td>\n",
       "      <td>2601.9626</td>\n",
       "      <td>2561.5618</td>\n",
       "      <td>2574.6792</td>\n",
       "      <td>27.0573</td>\n",
       "      <td>1.0509</td>\n",
       "      <td>145963473.0</td>\n",
       "      <td>119578491.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181127</td>\n",
       "      <td>2574.6792</td>\n",
       "      <td>2585.8261</td>\n",
       "      <td>2592.5353</td>\n",
       "      <td>2566.1663</td>\n",
       "      <td>2575.8101</td>\n",
       "      <td>-1.1309</td>\n",
       "      <td>-0.0439</td>\n",
       "      <td>123658436.0</td>\n",
       "      <td>102226561.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181126</td>\n",
       "      <td>2575.8101</td>\n",
       "      <td>2580.8424</td>\n",
       "      <td>2594.9966</td>\n",
       "      <td>2568.0352</td>\n",
       "      <td>2579.4831</td>\n",
       "      <td>-3.6730</td>\n",
       "      <td>-0.1424</td>\n",
       "      <td>134314540.0</td>\n",
       "      <td>108158093.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181123</td>\n",
       "      <td>2579.4831</td>\n",
       "      <td>2640.6674</td>\n",
       "      <td>2642.0356</td>\n",
       "      <td>2577.3511</td>\n",
       "      <td>2645.4339</td>\n",
       "      <td>-65.9508</td>\n",
       "      <td>-2.4930</td>\n",
       "      <td>191474549.0</td>\n",
       "      <td>149975643.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181122</td>\n",
       "      <td>2645.4339</td>\n",
       "      <td>2655.8964</td>\n",
       "      <td>2658.0011</td>\n",
       "      <td>2634.4827</td>\n",
       "      <td>2651.5052</td>\n",
       "      <td>-6.0713</td>\n",
       "      <td>-0.2290</td>\n",
       "      <td>149309063.0</td>\n",
       "      <td>125570173.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ts_code trade_date      close       open       high        low  \\\n",
       "0  000001.SH   20181128  2601.7365  2575.4541  2601.9626  2561.5618   \n",
       "1  000001.SH   20181127  2574.6792  2585.8261  2592.5353  2566.1663   \n",
       "2  000001.SH   20181126  2575.8101  2580.8424  2594.9966  2568.0352   \n",
       "3  000001.SH   20181123  2579.4831  2640.6674  2642.0356  2577.3511   \n",
       "4  000001.SH   20181122  2645.4339  2655.8964  2658.0011  2634.4827   \n",
       "\n",
       "   pre_close   change  pct_chg          vol       amount  \n",
       "0  2574.6792  27.0573   1.0509  145963473.0  119578491.7  \n",
       "1  2575.8101  -1.1309  -0.0439  123658436.0  102226561.4  \n",
       "2  2579.4831  -3.6730  -0.1424  134314540.0  108158093.0  \n",
       "3  2645.4339 -65.9508  -2.4930  191474549.0  149975643.9  \n",
       "4  2651.5052  -6.0713  -0.2290  149309063.0  125570173.8  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_code</th>\n",
       "      <th>trade_date</th>\n",
       "      <th>close</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>pre_close</th>\n",
       "      <th>change</th>\n",
       "      <th>pct_chg</th>\n",
       "      <th>vol</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20110713</td>\n",
       "      <td>2795.476</td>\n",
       "      <td>2756.032</td>\n",
       "      <td>2797.266</td>\n",
       "      <td>2756.032</td>\n",
       "      <td>2754.582</td>\n",
       "      <td>40.894</td>\n",
       "      <td>1.4846</td>\n",
       "      <td>91929128.0</td>\n",
       "      <td>1.076960e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20110712</td>\n",
       "      <td>2754.582</td>\n",
       "      <td>2783.722</td>\n",
       "      <td>2783.722</td>\n",
       "      <td>2752.186</td>\n",
       "      <td>2802.692</td>\n",
       "      <td>-48.110</td>\n",
       "      <td>-1.7166</td>\n",
       "      <td>96674475.0</td>\n",
       "      <td>1.115352e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20110711</td>\n",
       "      <td>2802.692</td>\n",
       "      <td>2789.454</td>\n",
       "      <td>2807.322</td>\n",
       "      <td>2780.665</td>\n",
       "      <td>2797.774</td>\n",
       "      <td>4.918</td>\n",
       "      <td>0.1758</td>\n",
       "      <td>86063595.0</td>\n",
       "      <td>1.014421e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20110708</td>\n",
       "      <td>2797.774</td>\n",
       "      <td>2797.026</td>\n",
       "      <td>2807.380</td>\n",
       "      <td>2784.126</td>\n",
       "      <td>2794.267</td>\n",
       "      <td>3.507</td>\n",
       "      <td>0.1255</td>\n",
       "      <td>86211609.0</td>\n",
       "      <td>1.027651e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20110707</td>\n",
       "      <td>2794.267</td>\n",
       "      <td>2813.193</td>\n",
       "      <td>2825.123</td>\n",
       "      <td>2793.892</td>\n",
       "      <td>2810.479</td>\n",
       "      <td>-16.212</td>\n",
       "      <td>-0.5768</td>\n",
       "      <td>116512143.0</td>\n",
       "      <td>1.400651e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ts_code trade_date     close      open      high       low  pre_close  \\\n",
       "1795  000001.SH   20110713  2795.476  2756.032  2797.266  2756.032   2754.582   \n",
       "1796  000001.SH   20110712  2754.582  2783.722  2783.722  2752.186   2802.692   \n",
       "1797  000001.SH   20110711  2802.692  2789.454  2807.322  2780.665   2797.774   \n",
       "1798  000001.SH   20110708  2797.774  2797.026  2807.380  2784.126   2794.267   \n",
       "1799  000001.SH   20110707  2794.267  2813.193  2825.123  2793.892   2810.479   \n",
       "\n",
       "      change  pct_chg          vol        amount  \n",
       "1795  40.894   1.4846   91929128.0  1.076960e+08  \n",
       "1796 -48.110  -1.7166   96674475.0  1.115352e+08  \n",
       "1797   4.918   0.1758   86063595.0  1.014421e+08  \n",
       "1798   3.507   0.1255   86211609.0  1.027651e+08  \n",
       "1799 -16.212  -0.5768  116512143.0  1.400651e+08  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pro.index_daily(ts_code='000001.SH', start_date='20040216', end_date='20110706')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffa = pd.concat([df,dff], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_code</th>\n",
       "      <th>trade_date</th>\n",
       "      <th>close</th>\n",
       "      <th>vol</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181128</td>\n",
       "      <td>2601.7365</td>\n",
       "      <td>145963473.0</td>\n",
       "      <td>119578491.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181127</td>\n",
       "      <td>2574.6792</td>\n",
       "      <td>123658436.0</td>\n",
       "      <td>102226561.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181126</td>\n",
       "      <td>2575.8101</td>\n",
       "      <td>134314540.0</td>\n",
       "      <td>108158093.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181123</td>\n",
       "      <td>2579.4831</td>\n",
       "      <td>191474549.0</td>\n",
       "      <td>149975643.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181122</td>\n",
       "      <td>2645.4339</td>\n",
       "      <td>149309063.0</td>\n",
       "      <td>125570173.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ts_code trade_date      close          vol       amount\n",
       "0  000001.SH   20181128  2601.7365  145963473.0  119578491.7\n",
       "1  000001.SH   20181127  2574.6792  123658436.0  102226561.4\n",
       "2  000001.SH   20181126  2575.8101  134314540.0  108158093.0\n",
       "3  000001.SH   20181123  2579.4831  191474549.0  149975643.9\n",
       "4  000001.SH   20181122  2645.4339  149309063.0  125570173.8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = dffa.loc[:,['ts_code', 'trade_date', 'close', 'vol', 'amount']]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normlize_field(df, fieldnamelist):\n",
    "    '''\n",
    "    #设df中某一字段 第一日净值为1\n",
    "    '''\n",
    "    for f in fieldnamelist:\n",
    "\n",
    "        df[f+'_1'] = df.apply(lambda x: x[f]/df1.iloc[df.shape[0]-1][f], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "normlize_field(df1, ['close','vol','amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_code</th>\n",
       "      <th>trade_date</th>\n",
       "      <th>close</th>\n",
       "      <th>vol</th>\n",
       "      <th>amount</th>\n",
       "      <th>close_1</th>\n",
       "      <th>vol_1</th>\n",
       "      <th>amount_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181128</td>\n",
       "      <td>2601.7365</td>\n",
       "      <td>145963473.0</td>\n",
       "      <td>119578491.7</td>\n",
       "      <td>1.533295</td>\n",
       "      <td>6.348693</td>\n",
       "      <td>6.157281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181127</td>\n",
       "      <td>2574.6792</td>\n",
       "      <td>123658436.0</td>\n",
       "      <td>102226561.4</td>\n",
       "      <td>1.517349</td>\n",
       "      <td>5.378534</td>\n",
       "      <td>5.263804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181126</td>\n",
       "      <td>2575.8101</td>\n",
       "      <td>134314540.0</td>\n",
       "      <td>108158093.0</td>\n",
       "      <td>1.518016</td>\n",
       "      <td>5.842022</td>\n",
       "      <td>5.569227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181123</td>\n",
       "      <td>2579.4831</td>\n",
       "      <td>191474549.0</td>\n",
       "      <td>149975643.9</td>\n",
       "      <td>1.520180</td>\n",
       "      <td>8.328201</td>\n",
       "      <td>7.722478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181122</td>\n",
       "      <td>2645.4339</td>\n",
       "      <td>149309063.0</td>\n",
       "      <td>125570173.8</td>\n",
       "      <td>1.559048</td>\n",
       "      <td>6.494210</td>\n",
       "      <td>6.465802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ts_code trade_date      close          vol       amount   close_1  \\\n",
       "0  000001.SH   20181128  2601.7365  145963473.0  119578491.7  1.533295   \n",
       "1  000001.SH   20181127  2574.6792  123658436.0  102226561.4  1.517349   \n",
       "2  000001.SH   20181126  2575.8101  134314540.0  108158093.0  1.518016   \n",
       "3  000001.SH   20181123  2579.4831  191474549.0  149975643.9  1.520180   \n",
       "4  000001.SH   20181122  2645.4339  149309063.0  125570173.8  1.559048   \n",
       "\n",
       "      vol_1  amount_1  \n",
       "0  6.348693  6.157281  \n",
       "1  5.378534  5.263804  \n",
       "2  5.842022  5.569227  \n",
       "3  8.328201  7.722478  \n",
       "4  6.494210  6.465802  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_code</th>\n",
       "      <th>trade_date</th>\n",
       "      <th>close</th>\n",
       "      <th>vol</th>\n",
       "      <th>amount</th>\n",
       "      <th>close_1</th>\n",
       "      <th>vol_1</th>\n",
       "      <th>amount_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20040216</td>\n",
       "      <td>1696.827</td>\n",
       "      <td>22991106.0</td>\n",
       "      <td>1.942066e+07</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20040217</td>\n",
       "      <td>1703.774</td>\n",
       "      <td>29754209.0</td>\n",
       "      <td>2.588424e+07</td>\n",
       "      <td>1.004094</td>\n",
       "      <td>1.294162</td>\n",
       "      <td>1.332819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20040218</td>\n",
       "      <td>1717.096</td>\n",
       "      <td>29218375.0</td>\n",
       "      <td>2.548816e+07</td>\n",
       "      <td>1.011945</td>\n",
       "      <td>1.270856</td>\n",
       "      <td>1.312425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20040219</td>\n",
       "      <td>1717.015</td>\n",
       "      <td>27338781.0</td>\n",
       "      <td>2.364013e+07</td>\n",
       "      <td>1.011898</td>\n",
       "      <td>1.189102</td>\n",
       "      <td>1.217267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20040220</td>\n",
       "      <td>1721.752</td>\n",
       "      <td>23409159.0</td>\n",
       "      <td>1.995414e+07</td>\n",
       "      <td>1.014689</td>\n",
       "      <td>1.018183</td>\n",
       "      <td>1.027470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ts_code trade_date     close         vol        amount   close_1  \\\n",
       "1799  000001.SH   20040216  1696.827  22991106.0  1.942066e+07  1.000000   \n",
       "1798  000001.SH   20040217  1703.774  29754209.0  2.588424e+07  1.004094   \n",
       "1797  000001.SH   20040218  1717.096  29218375.0  2.548816e+07  1.011945   \n",
       "1796  000001.SH   20040219  1717.015  27338781.0  2.364013e+07  1.011898   \n",
       "1795  000001.SH   20040220  1721.752  23409159.0  1.995414e+07  1.014689   \n",
       "\n",
       "         vol_1  amount_1  \n",
       "1799  1.000000  1.000000  \n",
       "1798  1.294162  1.332819  \n",
       "1797  1.270856  1.312425  \n",
       "1796  1.189102  1.217267  \n",
       "1795  1.018183  1.027470  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df1.sort_values('trade_date', ascending=True)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_flag(df,series_len,pro_len):\n",
    "    '''\n",
    "    构建训练集\n",
    "    \n",
    "    series_len: 参考的之前的序列范围。如以之前的series_len个序列预测下一个序列，则series_len=series_len\n",
    "    pro_len: 预测日以后的天数（含预测日）\n",
    "    '''\n",
    "    ll = list()\n",
    "    l1 = list(df['close_1'])\n",
    "    l4 = list(df['vol_1'])\n",
    "    l5 = list(df['amount_1'])\n",
    "    for i in range(series_len,(df.shape[0]-pro_len)):\n",
    "        final_list = list()\n",
    "        l2 = l1[i-series_len : i]\n",
    "        l3 = l1[i:i+pro_len]\n",
    "        #最低点和买点的关系：最低点一定是买点，买点不一定是最低点\n",
    "        #买点特征\n",
    "        #买点日之后14日最高收盘价涨幅超过0.05，最低价不得低于买点日收盘价\n",
    "        #最低点的特征：在买点特征的基础上\n",
    "        #最低日收盘价低于前一日收盘价\n",
    "\n",
    "        f1 = 0 # 买点标志 1表示买点\n",
    "        f2 = 0 # 最低点标志 1表示最低点\n",
    "        if (max(l3) - l1[i])/l1[i] > 0.05 and min(l3[1:]) > l1[i]:#未来pro_len日最高收盘价涨幅超过0.05\n",
    "            f1 = 1\n",
    "            if l1[i] < l1[i-1]:\n",
    "                f2 = 1\n",
    "        final_list.append(list(zip(l1[i-series_len : i],l4[i-series_len : i],l5[i-series_len : i])))\n",
    "        final_list.append(f1)\n",
    "        final_list.append(f2)\n",
    "        ll.append(final_list)    \n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[(1.0, 1.0, 1.0),\n",
       "   (1.0040941121281073, 1.2941617075750944, 1.332819407903278),\n",
       "   (1.0119452366092714, 1.2708555647562148, 1.3124245934986103),\n",
       "   (1.011897500452315, 1.1891024729301845, 1.2172667282899259),\n",
       "   (1.0146891816313626, 1.018183248774548, 1.0274695841628634),\n",
       "   (0.9910580159320898, 1.2423488891747965, 1.2299252368464704),\n",
       "   (0.9914369585113862, 1.1743402427008078, 1.1305174547033425),\n",
       "   (0.9708597281867863, 1.0693789589765712, 1.041228335492268),\n",
       "   (0.9752897614194022, 1.0052445497837295, 0.9405989823856901),\n",
       "   (0.9871760645015668, 0.8572168298471592, 0.8476502332207176),\n",
       "   (0.9958351676393645, 0.8694743958816075, 0.8801622380218815),\n",
       "   (0.9943724374965744, 0.8919754882605474, 0.8952930459535767),\n",
       "   (0.9793644254835643, 0.8368442127142557, 0.8213201835875515),\n",
       "   (0.9835263111678445, 0.777928169266846, 0.7572829778961754),\n",
       "   (0.9795317967005476, 0.6992081633654336, 0.6828425689988216),\n",
       "   (0.9648267030168662, 0.6802276062752266, 0.6611068173195006),\n",
       "   (0.9648001829296681, 0.7055970687099612, 0.6528758695737329),\n",
       "   (0.9889476063263963, 0.7200425242700373, 0.6789790462717957),\n",
       "   (0.996944885954785, 0.8048059105986463, 0.8080294687830256),\n",
       "   (0.9987718252950949, 0.6948530009822059, 0.6994799517977691),\n",
       "   (1.0232156843331701, 1.0960835028989036, 1.11589222547362),\n",
       "   (1.0287100570653342, 0.9645577729057488, 0.9875819153056841),\n",
       "   (1.028447802869709, 0.8303478310264848, 0.84843428319706),\n",
       "   (1.0120418875937265, 0.9130141455569819, 0.9095659861952765),\n",
       "   (1.030082618911651, 0.7707875819458185, 0.7661110321113226),\n",
       "   (1.0346870953845029, 0.764898739538672, 0.7710709697811854),\n",
       "   (1.0264422949422658, 0.8039458823772985, 0.8175795759175741),\n",
       "   (1.0254751957624437, 0.7751499210172839, 0.7981448290565998),\n",
       "   (1.0233671434978346, 0.7039870548202423, 0.7115077231086422),\n",
       "   (1.02193505878914, 0.7129697022840049, 0.7514211377387219),\n",
       "   (1.0134645429380837, 0.7146624873113977, 0.7393626836455371),\n",
       "   (1.0161772531908084, 0.5978954644461211, 0.6333551896562776),\n",
       "   (1.0263998628027489, 0.7021864454889643, 0.7305225005967766),\n",
       "   (1.0361380388218715, 0.781525951818064, 0.8282228659221212),\n",
       "   (1.0423248804975405, 0.9258102241797328, 0.9732034502981906),\n",
       "   (1.041050737641492, 1.0334955612835677, 1.1145604549299162),\n",
       "   (1.047552873687182, 1.0028097386876473, 1.0702144760040626),\n",
       "   (1.0458060839437373, 0.973101120059209, 1.054222873860367),\n",
       "   (1.0432878543304651, 0.9729348383674974, 1.063617238635656),\n",
       "   (1.0179871018082574, 1.0618914983907255, 1.1415943561478907),\n",
       "   (1.01541642135586, 0.8021979890832568, 0.8401414854027145),\n",
       "   (1.0107901394779786, 0.8307680369965673, 0.8614721125158838),\n",
       "   (1.0001938913041812, 0.8493464385749864, 0.8668569880293324),\n",
       "   (0.9895387096032772, 0.6584582751260423, 0.66744453322924),\n",
       "   (0.9982520315860133, 0.7618355985136165, 0.744937424656698),\n",
       "   (0.9876581407533002, 0.5998847989305082, 0.6021605438419956),\n",
       "   (0.9838492668963895, 0.5594191075453264, 0.5704979696865726),\n",
       "   (0.9733325789841863, 0.6631797095798697, 0.6330286255870792),\n",
       "   (0.9769911723469747, 0.5458833950832988, 0.5273709570758164),\n",
       "   (0.9638584251665019, 0.5333171879595527, 0.5047422297407423),\n",
       "   (0.9638572464959598, 0.4441962905133837, 0.4271914772632682),\n",
       "   (0.9499259500231902, 0.5540931784664904, 0.5361615769886581),\n",
       "   (0.9469445028868588, 0.5436978542920031, 0.5236409558404092),\n",
       "   (0.9308238258820728, 0.5839538124003256, 0.5658995213596615),\n",
       "   (0.9403356971571056, 0.5401384344015464, 0.5123432581840437),\n",
       "   (0.9194897299488987, 0.3558295107682075, 0.32547333825277075),\n",
       "   (0.9241124758151538, 0.33684321232741044, 0.315036836030531),\n",
       "   (0.9451570490097105, 0.4778182919951741, 0.4471493353371195),\n",
       "   (0.9415043489996328, 0.3856126799641566, 0.3758259737557254),\n",
       "   (0.9209471560742493, 0.4093519467919464, 0.37796247714243103),\n",
       "   (0.9075521546981513, 0.36541691382746005, 0.33421716877628277),\n",
       "   (0.9166768327000926, 0.5242601204135199, 0.5031628036046603),\n",
       "   (0.9183534915462802, 0.5431906581614647, 0.49374406988891406),\n",
       "   (0.9129162843354095, 0.3979802450565014, 0.3637224338195123),\n",
       "   (0.9183788329629361, 0.3670152275405977, 0.32733315923289924),\n",
       "   (0.9136747588292736, 0.32448817381817124, 0.30798085672170344),\n",
       "   (0.8946374615679736, 0.39969577801085343, 0.36292737608349174),\n",
       "   (0.8984039032853673, 0.32214931286907206, 0.2849037656702479),\n",
       "   (0.9176999187306661, 0.5336216535211485, 0.4772261960327517),\n",
       "   (0.9127164996785176, 0.37087341513714045, 0.34131237197609016),\n",
       "   (0.916950284265868, 0.3183135687339269, 0.2876419261946763),\n",
       "   (0.930948175624268, 0.5650676831292936, 0.5682259982733425),\n",
       "   (0.9235278552262547, 0.5943407855194092, 0.6023078184680397),\n",
       "   (0.9096549029453209, 0.49139754303251004, 0.4745899376379035),\n",
       "   (0.9088056708197123, 0.45565550435024743, 0.41972421978812496),\n",
       "   (0.8941070598240127, 0.39690204551273, 0.36388952038421024),\n",
       "   (0.8803572786147321, 0.43555660175721866, 0.40010575691027866),\n",
       "   (0.8651860207316361, 0.4921572715988522, 0.45089601674387336),\n",
       "   (0.8653469092606377, 0.4460376982299155, 0.39850325622821764),\n",
       "   (0.867543951151178, 0.45106681688127576, 0.4002016565673818),\n",
       "   (0.8478978705548651, 0.4511012214897361, 0.3909951009749236),\n",
       "   (0.8626554150776715, 0.4326692678464446, 0.37196083414266984),\n",
       "   (0.8695146882976285, 0.4728264051324891, 0.40393088568705987),\n",
       "   (0.8518847236636381, 0.4010725712803899, 0.34929983180370494),\n",
       "   (0.8411588217302058, 0.4317508257323506, 0.378146157681628),\n",
       "   (0.8447926630116094, 0.330876731202057, 0.3010324784207792),\n",
       "   (0.8549386590383109, 0.4640846334230289, 0.4282317563288395),\n",
       "   (0.8490724157501031, 0.36073427698519595, 0.3349910757934587),\n",
       "   (0.8414257906079995, 0.3965942308299566, 0.3571415646866677),\n",
       "   (0.8257253096514848, 0.4107166049340993, 0.36374197048436224),\n",
       "   (0.8163360201128341, 0.34559994634446906, 0.2894486178374069),\n",
       "   (0.8301901136650938, 0.4441551441674881, 0.3525914976617162),\n",
       "   (0.8245755165376317, 0.35669571529094773, 0.2866902530602372),\n",
       "   (0.8492722004069949, 0.6011962625895423, 0.48347022080927876),\n",
       "   (0.8493440993100653, 0.4642083334311973, 0.3738285472320232),\n",
       "   (0.8513478392316953, 0.35419705341709096, 0.27922111964509727),\n",
       "   (0.8548314000189766, 0.5254985558328512, 0.4258200854683257),\n",
       "   (0.8476986752332442, 0.3403325181485397, 0.2747518591268355),\n",
       "   (0.8485149045836728, 0.30034875225228397, 0.23926617224312408),\n",
       "   (0.8431348628940959, 0.30998847989305084, 0.2525142276922261),\n",
       "   (0.8233885953017013, 0.36457106500226655, 0.29100416847651533),\n",
       "   (0.824751727783681, 0.30423956115899775, 0.23547200234632307),\n",
       "   (0.8272823334376458, 0.440204181564819, 0.33801746101787944),\n",
       "   (0.8377660185746691, 0.5370137478379683, 0.43146434758408314),\n",
       "   (0.8581487682598168, 0.9570068095027703, 0.7477492088746347),\n",
       "   (0.858977962986209, 0.5603984427717397, 0.4592098684529573),\n",
       "   (0.8533227017250433, 0.5271578496484685, 0.425710492973397),\n",
       "   (0.8533792779110658, 0.4053373073918236, 0.335982950694824),\n",
       "   (0.8294528552409881, 0.55587121385113, 0.449902020287838),\n",
       "   (0.8294281031596031, 0.45576850456867973, 0.35652521350950284),\n",
       "   (0.8178028756025217, 0.35226139186170513, 0.2729157892401061),\n",
       "   (0.8157154500723998, 0.3900588775503014, 0.2995863521052109),\n",
       "   (0.8184747178115388, 0.4290769656753355, 0.31806651820713777),\n",
       "   (0.8293897963669837, 0.48192883804719966, 0.3670258263621642),\n",
       "   (0.8169377314245942, 0.3913380678598063, 0.302334260700141),\n",
       "   (0.8092852129297802, 0.30373206056289764, 0.2438344383885318),\n",
       "   (0.80518402877842, 0.33064625077192894, 0.2799429850322888),\n",
       "   (0.8277243348909465, 0.7435044664662935, 0.5577722600285192),\n",
       "   (0.8186096755886133, 0.4654612092171642, 0.3501711678283859),\n",
       "   (0.8194477103440716, 0.39253122490062026, 0.30371714134027417),\n",
       "   (0.8226224594493133, 0.3458714426352521, 0.26097400175637836),\n",
       "   (0.8255225783182375, 0.29650178638644004, 0.23207707987361928),\n",
       "   (0.8162081343590124, 0.32884137892278864, 0.24747824293074347),\n",
       "   (0.8063261605337492, 0.36081809200479525, 0.2645155652516553),\n",
       "   (0.8064758516926004, 0.3593396942278462, 0.275656137485841),\n",
       "   (0.7982611073491876, 0.32579472253313957, 0.24064157538395908),\n",
       "   (0.8016138357062919, 0.3010104864028725, 0.24191630997817662),\n",
       "   (0.8004445945284934, 0.32393526435831316, 0.26272168650772404),\n",
       "   (0.7899744641027047, 0.35224869129827857, 0.26252994290737014),\n",
       "   (0.7907335279318398, 0.3492159533343024, 0.2536885036708582),\n",
       "   (0.7811126296316596, 0.26939765316205316, 0.20198684321913438),\n",
       "   (0.7924355281946833, 0.4116736271843556, 0.2797488887832073),\n",
       "   (0.7895819668121736, 0.2853709604052976, 0.19621165190214018),\n",
       "   (0.7864219510887085, 0.25362111766176015, 0.1800797901346339),\n",
       "   (0.7782844096658057, 0.29368948148905927, 0.20772702069691648),\n",
       "   (0.7775730819936268, 0.25753654478388294, 0.17559583737913415),\n",
       "   (0.7909244725596657, 0.5068365132151538, 0.3539193092779211),\n",
       "   (0.778784165975671, 0.27168740816557496, 0.18737985054422193),\n",
       "   (0.7825942185031237, 0.3711464772508117, 0.24781722567946646),\n",
       "   (0.7821162675982879, 0.3249996759616523, 0.2088775385278078),\n",
       "   (0.7787859339814842, 0.2517192517837115, 0.1644751951537168),\n",
       "   (0.7804531634633348, 0.2458695549487702, 0.1630497230755085),\n",
       "   (0.7716361184728909, 0.3572331839973249, 0.20845949277613593),\n",
       "   (0.7568874139791505, 0.35169412902537184, 0.22769220236183532),\n",
       "   (0.7585222300210922, 0.2658033502172536, 0.17350010376259636),\n",
       "   (0.7427486714909652, 0.29847367934365576, 0.1963432436965883),\n",
       "   (0.7663462450797871, 0.5485272000398763, 0.3586857617488764),\n",
       "   (0.7986824820680011, 1.2749613698444955, 0.8765064475350962),\n",
       "   (0.8080971130233077, 1.0014707426428289, 0.6945878147969746),\n",
       "   (0.8337337866500238, 1.3520724057381146, 0.9702061747893572),\n",
       "   (0.8623277446669578, 1.8219286623270756, 1.3537547174271256),\n",
       "   (0.8536886789283763, 1.5126060051221546, 1.153005275526316),\n",
       "   (0.8369433065362585, 1.3129156118022334, 1.0039221672209113),\n",
       "   (0.8632459290192813, 0.9722169085732544, 0.7569555523219242),\n",
       "   (0.8460279097397673, 1.7770047687136061, 1.4761348802462038),\n",
       "   (0.8412159872514994, 0.8590164387915918, 0.6644803150353531),\n",
       "   (0.8460432324568149, 0.6142536161592226, 0.4765467662187262),\n",
       "   (0.8368613889335802, 0.5732453236481968, 0.446782946110161),\n",
       "   (0.8231245731002631, 0.5636548759333283, 0.4240154913852049),\n",
       "   (0.838582247925098, 0.6008403423480366, 0.48326117033397437),\n",
       "   (0.8328214956504111, 1.0097308498338444, 0.8462208470641468),\n",
       "   (0.8158987333417019, 0.7433238313981068, 0.5958862438387417),\n",
       "   (0.8172435964302784, 0.6212885539303764, 0.5041107289269383),\n",
       "   (0.7855473775464441, 0.7501931833988326, 0.6004600505259691),\n",
       "   (0.7841211861904602, 0.5890611787010159, 0.46138089959868267),\n",
       "   (0.7869947849721863, 0.440132632157844, 0.35035171814502214),\n",
       "   (0.7883054666150409, 0.5671918958574677, 0.43481646997496826),\n",
       "   (0.7841600823183507, 0.5160500325647666, 0.39162717525007334),\n",
       "   (0.7723503928214249, 0.7149055378197117, 0.5495943057339602),\n",
       "   (0.7834357892702085, 0.7796519662864414, 0.5982941947955122),\n",
       "   (0.772707529995692, 0.76403601462235, 0.6080615156690703),\n",
       "   (0.7807413484108868, 0.7424484929085187, 0.5646259239562329),\n",
       "   (0.7913588126544426, 0.9033163519841108, 0.6674142102698504),\n",
       "   (0.7907329385965688, 0.8522351643283277, 0.6304713843162126),\n",
       "   (0.7782384415146624, 0.7788105104643509, 0.5775254644386703),\n",
       "   (0.7692540253072352, 0.5202844526052813, 0.37168383429071283),\n",
       "   (0.7670363566822075, 0.4767476171002822, 0.34558676510788056),\n",
       "   (0.781899981553806, 0.5777379739800251, 0.43310899316953416),\n",
       "   (0.7689505176426353, 0.6078169532166047, 0.45073744698383394),\n",
       "   (0.7691579636580511, 0.4347924801877735, 0.3222163690460494)],\n",
       "  1,\n",
       "  1],\n",
       " [[(1.0040941121281073, 1.2941617075750944, 1.332819407903278),\n",
       "   (1.0119452366092714, 1.2708555647562148, 1.3124245934986103),\n",
       "   (1.011897500452315, 1.1891024729301845, 1.2172667282899259),\n",
       "   (1.0146891816313626, 1.018183248774548, 1.0274695841628634),\n",
       "   (0.9910580159320898, 1.2423488891747965, 1.2299252368464704),\n",
       "   (0.9914369585113862, 1.1743402427008078, 1.1305174547033425),\n",
       "   (0.9708597281867863, 1.0693789589765712, 1.041228335492268),\n",
       "   (0.9752897614194022, 1.0052445497837295, 0.9405989823856901),\n",
       "   (0.9871760645015668, 0.8572168298471592, 0.8476502332207176),\n",
       "   (0.9958351676393645, 0.8694743958816075, 0.8801622380218815),\n",
       "   (0.9943724374965744, 0.8919754882605474, 0.8952930459535767),\n",
       "   (0.9793644254835643, 0.8368442127142557, 0.8213201835875515),\n",
       "   (0.9835263111678445, 0.777928169266846, 0.7572829778961754),\n",
       "   (0.9795317967005476, 0.6992081633654336, 0.6828425689988216),\n",
       "   (0.9648267030168662, 0.6802276062752266, 0.6611068173195006),\n",
       "   (0.9648001829296681, 0.7055970687099612, 0.6528758695737329),\n",
       "   (0.9889476063263963, 0.7200425242700373, 0.6789790462717957),\n",
       "   (0.996944885954785, 0.8048059105986463, 0.8080294687830256),\n",
       "   (0.9987718252950949, 0.6948530009822059, 0.6994799517977691),\n",
       "   (1.0232156843331701, 1.0960835028989036, 1.11589222547362),\n",
       "   (1.0287100570653342, 0.9645577729057488, 0.9875819153056841),\n",
       "   (1.028447802869709, 0.8303478310264848, 0.84843428319706),\n",
       "   (1.0120418875937265, 0.9130141455569819, 0.9095659861952765),\n",
       "   (1.030082618911651, 0.7707875819458185, 0.7661110321113226),\n",
       "   (1.0346870953845029, 0.764898739538672, 0.7710709697811854),\n",
       "   (1.0264422949422658, 0.8039458823772985, 0.8175795759175741),\n",
       "   (1.0254751957624437, 0.7751499210172839, 0.7981448290565998),\n",
       "   (1.0233671434978346, 0.7039870548202423, 0.7115077231086422),\n",
       "   (1.02193505878914, 0.7129697022840049, 0.7514211377387219),\n",
       "   (1.0134645429380837, 0.7146624873113977, 0.7393626836455371),\n",
       "   (1.0161772531908084, 0.5978954644461211, 0.6333551896562776),\n",
       "   (1.0263998628027489, 0.7021864454889643, 0.7305225005967766),\n",
       "   (1.0361380388218715, 0.781525951818064, 0.8282228659221212),\n",
       "   (1.0423248804975405, 0.9258102241797328, 0.9732034502981906),\n",
       "   (1.041050737641492, 1.0334955612835677, 1.1145604549299162),\n",
       "   (1.047552873687182, 1.0028097386876473, 1.0702144760040626),\n",
       "   (1.0458060839437373, 0.973101120059209, 1.054222873860367),\n",
       "   (1.0432878543304651, 0.9729348383674974, 1.063617238635656),\n",
       "   (1.0179871018082574, 1.0618914983907255, 1.1415943561478907),\n",
       "   (1.01541642135586, 0.8021979890832568, 0.8401414854027145),\n",
       "   (1.0107901394779786, 0.8307680369965673, 0.8614721125158838),\n",
       "   (1.0001938913041812, 0.8493464385749864, 0.8668569880293324),\n",
       "   (0.9895387096032772, 0.6584582751260423, 0.66744453322924),\n",
       "   (0.9982520315860133, 0.7618355985136165, 0.744937424656698),\n",
       "   (0.9876581407533002, 0.5998847989305082, 0.6021605438419956),\n",
       "   (0.9838492668963895, 0.5594191075453264, 0.5704979696865726),\n",
       "   (0.9733325789841863, 0.6631797095798697, 0.6330286255870792),\n",
       "   (0.9769911723469747, 0.5458833950832988, 0.5273709570758164),\n",
       "   (0.9638584251665019, 0.5333171879595527, 0.5047422297407423),\n",
       "   (0.9638572464959598, 0.4441962905133837, 0.4271914772632682),\n",
       "   (0.9499259500231902, 0.5540931784664904, 0.5361615769886581),\n",
       "   (0.9469445028868588, 0.5436978542920031, 0.5236409558404092),\n",
       "   (0.9308238258820728, 0.5839538124003256, 0.5658995213596615),\n",
       "   (0.9403356971571056, 0.5401384344015464, 0.5123432581840437),\n",
       "   (0.9194897299488987, 0.3558295107682075, 0.32547333825277075),\n",
       "   (0.9241124758151538, 0.33684321232741044, 0.315036836030531),\n",
       "   (0.9451570490097105, 0.4778182919951741, 0.4471493353371195),\n",
       "   (0.9415043489996328, 0.3856126799641566, 0.3758259737557254),\n",
       "   (0.9209471560742493, 0.4093519467919464, 0.37796247714243103),\n",
       "   (0.9075521546981513, 0.36541691382746005, 0.33421716877628277),\n",
       "   (0.9166768327000926, 0.5242601204135199, 0.5031628036046603),\n",
       "   (0.9183534915462802, 0.5431906581614647, 0.49374406988891406),\n",
       "   (0.9129162843354095, 0.3979802450565014, 0.3637224338195123),\n",
       "   (0.9183788329629361, 0.3670152275405977, 0.32733315923289924),\n",
       "   (0.9136747588292736, 0.32448817381817124, 0.30798085672170344),\n",
       "   (0.8946374615679736, 0.39969577801085343, 0.36292737608349174),\n",
       "   (0.8984039032853673, 0.32214931286907206, 0.2849037656702479),\n",
       "   (0.9176999187306661, 0.5336216535211485, 0.4772261960327517),\n",
       "   (0.9127164996785176, 0.37087341513714045, 0.34131237197609016),\n",
       "   (0.916950284265868, 0.3183135687339269, 0.2876419261946763),\n",
       "   (0.930948175624268, 0.5650676831292936, 0.5682259982733425),\n",
       "   (0.9235278552262547, 0.5943407855194092, 0.6023078184680397),\n",
       "   (0.9096549029453209, 0.49139754303251004, 0.4745899376379035),\n",
       "   (0.9088056708197123, 0.45565550435024743, 0.41972421978812496),\n",
       "   (0.8941070598240127, 0.39690204551273, 0.36388952038421024),\n",
       "   (0.8803572786147321, 0.43555660175721866, 0.40010575691027866),\n",
       "   (0.8651860207316361, 0.4921572715988522, 0.45089601674387336),\n",
       "   (0.8653469092606377, 0.4460376982299155, 0.39850325622821764),\n",
       "   (0.867543951151178, 0.45106681688127576, 0.4002016565673818),\n",
       "   (0.8478978705548651, 0.4511012214897361, 0.3909951009749236),\n",
       "   (0.8626554150776715, 0.4326692678464446, 0.37196083414266984),\n",
       "   (0.8695146882976285, 0.4728264051324891, 0.40393088568705987),\n",
       "   (0.8518847236636381, 0.4010725712803899, 0.34929983180370494),\n",
       "   (0.8411588217302058, 0.4317508257323506, 0.378146157681628),\n",
       "   (0.8447926630116094, 0.330876731202057, 0.3010324784207792),\n",
       "   (0.8549386590383109, 0.4640846334230289, 0.4282317563288395),\n",
       "   (0.8490724157501031, 0.36073427698519595, 0.3349910757934587),\n",
       "   (0.8414257906079995, 0.3965942308299566, 0.3571415646866677),\n",
       "   (0.8257253096514848, 0.4107166049340993, 0.36374197048436224),\n",
       "   (0.8163360201128341, 0.34559994634446906, 0.2894486178374069),\n",
       "   (0.8301901136650938, 0.4441551441674881, 0.3525914976617162),\n",
       "   (0.8245755165376317, 0.35669571529094773, 0.2866902530602372),\n",
       "   (0.8492722004069949, 0.6011962625895423, 0.48347022080927876),\n",
       "   (0.8493440993100653, 0.4642083334311973, 0.3738285472320232),\n",
       "   (0.8513478392316953, 0.35419705341709096, 0.27922111964509727),\n",
       "   (0.8548314000189766, 0.5254985558328512, 0.4258200854683257),\n",
       "   (0.8476986752332442, 0.3403325181485397, 0.2747518591268355),\n",
       "   (0.8485149045836728, 0.30034875225228397, 0.23926617224312408),\n",
       "   (0.8431348628940959, 0.30998847989305084, 0.2525142276922261),\n",
       "   (0.8233885953017013, 0.36457106500226655, 0.29100416847651533),\n",
       "   (0.824751727783681, 0.30423956115899775, 0.23547200234632307),\n",
       "   (0.8272823334376458, 0.440204181564819, 0.33801746101787944),\n",
       "   (0.8377660185746691, 0.5370137478379683, 0.43146434758408314),\n",
       "   (0.8581487682598168, 0.9570068095027703, 0.7477492088746347),\n",
       "   (0.858977962986209, 0.5603984427717397, 0.4592098684529573),\n",
       "   (0.8533227017250433, 0.5271578496484685, 0.425710492973397),\n",
       "   (0.8533792779110658, 0.4053373073918236, 0.335982950694824),\n",
       "   (0.8294528552409881, 0.55587121385113, 0.449902020287838),\n",
       "   (0.8294281031596031, 0.45576850456867973, 0.35652521350950284),\n",
       "   (0.8178028756025217, 0.35226139186170513, 0.2729157892401061),\n",
       "   (0.8157154500723998, 0.3900588775503014, 0.2995863521052109),\n",
       "   (0.8184747178115388, 0.4290769656753355, 0.31806651820713777),\n",
       "   (0.8293897963669837, 0.48192883804719966, 0.3670258263621642),\n",
       "   (0.8169377314245942, 0.3913380678598063, 0.302334260700141),\n",
       "   (0.8092852129297802, 0.30373206056289764, 0.2438344383885318),\n",
       "   (0.80518402877842, 0.33064625077192894, 0.2799429850322888),\n",
       "   (0.8277243348909465, 0.7435044664662935, 0.5577722600285192),\n",
       "   (0.8186096755886133, 0.4654612092171642, 0.3501711678283859),\n",
       "   (0.8194477103440716, 0.39253122490062026, 0.30371714134027417),\n",
       "   (0.8226224594493133, 0.3458714426352521, 0.26097400175637836),\n",
       "   (0.8255225783182375, 0.29650178638644004, 0.23207707987361928),\n",
       "   (0.8162081343590124, 0.32884137892278864, 0.24747824293074347),\n",
       "   (0.8063261605337492, 0.36081809200479525, 0.2645155652516553),\n",
       "   (0.8064758516926004, 0.3593396942278462, 0.275656137485841),\n",
       "   (0.7982611073491876, 0.32579472253313957, 0.24064157538395908),\n",
       "   (0.8016138357062919, 0.3010104864028725, 0.24191630997817662),\n",
       "   (0.8004445945284934, 0.32393526435831316, 0.26272168650772404),\n",
       "   (0.7899744641027047, 0.35224869129827857, 0.26252994290737014),\n",
       "   (0.7907335279318398, 0.3492159533343024, 0.2536885036708582),\n",
       "   (0.7811126296316596, 0.26939765316205316, 0.20198684321913438),\n",
       "   (0.7924355281946833, 0.4116736271843556, 0.2797488887832073),\n",
       "   (0.7895819668121736, 0.2853709604052976, 0.19621165190214018),\n",
       "   (0.7864219510887085, 0.25362111766176015, 0.1800797901346339),\n",
       "   (0.7782844096658057, 0.29368948148905927, 0.20772702069691648),\n",
       "   (0.7775730819936268, 0.25753654478388294, 0.17559583737913415),\n",
       "   (0.7909244725596657, 0.5068365132151538, 0.3539193092779211),\n",
       "   (0.778784165975671, 0.27168740816557496, 0.18737985054422193),\n",
       "   (0.7825942185031237, 0.3711464772508117, 0.24781722567946646),\n",
       "   (0.7821162675982879, 0.3249996759616523, 0.2088775385278078),\n",
       "   (0.7787859339814842, 0.2517192517837115, 0.1644751951537168),\n",
       "   (0.7804531634633348, 0.2458695549487702, 0.1630497230755085),\n",
       "   (0.7716361184728909, 0.3572331839973249, 0.20845949277613593),\n",
       "   (0.7568874139791505, 0.35169412902537184, 0.22769220236183532),\n",
       "   (0.7585222300210922, 0.2658033502172536, 0.17350010376259636),\n",
       "   (0.7427486714909652, 0.29847367934365576, 0.1963432436965883),\n",
       "   (0.7663462450797871, 0.5485272000398763, 0.3586857617488764),\n",
       "   (0.7986824820680011, 1.2749613698444955, 0.8765064475350962),\n",
       "   (0.8080971130233077, 1.0014707426428289, 0.6945878147969746),\n",
       "   (0.8337337866500238, 1.3520724057381146, 0.9702061747893572),\n",
       "   (0.8623277446669578, 1.8219286623270756, 1.3537547174271256),\n",
       "   (0.8536886789283763, 1.5126060051221546, 1.153005275526316),\n",
       "   (0.8369433065362585, 1.3129156118022334, 1.0039221672209113),\n",
       "   (0.8632459290192813, 0.9722169085732544, 0.7569555523219242),\n",
       "   (0.8460279097397673, 1.7770047687136061, 1.4761348802462038),\n",
       "   (0.8412159872514994, 0.8590164387915918, 0.6644803150353531),\n",
       "   (0.8460432324568149, 0.6142536161592226, 0.4765467662187262),\n",
       "   (0.8368613889335802, 0.5732453236481968, 0.446782946110161),\n",
       "   (0.8231245731002631, 0.5636548759333283, 0.4240154913852049),\n",
       "   (0.838582247925098, 0.6008403423480366, 0.48326117033397437),\n",
       "   (0.8328214956504111, 1.0097308498338444, 0.8462208470641468),\n",
       "   (0.8158987333417019, 0.7433238313981068, 0.5958862438387417),\n",
       "   (0.8172435964302784, 0.6212885539303764, 0.5041107289269383),\n",
       "   (0.7855473775464441, 0.7501931833988326, 0.6004600505259691),\n",
       "   (0.7841211861904602, 0.5890611787010159, 0.46138089959868267),\n",
       "   (0.7869947849721863, 0.440132632157844, 0.35035171814502214),\n",
       "   (0.7883054666150409, 0.5671918958574677, 0.43481646997496826),\n",
       "   (0.7841600823183507, 0.5160500325647666, 0.39162717525007334),\n",
       "   (0.7723503928214249, 0.7149055378197117, 0.5495943057339602),\n",
       "   (0.7834357892702085, 0.7796519662864414, 0.5982941947955122),\n",
       "   (0.772707529995692, 0.76403601462235, 0.6080615156690703),\n",
       "   (0.7807413484108868, 0.7424484929085187, 0.5646259239562329),\n",
       "   (0.7913588126544426, 0.9033163519841108, 0.6674142102698504),\n",
       "   (0.7907329385965688, 0.8522351643283277, 0.6304713843162126),\n",
       "   (0.7782384415146624, 0.7788105104643509, 0.5775254644386703),\n",
       "   (0.7692540253072352, 0.5202844526052813, 0.37168383429071283),\n",
       "   (0.7670363566822075, 0.4767476171002822, 0.34558676510788056),\n",
       "   (0.781899981553806, 0.5777379739800251, 0.43310899316953416),\n",
       "   (0.7689505176426353, 0.6078169532166047, 0.45073744698383394),\n",
       "   (0.7691579636580511, 0.4347924801877735, 0.3222163690460494),\n",
       "   (0.7686281512493613, 0.3208482445342125, 0.22647033758762491)],\n",
       "  1,\n",
       "  0],\n",
       " [[(1.0119452366092714, 1.2708555647562148, 1.3124245934986103),\n",
       "   (1.011897500452315, 1.1891024729301845, 1.2172667282899259),\n",
       "   (1.0146891816313626, 1.018183248774548, 1.0274695841628634),\n",
       "   (0.9910580159320898, 1.2423488891747965, 1.2299252368464704),\n",
       "   (0.9914369585113862, 1.1743402427008078, 1.1305174547033425),\n",
       "   (0.9708597281867863, 1.0693789589765712, 1.041228335492268),\n",
       "   (0.9752897614194022, 1.0052445497837295, 0.9405989823856901),\n",
       "   (0.9871760645015668, 0.8572168298471592, 0.8476502332207176),\n",
       "   (0.9958351676393645, 0.8694743958816075, 0.8801622380218815),\n",
       "   (0.9943724374965744, 0.8919754882605474, 0.8952930459535767),\n",
       "   (0.9793644254835643, 0.8368442127142557, 0.8213201835875515),\n",
       "   (0.9835263111678445, 0.777928169266846, 0.7572829778961754),\n",
       "   (0.9795317967005476, 0.6992081633654336, 0.6828425689988216),\n",
       "   (0.9648267030168662, 0.6802276062752266, 0.6611068173195006),\n",
       "   (0.9648001829296681, 0.7055970687099612, 0.6528758695737329),\n",
       "   (0.9889476063263963, 0.7200425242700373, 0.6789790462717957),\n",
       "   (0.996944885954785, 0.8048059105986463, 0.8080294687830256),\n",
       "   (0.9987718252950949, 0.6948530009822059, 0.6994799517977691),\n",
       "   (1.0232156843331701, 1.0960835028989036, 1.11589222547362),\n",
       "   (1.0287100570653342, 0.9645577729057488, 0.9875819153056841),\n",
       "   (1.028447802869709, 0.8303478310264848, 0.84843428319706),\n",
       "   (1.0120418875937265, 0.9130141455569819, 0.9095659861952765),\n",
       "   (1.030082618911651, 0.7707875819458185, 0.7661110321113226),\n",
       "   (1.0346870953845029, 0.764898739538672, 0.7710709697811854),\n",
       "   (1.0264422949422658, 0.8039458823772985, 0.8175795759175741),\n",
       "   (1.0254751957624437, 0.7751499210172839, 0.7981448290565998),\n",
       "   (1.0233671434978346, 0.7039870548202423, 0.7115077231086422),\n",
       "   (1.02193505878914, 0.7129697022840049, 0.7514211377387219),\n",
       "   (1.0134645429380837, 0.7146624873113977, 0.7393626836455371),\n",
       "   (1.0161772531908084, 0.5978954644461211, 0.6333551896562776),\n",
       "   (1.0263998628027489, 0.7021864454889643, 0.7305225005967766),\n",
       "   (1.0361380388218715, 0.781525951818064, 0.8282228659221212),\n",
       "   (1.0423248804975405, 0.9258102241797328, 0.9732034502981906),\n",
       "   (1.041050737641492, 1.0334955612835677, 1.1145604549299162),\n",
       "   (1.047552873687182, 1.0028097386876473, 1.0702144760040626),\n",
       "   (1.0458060839437373, 0.973101120059209, 1.054222873860367),\n",
       "   (1.0432878543304651, 0.9729348383674974, 1.063617238635656),\n",
       "   (1.0179871018082574, 1.0618914983907255, 1.1415943561478907),\n",
       "   (1.01541642135586, 0.8021979890832568, 0.8401414854027145),\n",
       "   (1.0107901394779786, 0.8307680369965673, 0.8614721125158838),\n",
       "   (1.0001938913041812, 0.8493464385749864, 0.8668569880293324),\n",
       "   (0.9895387096032772, 0.6584582751260423, 0.66744453322924),\n",
       "   (0.9982520315860133, 0.7618355985136165, 0.744937424656698),\n",
       "   (0.9876581407533002, 0.5998847989305082, 0.6021605438419956),\n",
       "   (0.9838492668963895, 0.5594191075453264, 0.5704979696865726),\n",
       "   (0.9733325789841863, 0.6631797095798697, 0.6330286255870792),\n",
       "   (0.9769911723469747, 0.5458833950832988, 0.5273709570758164),\n",
       "   (0.9638584251665019, 0.5333171879595527, 0.5047422297407423),\n",
       "   (0.9638572464959598, 0.4441962905133837, 0.4271914772632682),\n",
       "   (0.9499259500231902, 0.5540931784664904, 0.5361615769886581),\n",
       "   (0.9469445028868588, 0.5436978542920031, 0.5236409558404092),\n",
       "   (0.9308238258820728, 0.5839538124003256, 0.5658995213596615),\n",
       "   (0.9403356971571056, 0.5401384344015464, 0.5123432581840437),\n",
       "   (0.9194897299488987, 0.3558295107682075, 0.32547333825277075),\n",
       "   (0.9241124758151538, 0.33684321232741044, 0.315036836030531),\n",
       "   (0.9451570490097105, 0.4778182919951741, 0.4471493353371195),\n",
       "   (0.9415043489996328, 0.3856126799641566, 0.3758259737557254),\n",
       "   (0.9209471560742493, 0.4093519467919464, 0.37796247714243103),\n",
       "   (0.9075521546981513, 0.36541691382746005, 0.33421716877628277),\n",
       "   (0.9166768327000926, 0.5242601204135199, 0.5031628036046603),\n",
       "   (0.9183534915462802, 0.5431906581614647, 0.49374406988891406),\n",
       "   (0.9129162843354095, 0.3979802450565014, 0.3637224338195123),\n",
       "   (0.9183788329629361, 0.3670152275405977, 0.32733315923289924),\n",
       "   (0.9136747588292736, 0.32448817381817124, 0.30798085672170344),\n",
       "   (0.8946374615679736, 0.39969577801085343, 0.36292737608349174),\n",
       "   (0.8984039032853673, 0.32214931286907206, 0.2849037656702479),\n",
       "   (0.9176999187306661, 0.5336216535211485, 0.4772261960327517),\n",
       "   (0.9127164996785176, 0.37087341513714045, 0.34131237197609016),\n",
       "   (0.916950284265868, 0.3183135687339269, 0.2876419261946763),\n",
       "   (0.930948175624268, 0.5650676831292936, 0.5682259982733425),\n",
       "   (0.9235278552262547, 0.5943407855194092, 0.6023078184680397),\n",
       "   (0.9096549029453209, 0.49139754303251004, 0.4745899376379035),\n",
       "   (0.9088056708197123, 0.45565550435024743, 0.41972421978812496),\n",
       "   (0.8941070598240127, 0.39690204551273, 0.36388952038421024),\n",
       "   (0.8803572786147321, 0.43555660175721866, 0.40010575691027866),\n",
       "   (0.8651860207316361, 0.4921572715988522, 0.45089601674387336),\n",
       "   (0.8653469092606377, 0.4460376982299155, 0.39850325622821764),\n",
       "   (0.867543951151178, 0.45106681688127576, 0.4002016565673818),\n",
       "   (0.8478978705548651, 0.4511012214897361, 0.3909951009749236),\n",
       "   (0.8626554150776715, 0.4326692678464446, 0.37196083414266984),\n",
       "   (0.8695146882976285, 0.4728264051324891, 0.40393088568705987),\n",
       "   (0.8518847236636381, 0.4010725712803899, 0.34929983180370494),\n",
       "   (0.8411588217302058, 0.4317508257323506, 0.378146157681628),\n",
       "   (0.8447926630116094, 0.330876731202057, 0.3010324784207792),\n",
       "   (0.8549386590383109, 0.4640846334230289, 0.4282317563288395),\n",
       "   (0.8490724157501031, 0.36073427698519595, 0.3349910757934587),\n",
       "   (0.8414257906079995, 0.3965942308299566, 0.3571415646866677),\n",
       "   (0.8257253096514848, 0.4107166049340993, 0.36374197048436224),\n",
       "   (0.8163360201128341, 0.34559994634446906, 0.2894486178374069),\n",
       "   (0.8301901136650938, 0.4441551441674881, 0.3525914976617162),\n",
       "   (0.8245755165376317, 0.35669571529094773, 0.2866902530602372),\n",
       "   (0.8492722004069949, 0.6011962625895423, 0.48347022080927876),\n",
       "   (0.8493440993100653, 0.4642083334311973, 0.3738285472320232),\n",
       "   (0.8513478392316953, 0.35419705341709096, 0.27922111964509727),\n",
       "   (0.8548314000189766, 0.5254985558328512, 0.4258200854683257),\n",
       "   (0.8476986752332442, 0.3403325181485397, 0.2747518591268355),\n",
       "   (0.8485149045836728, 0.30034875225228397, 0.23926617224312408),\n",
       "   (0.8431348628940959, 0.30998847989305084, 0.2525142276922261),\n",
       "   (0.8233885953017013, 0.36457106500226655, 0.29100416847651533),\n",
       "   (0.824751727783681, 0.30423956115899775, 0.23547200234632307),\n",
       "   (0.8272823334376458, 0.440204181564819, 0.33801746101787944),\n",
       "   (0.8377660185746691, 0.5370137478379683, 0.43146434758408314),\n",
       "   (0.8581487682598168, 0.9570068095027703, 0.7477492088746347),\n",
       "   (0.858977962986209, 0.5603984427717397, 0.4592098684529573),\n",
       "   (0.8533227017250433, 0.5271578496484685, 0.425710492973397),\n",
       "   (0.8533792779110658, 0.4053373073918236, 0.335982950694824),\n",
       "   (0.8294528552409881, 0.55587121385113, 0.449902020287838),\n",
       "   (0.8294281031596031, 0.45576850456867973, 0.35652521350950284),\n",
       "   (0.8178028756025217, 0.35226139186170513, 0.2729157892401061),\n",
       "   (0.8157154500723998, 0.3900588775503014, 0.2995863521052109),\n",
       "   (0.8184747178115388, 0.4290769656753355, 0.31806651820713777),\n",
       "   (0.8293897963669837, 0.48192883804719966, 0.3670258263621642),\n",
       "   (0.8169377314245942, 0.3913380678598063, 0.302334260700141),\n",
       "   (0.8092852129297802, 0.30373206056289764, 0.2438344383885318),\n",
       "   (0.80518402877842, 0.33064625077192894, 0.2799429850322888),\n",
       "   (0.8277243348909465, 0.7435044664662935, 0.5577722600285192),\n",
       "   (0.8186096755886133, 0.4654612092171642, 0.3501711678283859),\n",
       "   (0.8194477103440716, 0.39253122490062026, 0.30371714134027417),\n",
       "   (0.8226224594493133, 0.3458714426352521, 0.26097400175637836),\n",
       "   (0.8255225783182375, 0.29650178638644004, 0.23207707987361928),\n",
       "   (0.8162081343590124, 0.32884137892278864, 0.24747824293074347),\n",
       "   (0.8063261605337492, 0.36081809200479525, 0.2645155652516553),\n",
       "   (0.8064758516926004, 0.3593396942278462, 0.275656137485841),\n",
       "   (0.7982611073491876, 0.32579472253313957, 0.24064157538395908),\n",
       "   (0.8016138357062919, 0.3010104864028725, 0.24191630997817662),\n",
       "   (0.8004445945284934, 0.32393526435831316, 0.26272168650772404),\n",
       "   (0.7899744641027047, 0.35224869129827857, 0.26252994290737014),\n",
       "   (0.7907335279318398, 0.3492159533343024, 0.2536885036708582),\n",
       "   (0.7811126296316596, 0.26939765316205316, 0.20198684321913438),\n",
       "   (0.7924355281946833, 0.4116736271843556, 0.2797488887832073),\n",
       "   (0.7895819668121736, 0.2853709604052976, 0.19621165190214018),\n",
       "   (0.7864219510887085, 0.25362111766176015, 0.1800797901346339),\n",
       "   (0.7782844096658057, 0.29368948148905927, 0.20772702069691648),\n",
       "   (0.7775730819936268, 0.25753654478388294, 0.17559583737913415),\n",
       "   (0.7909244725596657, 0.5068365132151538, 0.3539193092779211),\n",
       "   (0.778784165975671, 0.27168740816557496, 0.18737985054422193),\n",
       "   (0.7825942185031237, 0.3711464772508117, 0.24781722567946646),\n",
       "   (0.7821162675982879, 0.3249996759616523, 0.2088775385278078),\n",
       "   (0.7787859339814842, 0.2517192517837115, 0.1644751951537168),\n",
       "   (0.7804531634633348, 0.2458695549487702, 0.1630497230755085),\n",
       "   (0.7716361184728909, 0.3572331839973249, 0.20845949277613593),\n",
       "   (0.7568874139791505, 0.35169412902537184, 0.22769220236183532),\n",
       "   (0.7585222300210922, 0.2658033502172536, 0.17350010376259636),\n",
       "   (0.7427486714909652, 0.29847367934365576, 0.1963432436965883),\n",
       "   (0.7663462450797871, 0.5485272000398763, 0.3586857617488764),\n",
       "   (0.7986824820680011, 1.2749613698444955, 0.8765064475350962),\n",
       "   (0.8080971130233077, 1.0014707426428289, 0.6945878147969746),\n",
       "   (0.8337337866500238, 1.3520724057381146, 0.9702061747893572),\n",
       "   (0.8623277446669578, 1.8219286623270756, 1.3537547174271256),\n",
       "   (0.8536886789283763, 1.5126060051221546, 1.153005275526316),\n",
       "   (0.8369433065362585, 1.3129156118022334, 1.0039221672209113),\n",
       "   (0.8632459290192813, 0.9722169085732544, 0.7569555523219242),\n",
       "   (0.8460279097397673, 1.7770047687136061, 1.4761348802462038),\n",
       "   (0.8412159872514994, 0.8590164387915918, 0.6644803150353531),\n",
       "   (0.8460432324568149, 0.6142536161592226, 0.4765467662187262),\n",
       "   (0.8368613889335802, 0.5732453236481968, 0.446782946110161),\n",
       "   (0.8231245731002631, 0.5636548759333283, 0.4240154913852049),\n",
       "   (0.838582247925098, 0.6008403423480366, 0.48326117033397437),\n",
       "   (0.8328214956504111, 1.0097308498338444, 0.8462208470641468),\n",
       "   (0.8158987333417019, 0.7433238313981068, 0.5958862438387417),\n",
       "   (0.8172435964302784, 0.6212885539303764, 0.5041107289269383),\n",
       "   (0.7855473775464441, 0.7501931833988326, 0.6004600505259691),\n",
       "   (0.7841211861904602, 0.5890611787010159, 0.46138089959868267),\n",
       "   (0.7869947849721863, 0.440132632157844, 0.35035171814502214),\n",
       "   (0.7883054666150409, 0.5671918958574677, 0.43481646997496826),\n",
       "   (0.7841600823183507, 0.5160500325647666, 0.39162717525007334),\n",
       "   (0.7723503928214249, 0.7149055378197117, 0.5495943057339602),\n",
       "   (0.7834357892702085, 0.7796519662864414, 0.5982941947955122),\n",
       "   (0.772707529995692, 0.76403601462235, 0.6080615156690703),\n",
       "   (0.7807413484108868, 0.7424484929085187, 0.5646259239562329),\n",
       "   (0.7913588126544426, 0.9033163519841108, 0.6674142102698504),\n",
       "   (0.7907329385965688, 0.8522351643283277, 0.6304713843162126),\n",
       "   (0.7782384415146624, 0.7788105104643509, 0.5775254644386703),\n",
       "   (0.7692540253072352, 0.5202844526052813, 0.37168383429071283),\n",
       "   (0.7670363566822075, 0.4767476171002822, 0.34558676510788056),\n",
       "   (0.781899981553806, 0.5777379739800251, 0.43310899316953416),\n",
       "   (0.7689505176426353, 0.6078169532166047, 0.45073744698383394),\n",
       "   (0.7691579636580511, 0.4347924801877735, 0.3222163690460494),\n",
       "   (0.7686281512493613, 0.3208482445342125, 0.22647033758762491),\n",
       "   (0.770514024116778, 0.30932135235251407, 0.2198893006438326)],\n",
       "  0,\n",
       "  0],\n",
       " [[(1.011897500452315, 1.1891024729301845, 1.2172667282899259),\n",
       "   (1.0146891816313626, 1.018183248774548, 1.0274695841628634),\n",
       "   (0.9910580159320898, 1.2423488891747965, 1.2299252368464704),\n",
       "   (0.9914369585113862, 1.1743402427008078, 1.1305174547033425),\n",
       "   (0.9708597281867863, 1.0693789589765712, 1.041228335492268),\n",
       "   (0.9752897614194022, 1.0052445497837295, 0.9405989823856901),\n",
       "   (0.9871760645015668, 0.8572168298471592, 0.8476502332207176),\n",
       "   (0.9958351676393645, 0.8694743958816075, 0.8801622380218815),\n",
       "   (0.9943724374965744, 0.8919754882605474, 0.8952930459535767),\n",
       "   (0.9793644254835643, 0.8368442127142557, 0.8213201835875515),\n",
       "   (0.9835263111678445, 0.777928169266846, 0.7572829778961754),\n",
       "   (0.9795317967005476, 0.6992081633654336, 0.6828425689988216),\n",
       "   (0.9648267030168662, 0.6802276062752266, 0.6611068173195006),\n",
       "   (0.9648001829296681, 0.7055970687099612, 0.6528758695737329),\n",
       "   (0.9889476063263963, 0.7200425242700373, 0.6789790462717957),\n",
       "   (0.996944885954785, 0.8048059105986463, 0.8080294687830256),\n",
       "   (0.9987718252950949, 0.6948530009822059, 0.6994799517977691),\n",
       "   (1.0232156843331701, 1.0960835028989036, 1.11589222547362),\n",
       "   (1.0287100570653342, 0.9645577729057488, 0.9875819153056841),\n",
       "   (1.028447802869709, 0.8303478310264848, 0.84843428319706),\n",
       "   (1.0120418875937265, 0.9130141455569819, 0.9095659861952765),\n",
       "   (1.030082618911651, 0.7707875819458185, 0.7661110321113226),\n",
       "   (1.0346870953845029, 0.764898739538672, 0.7710709697811854),\n",
       "   (1.0264422949422658, 0.8039458823772985, 0.8175795759175741),\n",
       "   (1.0254751957624437, 0.7751499210172839, 0.7981448290565998),\n",
       "   (1.0233671434978346, 0.7039870548202423, 0.7115077231086422),\n",
       "   (1.02193505878914, 0.7129697022840049, 0.7514211377387219),\n",
       "   (1.0134645429380837, 0.7146624873113977, 0.7393626836455371),\n",
       "   (1.0161772531908084, 0.5978954644461211, 0.6333551896562776),\n",
       "   (1.0263998628027489, 0.7021864454889643, 0.7305225005967766),\n",
       "   (1.0361380388218715, 0.781525951818064, 0.8282228659221212),\n",
       "   (1.0423248804975405, 0.9258102241797328, 0.9732034502981906),\n",
       "   (1.041050737641492, 1.0334955612835677, 1.1145604549299162),\n",
       "   (1.047552873687182, 1.0028097386876473, 1.0702144760040626),\n",
       "   (1.0458060839437373, 0.973101120059209, 1.054222873860367),\n",
       "   (1.0432878543304651, 0.9729348383674974, 1.063617238635656),\n",
       "   (1.0179871018082574, 1.0618914983907255, 1.1415943561478907),\n",
       "   (1.01541642135586, 0.8021979890832568, 0.8401414854027145),\n",
       "   (1.0107901394779786, 0.8307680369965673, 0.8614721125158838),\n",
       "   (1.0001938913041812, 0.8493464385749864, 0.8668569880293324),\n",
       "   (0.9895387096032772, 0.6584582751260423, 0.66744453322924),\n",
       "   (0.9982520315860133, 0.7618355985136165, 0.744937424656698),\n",
       "   (0.9876581407533002, 0.5998847989305082, 0.6021605438419956),\n",
       "   (0.9838492668963895, 0.5594191075453264, 0.5704979696865726),\n",
       "   (0.9733325789841863, 0.6631797095798697, 0.6330286255870792),\n",
       "   (0.9769911723469747, 0.5458833950832988, 0.5273709570758164),\n",
       "   (0.9638584251665019, 0.5333171879595527, 0.5047422297407423),\n",
       "   (0.9638572464959598, 0.4441962905133837, 0.4271914772632682),\n",
       "   (0.9499259500231902, 0.5540931784664904, 0.5361615769886581),\n",
       "   (0.9469445028868588, 0.5436978542920031, 0.5236409558404092),\n",
       "   (0.9308238258820728, 0.5839538124003256, 0.5658995213596615),\n",
       "   (0.9403356971571056, 0.5401384344015464, 0.5123432581840437),\n",
       "   (0.9194897299488987, 0.3558295107682075, 0.32547333825277075),\n",
       "   (0.9241124758151538, 0.33684321232741044, 0.315036836030531),\n",
       "   (0.9451570490097105, 0.4778182919951741, 0.4471493353371195),\n",
       "   (0.9415043489996328, 0.3856126799641566, 0.3758259737557254),\n",
       "   (0.9209471560742493, 0.4093519467919464, 0.37796247714243103),\n",
       "   (0.9075521546981513, 0.36541691382746005, 0.33421716877628277),\n",
       "   (0.9166768327000926, 0.5242601204135199, 0.5031628036046603),\n",
       "   (0.9183534915462802, 0.5431906581614647, 0.49374406988891406),\n",
       "   (0.9129162843354095, 0.3979802450565014, 0.3637224338195123),\n",
       "   (0.9183788329629361, 0.3670152275405977, 0.32733315923289924),\n",
       "   (0.9136747588292736, 0.32448817381817124, 0.30798085672170344),\n",
       "   (0.8946374615679736, 0.39969577801085343, 0.36292737608349174),\n",
       "   (0.8984039032853673, 0.32214931286907206, 0.2849037656702479),\n",
       "   (0.9176999187306661, 0.5336216535211485, 0.4772261960327517),\n",
       "   (0.9127164996785176, 0.37087341513714045, 0.34131237197609016),\n",
       "   (0.916950284265868, 0.3183135687339269, 0.2876419261946763),\n",
       "   (0.930948175624268, 0.5650676831292936, 0.5682259982733425),\n",
       "   (0.9235278552262547, 0.5943407855194092, 0.6023078184680397),\n",
       "   (0.9096549029453209, 0.49139754303251004, 0.4745899376379035),\n",
       "   (0.9088056708197123, 0.45565550435024743, 0.41972421978812496),\n",
       "   (0.8941070598240127, 0.39690204551273, 0.36388952038421024),\n",
       "   (0.8803572786147321, 0.43555660175721866, 0.40010575691027866),\n",
       "   (0.8651860207316361, 0.4921572715988522, 0.45089601674387336),\n",
       "   (0.8653469092606377, 0.4460376982299155, 0.39850325622821764),\n",
       "   (0.867543951151178, 0.45106681688127576, 0.4002016565673818),\n",
       "   (0.8478978705548651, 0.4511012214897361, 0.3909951009749236),\n",
       "   (0.8626554150776715, 0.4326692678464446, 0.37196083414266984),\n",
       "   (0.8695146882976285, 0.4728264051324891, 0.40393088568705987),\n",
       "   (0.8518847236636381, 0.4010725712803899, 0.34929983180370494),\n",
       "   (0.8411588217302058, 0.4317508257323506, 0.378146157681628),\n",
       "   (0.8447926630116094, 0.330876731202057, 0.3010324784207792),\n",
       "   (0.8549386590383109, 0.4640846334230289, 0.4282317563288395),\n",
       "   (0.8490724157501031, 0.36073427698519595, 0.3349910757934587),\n",
       "   (0.8414257906079995, 0.3965942308299566, 0.3571415646866677),\n",
       "   (0.8257253096514848, 0.4107166049340993, 0.36374197048436224),\n",
       "   (0.8163360201128341, 0.34559994634446906, 0.2894486178374069),\n",
       "   (0.8301901136650938, 0.4441551441674881, 0.3525914976617162),\n",
       "   (0.8245755165376317, 0.35669571529094773, 0.2866902530602372),\n",
       "   (0.8492722004069949, 0.6011962625895423, 0.48347022080927876),\n",
       "   (0.8493440993100653, 0.4642083334311973, 0.3738285472320232),\n",
       "   (0.8513478392316953, 0.35419705341709096, 0.27922111964509727),\n",
       "   (0.8548314000189766, 0.5254985558328512, 0.4258200854683257),\n",
       "   (0.8476986752332442, 0.3403325181485397, 0.2747518591268355),\n",
       "   (0.8485149045836728, 0.30034875225228397, 0.23926617224312408),\n",
       "   (0.8431348628940959, 0.30998847989305084, 0.2525142276922261),\n",
       "   (0.8233885953017013, 0.36457106500226655, 0.29100416847651533),\n",
       "   (0.824751727783681, 0.30423956115899775, 0.23547200234632307),\n",
       "   (0.8272823334376458, 0.440204181564819, 0.33801746101787944),\n",
       "   (0.8377660185746691, 0.5370137478379683, 0.43146434758408314),\n",
       "   (0.8581487682598168, 0.9570068095027703, 0.7477492088746347),\n",
       "   (0.858977962986209, 0.5603984427717397, 0.4592098684529573),\n",
       "   (0.8533227017250433, 0.5271578496484685, 0.425710492973397),\n",
       "   (0.8533792779110658, 0.4053373073918236, 0.335982950694824),\n",
       "   (0.8294528552409881, 0.55587121385113, 0.449902020287838),\n",
       "   (0.8294281031596031, 0.45576850456867973, 0.35652521350950284),\n",
       "   (0.8178028756025217, 0.35226139186170513, 0.2729157892401061),\n",
       "   (0.8157154500723998, 0.3900588775503014, 0.2995863521052109),\n",
       "   (0.8184747178115388, 0.4290769656753355, 0.31806651820713777),\n",
       "   (0.8293897963669837, 0.48192883804719966, 0.3670258263621642),\n",
       "   (0.8169377314245942, 0.3913380678598063, 0.302334260700141),\n",
       "   (0.8092852129297802, 0.30373206056289764, 0.2438344383885318),\n",
       "   (0.80518402877842, 0.33064625077192894, 0.2799429850322888),\n",
       "   (0.8277243348909465, 0.7435044664662935, 0.5577722600285192),\n",
       "   (0.8186096755886133, 0.4654612092171642, 0.3501711678283859),\n",
       "   (0.8194477103440716, 0.39253122490062026, 0.30371714134027417),\n",
       "   (0.8226224594493133, 0.3458714426352521, 0.26097400175637836),\n",
       "   (0.8255225783182375, 0.29650178638644004, 0.23207707987361928),\n",
       "   (0.8162081343590124, 0.32884137892278864, 0.24747824293074347),\n",
       "   (0.8063261605337492, 0.36081809200479525, 0.2645155652516553),\n",
       "   (0.8064758516926004, 0.3593396942278462, 0.275656137485841),\n",
       "   (0.7982611073491876, 0.32579472253313957, 0.24064157538395908),\n",
       "   (0.8016138357062919, 0.3010104864028725, 0.24191630997817662),\n",
       "   (0.8004445945284934, 0.32393526435831316, 0.26272168650772404),\n",
       "   (0.7899744641027047, 0.35224869129827857, 0.26252994290737014),\n",
       "   (0.7907335279318398, 0.3492159533343024, 0.2536885036708582),\n",
       "   (0.7811126296316596, 0.26939765316205316, 0.20198684321913438),\n",
       "   (0.7924355281946833, 0.4116736271843556, 0.2797488887832073),\n",
       "   (0.7895819668121736, 0.2853709604052976, 0.19621165190214018),\n",
       "   (0.7864219510887085, 0.25362111766176015, 0.1800797901346339),\n",
       "   (0.7782844096658057, 0.29368948148905927, 0.20772702069691648),\n",
       "   (0.7775730819936268, 0.25753654478388294, 0.17559583737913415),\n",
       "   (0.7909244725596657, 0.5068365132151538, 0.3539193092779211),\n",
       "   (0.778784165975671, 0.27168740816557496, 0.18737985054422193),\n",
       "   (0.7825942185031237, 0.3711464772508117, 0.24781722567946646),\n",
       "   (0.7821162675982879, 0.3249996759616523, 0.2088775385278078),\n",
       "   (0.7787859339814842, 0.2517192517837115, 0.1644751951537168),\n",
       "   (0.7804531634633348, 0.2458695549487702, 0.1630497230755085),\n",
       "   (0.7716361184728909, 0.3572331839973249, 0.20845949277613593),\n",
       "   (0.7568874139791505, 0.35169412902537184, 0.22769220236183532),\n",
       "   (0.7585222300210922, 0.2658033502172536, 0.17350010376259636),\n",
       "   (0.7427486714909652, 0.29847367934365576, 0.1963432436965883),\n",
       "   (0.7663462450797871, 0.5485272000398763, 0.3586857617488764),\n",
       "   (0.7986824820680011, 1.2749613698444955, 0.8765064475350962),\n",
       "   (0.8080971130233077, 1.0014707426428289, 0.6945878147969746),\n",
       "   (0.8337337866500238, 1.3520724057381146, 0.9702061747893572),\n",
       "   (0.8623277446669578, 1.8219286623270756, 1.3537547174271256),\n",
       "   (0.8536886789283763, 1.5126060051221546, 1.153005275526316),\n",
       "   (0.8369433065362585, 1.3129156118022334, 1.0039221672209113),\n",
       "   (0.8632459290192813, 0.9722169085732544, 0.7569555523219242),\n",
       "   (0.8460279097397673, 1.7770047687136061, 1.4761348802462038),\n",
       "   (0.8412159872514994, 0.8590164387915918, 0.6644803150353531),\n",
       "   (0.8460432324568149, 0.6142536161592226, 0.4765467662187262),\n",
       "   (0.8368613889335802, 0.5732453236481968, 0.446782946110161),\n",
       "   (0.8231245731002631, 0.5636548759333283, 0.4240154913852049),\n",
       "   (0.838582247925098, 0.6008403423480366, 0.48326117033397437),\n",
       "   (0.8328214956504111, 1.0097308498338444, 0.8462208470641468),\n",
       "   (0.8158987333417019, 0.7433238313981068, 0.5958862438387417),\n",
       "   (0.8172435964302784, 0.6212885539303764, 0.5041107289269383),\n",
       "   (0.7855473775464441, 0.7501931833988326, 0.6004600505259691),\n",
       "   (0.7841211861904602, 0.5890611787010159, 0.46138089959868267),\n",
       "   (0.7869947849721863, 0.440132632157844, 0.35035171814502214),\n",
       "   (0.7883054666150409, 0.5671918958574677, 0.43481646997496826),\n",
       "   (0.7841600823183507, 0.5160500325647666, 0.39162717525007334),\n",
       "   (0.7723503928214249, 0.7149055378197117, 0.5495943057339602),\n",
       "   (0.7834357892702085, 0.7796519662864414, 0.5982941947955122),\n",
       "   (0.772707529995692, 0.76403601462235, 0.6080615156690703),\n",
       "   (0.7807413484108868, 0.7424484929085187, 0.5646259239562329),\n",
       "   (0.7913588126544426, 0.9033163519841108, 0.6674142102698504),\n",
       "   (0.7907329385965688, 0.8522351643283277, 0.6304713843162126),\n",
       "   (0.7782384415146624, 0.7788105104643509, 0.5775254644386703),\n",
       "   (0.7692540253072352, 0.5202844526052813, 0.37168383429071283),\n",
       "   (0.7670363566822075, 0.4767476171002822, 0.34558676510788056),\n",
       "   (0.781899981553806, 0.5777379739800251, 0.43310899316953416),\n",
       "   (0.7689505176426353, 0.6078169532166047, 0.45073744698383394),\n",
       "   (0.7691579636580511, 0.4347924801877735, 0.3222163690460494),\n",
       "   (0.7686281512493613, 0.3208482445342125, 0.22647033758762491),\n",
       "   (0.770514024116778, 0.30932135235251407, 0.2198893006438326),\n",
       "   (0.7981874404403041, 1.0142938752054815, 0.7261408958876437)],\n",
       "  0,\n",
       "  0],\n",
       " [[(1.0146891816313626, 1.018183248774548, 1.0274695841628634),\n",
       "   (0.9910580159320898, 1.2423488891747965, 1.2299252368464704),\n",
       "   (0.9914369585113862, 1.1743402427008078, 1.1305174547033425),\n",
       "   (0.9708597281867863, 1.0693789589765712, 1.041228335492268),\n",
       "   (0.9752897614194022, 1.0052445497837295, 0.9405989823856901),\n",
       "   (0.9871760645015668, 0.8572168298471592, 0.8476502332207176),\n",
       "   (0.9958351676393645, 0.8694743958816075, 0.8801622380218815),\n",
       "   (0.9943724374965744, 0.8919754882605474, 0.8952930459535767),\n",
       "   (0.9793644254835643, 0.8368442127142557, 0.8213201835875515),\n",
       "   (0.9835263111678445, 0.777928169266846, 0.7572829778961754),\n",
       "   (0.9795317967005476, 0.6992081633654336, 0.6828425689988216),\n",
       "   (0.9648267030168662, 0.6802276062752266, 0.6611068173195006),\n",
       "   (0.9648001829296681, 0.7055970687099612, 0.6528758695737329),\n",
       "   (0.9889476063263963, 0.7200425242700373, 0.6789790462717957),\n",
       "   (0.996944885954785, 0.8048059105986463, 0.8080294687830256),\n",
       "   (0.9987718252950949, 0.6948530009822059, 0.6994799517977691),\n",
       "   (1.0232156843331701, 1.0960835028989036, 1.11589222547362),\n",
       "   (1.0287100570653342, 0.9645577729057488, 0.9875819153056841),\n",
       "   (1.028447802869709, 0.8303478310264848, 0.84843428319706),\n",
       "   (1.0120418875937265, 0.9130141455569819, 0.9095659861952765),\n",
       "   (1.030082618911651, 0.7707875819458185, 0.7661110321113226),\n",
       "   (1.0346870953845029, 0.764898739538672, 0.7710709697811854),\n",
       "   (1.0264422949422658, 0.8039458823772985, 0.8175795759175741),\n",
       "   (1.0254751957624437, 0.7751499210172839, 0.7981448290565998),\n",
       "   (1.0233671434978346, 0.7039870548202423, 0.7115077231086422),\n",
       "   (1.02193505878914, 0.7129697022840049, 0.7514211377387219),\n",
       "   (1.0134645429380837, 0.7146624873113977, 0.7393626836455371),\n",
       "   (1.0161772531908084, 0.5978954644461211, 0.6333551896562776),\n",
       "   (1.0263998628027489, 0.7021864454889643, 0.7305225005967766),\n",
       "   (1.0361380388218715, 0.781525951818064, 0.8282228659221212),\n",
       "   (1.0423248804975405, 0.9258102241797328, 0.9732034502981906),\n",
       "   (1.041050737641492, 1.0334955612835677, 1.1145604549299162),\n",
       "   (1.047552873687182, 1.0028097386876473, 1.0702144760040626),\n",
       "   (1.0458060839437373, 0.973101120059209, 1.054222873860367),\n",
       "   (1.0432878543304651, 0.9729348383674974, 1.063617238635656),\n",
       "   (1.0179871018082574, 1.0618914983907255, 1.1415943561478907),\n",
       "   (1.01541642135586, 0.8021979890832568, 0.8401414854027145),\n",
       "   (1.0107901394779786, 0.8307680369965673, 0.8614721125158838),\n",
       "   (1.0001938913041812, 0.8493464385749864, 0.8668569880293324),\n",
       "   (0.9895387096032772, 0.6584582751260423, 0.66744453322924),\n",
       "   (0.9982520315860133, 0.7618355985136165, 0.744937424656698),\n",
       "   (0.9876581407533002, 0.5998847989305082, 0.6021605438419956),\n",
       "   (0.9838492668963895, 0.5594191075453264, 0.5704979696865726),\n",
       "   (0.9733325789841863, 0.6631797095798697, 0.6330286255870792),\n",
       "   (0.9769911723469747, 0.5458833950832988, 0.5273709570758164),\n",
       "   (0.9638584251665019, 0.5333171879595527, 0.5047422297407423),\n",
       "   (0.9638572464959598, 0.4441962905133837, 0.4271914772632682),\n",
       "   (0.9499259500231902, 0.5540931784664904, 0.5361615769886581),\n",
       "   (0.9469445028868588, 0.5436978542920031, 0.5236409558404092),\n",
       "   (0.9308238258820728, 0.5839538124003256, 0.5658995213596615),\n",
       "   (0.9403356971571056, 0.5401384344015464, 0.5123432581840437),\n",
       "   (0.9194897299488987, 0.3558295107682075, 0.32547333825277075),\n",
       "   (0.9241124758151538, 0.33684321232741044, 0.315036836030531),\n",
       "   (0.9451570490097105, 0.4778182919951741, 0.4471493353371195),\n",
       "   (0.9415043489996328, 0.3856126799641566, 0.3758259737557254),\n",
       "   (0.9209471560742493, 0.4093519467919464, 0.37796247714243103),\n",
       "   (0.9075521546981513, 0.36541691382746005, 0.33421716877628277),\n",
       "   (0.9166768327000926, 0.5242601204135199, 0.5031628036046603),\n",
       "   (0.9183534915462802, 0.5431906581614647, 0.49374406988891406),\n",
       "   (0.9129162843354095, 0.3979802450565014, 0.3637224338195123),\n",
       "   (0.9183788329629361, 0.3670152275405977, 0.32733315923289924),\n",
       "   (0.9136747588292736, 0.32448817381817124, 0.30798085672170344),\n",
       "   (0.8946374615679736, 0.39969577801085343, 0.36292737608349174),\n",
       "   (0.8984039032853673, 0.32214931286907206, 0.2849037656702479),\n",
       "   (0.9176999187306661, 0.5336216535211485, 0.4772261960327517),\n",
       "   (0.9127164996785176, 0.37087341513714045, 0.34131237197609016),\n",
       "   (0.916950284265868, 0.3183135687339269, 0.2876419261946763),\n",
       "   (0.930948175624268, 0.5650676831292936, 0.5682259982733425),\n",
       "   (0.9235278552262547, 0.5943407855194092, 0.6023078184680397),\n",
       "   (0.9096549029453209, 0.49139754303251004, 0.4745899376379035),\n",
       "   (0.9088056708197123, 0.45565550435024743, 0.41972421978812496),\n",
       "   (0.8941070598240127, 0.39690204551273, 0.36388952038421024),\n",
       "   (0.8803572786147321, 0.43555660175721866, 0.40010575691027866),\n",
       "   (0.8651860207316361, 0.4921572715988522, 0.45089601674387336),\n",
       "   (0.8653469092606377, 0.4460376982299155, 0.39850325622821764),\n",
       "   (0.867543951151178, 0.45106681688127576, 0.4002016565673818),\n",
       "   (0.8478978705548651, 0.4511012214897361, 0.3909951009749236),\n",
       "   (0.8626554150776715, 0.4326692678464446, 0.37196083414266984),\n",
       "   (0.8695146882976285, 0.4728264051324891, 0.40393088568705987),\n",
       "   (0.8518847236636381, 0.4010725712803899, 0.34929983180370494),\n",
       "   (0.8411588217302058, 0.4317508257323506, 0.378146157681628),\n",
       "   (0.8447926630116094, 0.330876731202057, 0.3010324784207792),\n",
       "   (0.8549386590383109, 0.4640846334230289, 0.4282317563288395),\n",
       "   (0.8490724157501031, 0.36073427698519595, 0.3349910757934587),\n",
       "   (0.8414257906079995, 0.3965942308299566, 0.3571415646866677),\n",
       "   (0.8257253096514848, 0.4107166049340993, 0.36374197048436224),\n",
       "   (0.8163360201128341, 0.34559994634446906, 0.2894486178374069),\n",
       "   (0.8301901136650938, 0.4441551441674881, 0.3525914976617162),\n",
       "   (0.8245755165376317, 0.35669571529094773, 0.2866902530602372),\n",
       "   (0.8492722004069949, 0.6011962625895423, 0.48347022080927876),\n",
       "   (0.8493440993100653, 0.4642083334311973, 0.3738285472320232),\n",
       "   (0.8513478392316953, 0.35419705341709096, 0.27922111964509727),\n",
       "   (0.8548314000189766, 0.5254985558328512, 0.4258200854683257),\n",
       "   (0.8476986752332442, 0.3403325181485397, 0.2747518591268355),\n",
       "   (0.8485149045836728, 0.30034875225228397, 0.23926617224312408),\n",
       "   (0.8431348628940959, 0.30998847989305084, 0.2525142276922261),\n",
       "   (0.8233885953017013, 0.36457106500226655, 0.29100416847651533),\n",
       "   (0.824751727783681, 0.30423956115899775, 0.23547200234632307),\n",
       "   (0.8272823334376458, 0.440204181564819, 0.33801746101787944),\n",
       "   (0.8377660185746691, 0.5370137478379683, 0.43146434758408314),\n",
       "   (0.8581487682598168, 0.9570068095027703, 0.7477492088746347),\n",
       "   (0.858977962986209, 0.5603984427717397, 0.4592098684529573),\n",
       "   (0.8533227017250433, 0.5271578496484685, 0.425710492973397),\n",
       "   (0.8533792779110658, 0.4053373073918236, 0.335982950694824),\n",
       "   (0.8294528552409881, 0.55587121385113, 0.449902020287838),\n",
       "   (0.8294281031596031, 0.45576850456867973, 0.35652521350950284),\n",
       "   (0.8178028756025217, 0.35226139186170513, 0.2729157892401061),\n",
       "   (0.8157154500723998, 0.3900588775503014, 0.2995863521052109),\n",
       "   (0.8184747178115388, 0.4290769656753355, 0.31806651820713777),\n",
       "   (0.8293897963669837, 0.48192883804719966, 0.3670258263621642),\n",
       "   (0.8169377314245942, 0.3913380678598063, 0.302334260700141),\n",
       "   (0.8092852129297802, 0.30373206056289764, 0.2438344383885318),\n",
       "   (0.80518402877842, 0.33064625077192894, 0.2799429850322888),\n",
       "   (0.8277243348909465, 0.7435044664662935, 0.5577722600285192),\n",
       "   (0.8186096755886133, 0.4654612092171642, 0.3501711678283859),\n",
       "   (0.8194477103440716, 0.39253122490062026, 0.30371714134027417),\n",
       "   (0.8226224594493133, 0.3458714426352521, 0.26097400175637836),\n",
       "   (0.8255225783182375, 0.29650178638644004, 0.23207707987361928),\n",
       "   (0.8162081343590124, 0.32884137892278864, 0.24747824293074347),\n",
       "   (0.8063261605337492, 0.36081809200479525, 0.2645155652516553),\n",
       "   (0.8064758516926004, 0.3593396942278462, 0.275656137485841),\n",
       "   (0.7982611073491876, 0.32579472253313957, 0.24064157538395908),\n",
       "   (0.8016138357062919, 0.3010104864028725, 0.24191630997817662),\n",
       "   (0.8004445945284934, 0.32393526435831316, 0.26272168650772404),\n",
       "   (0.7899744641027047, 0.35224869129827857, 0.26252994290737014),\n",
       "   (0.7907335279318398, 0.3492159533343024, 0.2536885036708582),\n",
       "   (0.7811126296316596, 0.26939765316205316, 0.20198684321913438),\n",
       "   (0.7924355281946833, 0.4116736271843556, 0.2797488887832073),\n",
       "   (0.7895819668121736, 0.2853709604052976, 0.19621165190214018),\n",
       "   (0.7864219510887085, 0.25362111766176015, 0.1800797901346339),\n",
       "   (0.7782844096658057, 0.29368948148905927, 0.20772702069691648),\n",
       "   (0.7775730819936268, 0.25753654478388294, 0.17559583737913415),\n",
       "   (0.7909244725596657, 0.5068365132151538, 0.3539193092779211),\n",
       "   (0.778784165975671, 0.27168740816557496, 0.18737985054422193),\n",
       "   (0.7825942185031237, 0.3711464772508117, 0.24781722567946646),\n",
       "   (0.7821162675982879, 0.3249996759616523, 0.2088775385278078),\n",
       "   (0.7787859339814842, 0.2517192517837115, 0.1644751951537168),\n",
       "   (0.7804531634633348, 0.2458695549487702, 0.1630497230755085),\n",
       "   (0.7716361184728909, 0.3572331839973249, 0.20845949277613593),\n",
       "   (0.7568874139791505, 0.35169412902537184, 0.22769220236183532),\n",
       "   (0.7585222300210922, 0.2658033502172536, 0.17350010376259636),\n",
       "   (0.7427486714909652, 0.29847367934365576, 0.1963432436965883),\n",
       "   (0.7663462450797871, 0.5485272000398763, 0.3586857617488764),\n",
       "   (0.7986824820680011, 1.2749613698444955, 0.8765064475350962),\n",
       "   (0.8080971130233077, 1.0014707426428289, 0.6945878147969746),\n",
       "   (0.8337337866500238, 1.3520724057381146, 0.9702061747893572),\n",
       "   (0.8623277446669578, 1.8219286623270756, 1.3537547174271256),\n",
       "   (0.8536886789283763, 1.5126060051221546, 1.153005275526316),\n",
       "   (0.8369433065362585, 1.3129156118022334, 1.0039221672209113),\n",
       "   (0.8632459290192813, 0.9722169085732544, 0.7569555523219242),\n",
       "   (0.8460279097397673, 1.7770047687136061, 1.4761348802462038),\n",
       "   (0.8412159872514994, 0.8590164387915918, 0.6644803150353531),\n",
       "   (0.8460432324568149, 0.6142536161592226, 0.4765467662187262),\n",
       "   (0.8368613889335802, 0.5732453236481968, 0.446782946110161),\n",
       "   (0.8231245731002631, 0.5636548759333283, 0.4240154913852049),\n",
       "   (0.838582247925098, 0.6008403423480366, 0.48326117033397437),\n",
       "   (0.8328214956504111, 1.0097308498338444, 0.8462208470641468),\n",
       "   (0.8158987333417019, 0.7433238313981068, 0.5958862438387417),\n",
       "   (0.8172435964302784, 0.6212885539303764, 0.5041107289269383),\n",
       "   (0.7855473775464441, 0.7501931833988326, 0.6004600505259691),\n",
       "   (0.7841211861904602, 0.5890611787010159, 0.46138089959868267),\n",
       "   (0.7869947849721863, 0.440132632157844, 0.35035171814502214),\n",
       "   (0.7883054666150409, 0.5671918958574677, 0.43481646997496826),\n",
       "   (0.7841600823183507, 0.5160500325647666, 0.39162717525007334),\n",
       "   (0.7723503928214249, 0.7149055378197117, 0.5495943057339602),\n",
       "   (0.7834357892702085, 0.7796519662864414, 0.5982941947955122),\n",
       "   (0.772707529995692, 0.76403601462235, 0.6080615156690703),\n",
       "   (0.7807413484108868, 0.7424484929085187, 0.5646259239562329),\n",
       "   (0.7913588126544426, 0.9033163519841108, 0.6674142102698504),\n",
       "   (0.7907329385965688, 0.8522351643283277, 0.6304713843162126),\n",
       "   (0.7782384415146624, 0.7788105104643509, 0.5775254644386703),\n",
       "   (0.7692540253072352, 0.5202844526052813, 0.37168383429071283),\n",
       "   (0.7670363566822075, 0.4767476171002822, 0.34558676510788056),\n",
       "   (0.781899981553806, 0.5777379739800251, 0.43310899316953416),\n",
       "   (0.7689505176426353, 0.6078169532166047, 0.45073744698383394),\n",
       "   (0.7691579636580511, 0.4347924801877735, 0.3222163690460494),\n",
       "   (0.7686281512493613, 0.3208482445342125, 0.22647033758762491),\n",
       "   (0.770514024116778, 0.30932135235251407, 0.2198893006438326),\n",
       "   (0.7981874404403041, 1.0142938752054815, 0.7261408958876437),\n",
       "   (0.7938758635971728, 1.1930654836700767, 0.8652214797216667)],\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll = build_flag(df2,180,15)\n",
    "ll[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [(i[-2],i[-1]) for i in ll]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3405"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(1, 1): 184, (1, 0): 242, (0, 0): 2979})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(1, 1): 179, (1, 0): 235, (0, 0): 2586})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y[:3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(0, 0): 393, (1, 1): 5, (1, 0): 7})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y[3000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/installed/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/ian/installed/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df3 = df2.iloc[180:(df2.shape[0]-15)]\n",
    "df3['y1'] = [i[0] for i in y]\n",
    "df3['y2'] = [i[1] for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_code</th>\n",
       "      <th>trade_date</th>\n",
       "      <th>close</th>\n",
       "      <th>vol</th>\n",
       "      <th>amount</th>\n",
       "      <th>close_1</th>\n",
       "      <th>vol_1</th>\n",
       "      <th>amount_1</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041108</td>\n",
       "      <td>1304.2290</td>\n",
       "      <td>7376656.0</td>\n",
       "      <td>4.398204e+06</td>\n",
       "      <td>0.768628</td>\n",
       "      <td>0.320848</td>\n",
       "      <td>0.226470</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041109</td>\n",
       "      <td>1307.4290</td>\n",
       "      <td>7111640.0</td>\n",
       "      <td>4.270396e+06</td>\n",
       "      <td>0.770514</td>\n",
       "      <td>0.309321</td>\n",
       "      <td>0.219889</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041110</td>\n",
       "      <td>1354.3860</td>\n",
       "      <td>23319738.0</td>\n",
       "      <td>1.410214e+07</td>\n",
       "      <td>0.798187</td>\n",
       "      <td>1.014294</td>\n",
       "      <td>0.726141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041111</td>\n",
       "      <td>1347.0700</td>\n",
       "      <td>27429895.0</td>\n",
       "      <td>1.680318e+07</td>\n",
       "      <td>0.793876</td>\n",
       "      <td>1.193065</td>\n",
       "      <td>0.865221</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041112</td>\n",
       "      <td>1352.2170</td>\n",
       "      <td>15744105.0</td>\n",
       "      <td>9.490678e+06</td>\n",
       "      <td>0.796909</td>\n",
       "      <td>0.684791</td>\n",
       "      <td>0.488690</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041115</td>\n",
       "      <td>1370.0460</td>\n",
       "      <td>14230535.0</td>\n",
       "      <td>8.929426e+06</td>\n",
       "      <td>0.807416</td>\n",
       "      <td>0.618958</td>\n",
       "      <td>0.459790</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041116</td>\n",
       "      <td>1370.3890</td>\n",
       "      <td>14165607.0</td>\n",
       "      <td>8.616093e+06</td>\n",
       "      <td>0.807619</td>\n",
       "      <td>0.616134</td>\n",
       "      <td>0.443656</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041117</td>\n",
       "      <td>1356.1380</td>\n",
       "      <td>12234261.0</td>\n",
       "      <td>7.593393e+06</td>\n",
       "      <td>0.799220</td>\n",
       "      <td>0.532130</td>\n",
       "      <td>0.390996</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041118</td>\n",
       "      <td>1367.8280</td>\n",
       "      <td>11589095.0</td>\n",
       "      <td>7.315559e+06</td>\n",
       "      <td>0.806109</td>\n",
       "      <td>0.504069</td>\n",
       "      <td>0.376689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041119</td>\n",
       "      <td>1379.9600</td>\n",
       "      <td>14107904.0</td>\n",
       "      <td>8.670277e+06</td>\n",
       "      <td>0.813259</td>\n",
       "      <td>0.613624</td>\n",
       "      <td>0.446446</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041122</td>\n",
       "      <td>1383.0200</td>\n",
       "      <td>16667439.0</td>\n",
       "      <td>1.027390e+07</td>\n",
       "      <td>0.815062</td>\n",
       "      <td>0.724952</td>\n",
       "      <td>0.529019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041123</td>\n",
       "      <td>1371.2440</td>\n",
       "      <td>16328520.0</td>\n",
       "      <td>1.003912e+07</td>\n",
       "      <td>0.808122</td>\n",
       "      <td>0.710210</td>\n",
       "      <td>0.516930</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1607</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041124</td>\n",
       "      <td>1359.1250</td>\n",
       "      <td>16355434.0</td>\n",
       "      <td>9.764013e+06</td>\n",
       "      <td>0.800980</td>\n",
       "      <td>0.711381</td>\n",
       "      <td>0.502764</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1606</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041125</td>\n",
       "      <td>1358.3330</td>\n",
       "      <td>13425338.0</td>\n",
       "      <td>7.758391e+06</td>\n",
       "      <td>0.800514</td>\n",
       "      <td>0.583936</td>\n",
       "      <td>0.399492</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041126</td>\n",
       "      <td>1356.7270</td>\n",
       "      <td>10434933.0</td>\n",
       "      <td>6.121806e+06</td>\n",
       "      <td>0.799567</td>\n",
       "      <td>0.453868</td>\n",
       "      <td>0.315221</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041129</td>\n",
       "      <td>1337.4340</td>\n",
       "      <td>10083151.0</td>\n",
       "      <td>6.080041e+06</td>\n",
       "      <td>0.788197</td>\n",
       "      <td>0.438567</td>\n",
       "      <td>0.313071</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041130</td>\n",
       "      <td>1340.7710</td>\n",
       "      <td>9277245.0</td>\n",
       "      <td>5.475046e+06</td>\n",
       "      <td>0.790164</td>\n",
       "      <td>0.403515</td>\n",
       "      <td>0.281919</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041201</td>\n",
       "      <td>1334.9440</td>\n",
       "      <td>9553429.0</td>\n",
       "      <td>5.797059e+06</td>\n",
       "      <td>0.786730</td>\n",
       "      <td>0.415527</td>\n",
       "      <td>0.298500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041202</td>\n",
       "      <td>1333.0900</td>\n",
       "      <td>12804483.0</td>\n",
       "      <td>7.714800e+06</td>\n",
       "      <td>0.785637</td>\n",
       "      <td>0.556932</td>\n",
       "      <td>0.397247</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041203</td>\n",
       "      <td>1337.1970</td>\n",
       "      <td>13505016.0</td>\n",
       "      <td>8.201370e+06</td>\n",
       "      <td>0.788057</td>\n",
       "      <td>0.587402</td>\n",
       "      <td>0.422301</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041206</td>\n",
       "      <td>1339.6440</td>\n",
       "      <td>10664785.0</td>\n",
       "      <td>6.326780e+06</td>\n",
       "      <td>0.789499</td>\n",
       "      <td>0.463866</td>\n",
       "      <td>0.325776</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041207</td>\n",
       "      <td>1323.7520</td>\n",
       "      <td>10236805.0</td>\n",
       "      <td>5.960763e+06</td>\n",
       "      <td>0.780134</td>\n",
       "      <td>0.445251</td>\n",
       "      <td>0.306929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041208</td>\n",
       "      <td>1326.4380</td>\n",
       "      <td>8593358.0</td>\n",
       "      <td>4.929901e+06</td>\n",
       "      <td>0.781717</td>\n",
       "      <td>0.373769</td>\n",
       "      <td>0.253848</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041209</td>\n",
       "      <td>1338.8100</td>\n",
       "      <td>15257911.0</td>\n",
       "      <td>8.334091e+06</td>\n",
       "      <td>0.789008</td>\n",
       "      <td>0.663644</td>\n",
       "      <td>0.429135</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041210</td>\n",
       "      <td>1317.7170</td>\n",
       "      <td>11072386.0</td>\n",
       "      <td>6.280951e+06</td>\n",
       "      <td>0.776577</td>\n",
       "      <td>0.481594</td>\n",
       "      <td>0.323416</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041213</td>\n",
       "      <td>1309.6950</td>\n",
       "      <td>8549453.0</td>\n",
       "      <td>4.859195e+06</td>\n",
       "      <td>0.771849</td>\n",
       "      <td>0.371859</td>\n",
       "      <td>0.250207</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041214</td>\n",
       "      <td>1307.5530</td>\n",
       "      <td>8045982.0</td>\n",
       "      <td>4.652200e+06</td>\n",
       "      <td>0.770587</td>\n",
       "      <td>0.349961</td>\n",
       "      <td>0.239549</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041215</td>\n",
       "      <td>1313.0450</td>\n",
       "      <td>11405089.0</td>\n",
       "      <td>6.514900e+06</td>\n",
       "      <td>0.773824</td>\n",
       "      <td>0.496065</td>\n",
       "      <td>0.335462</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041216</td>\n",
       "      <td>1305.0180</td>\n",
       "      <td>9058802.0</td>\n",
       "      <td>5.060281e+06</td>\n",
       "      <td>0.769093</td>\n",
       "      <td>0.394013</td>\n",
       "      <td>0.260562</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20041217</td>\n",
       "      <td>1290.4900</td>\n",
       "      <td>7662099.0</td>\n",
       "      <td>4.229595e+06</td>\n",
       "      <td>0.760531</td>\n",
       "      <td>0.333264</td>\n",
       "      <td>0.217788</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20180919</td>\n",
       "      <td>2730.8503</td>\n",
       "      <td>141550318.0</td>\n",
       "      <td>1.401070e+08</td>\n",
       "      <td>1.609386</td>\n",
       "      <td>6.156742</td>\n",
       "      <td>7.214327</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20180920</td>\n",
       "      <td>2729.2438</td>\n",
       "      <td>111387463.0</td>\n",
       "      <td>1.048759e+08</td>\n",
       "      <td>1.608440</td>\n",
       "      <td>4.844807</td>\n",
       "      <td>5.400221</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20180921</td>\n",
       "      <td>2797.4848</td>\n",
       "      <td>158016642.0</td>\n",
       "      <td>1.489764e+08</td>\n",
       "      <td>1.648656</td>\n",
       "      <td>6.872947</td>\n",
       "      <td>7.671023</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20180925</td>\n",
       "      <td>2781.1385</td>\n",
       "      <td>111983179.0</td>\n",
       "      <td>1.085280e+08</td>\n",
       "      <td>1.639023</td>\n",
       "      <td>4.870717</td>\n",
       "      <td>5.588277</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20180926</td>\n",
       "      <td>2806.8133</td>\n",
       "      <td>139567706.0</td>\n",
       "      <td>1.450588e+08</td>\n",
       "      <td>1.654154</td>\n",
       "      <td>6.070509</td>\n",
       "      <td>7.469300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20180927</td>\n",
       "      <td>2791.7748</td>\n",
       "      <td>123997696.0</td>\n",
       "      <td>1.234477e+08</td>\n",
       "      <td>1.645291</td>\n",
       "      <td>5.393290</td>\n",
       "      <td>6.356513</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20180928</td>\n",
       "      <td>2821.3501</td>\n",
       "      <td>134290456.0</td>\n",
       "      <td>1.253700e+08</td>\n",
       "      <td>1.662721</td>\n",
       "      <td>5.840974</td>\n",
       "      <td>6.455495</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181008</td>\n",
       "      <td>2716.5104</td>\n",
       "      <td>149501388.0</td>\n",
       "      <td>1.415316e+08</td>\n",
       "      <td>1.600935</td>\n",
       "      <td>6.502575</td>\n",
       "      <td>7.287678</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181009</td>\n",
       "      <td>2721.0130</td>\n",
       "      <td>116771899.0</td>\n",
       "      <td>1.102925e+08</td>\n",
       "      <td>1.603589</td>\n",
       "      <td>5.079003</td>\n",
       "      <td>5.679129</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181010</td>\n",
       "      <td>2725.8367</td>\n",
       "      <td>113485736.0</td>\n",
       "      <td>1.113125e+08</td>\n",
       "      <td>1.606432</td>\n",
       "      <td>4.936071</td>\n",
       "      <td>5.731650</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181011</td>\n",
       "      <td>2583.4575</td>\n",
       "      <td>197150702.0</td>\n",
       "      <td>1.700578e+08</td>\n",
       "      <td>1.522523</td>\n",
       "      <td>8.575086</td>\n",
       "      <td>8.756537</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181012</td>\n",
       "      <td>2606.9125</td>\n",
       "      <td>170081635.0</td>\n",
       "      <td>1.428032e+08</td>\n",
       "      <td>1.536345</td>\n",
       "      <td>7.397714</td>\n",
       "      <td>7.353158</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181015</td>\n",
       "      <td>2568.0984</td>\n",
       "      <td>118576197.0</td>\n",
       "      <td>1.052147e+08</td>\n",
       "      <td>1.513471</td>\n",
       "      <td>5.157481</td>\n",
       "      <td>5.417670</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181016</td>\n",
       "      <td>2546.3296</td>\n",
       "      <td>119419144.0</td>\n",
       "      <td>1.067922e+08</td>\n",
       "      <td>1.500642</td>\n",
       "      <td>5.194145</td>\n",
       "      <td>5.498898</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181017</td>\n",
       "      <td>2561.6140</td>\n",
       "      <td>129953997.0</td>\n",
       "      <td>1.173884e+08</td>\n",
       "      <td>1.509649</td>\n",
       "      <td>5.652360</td>\n",
       "      <td>6.044508</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181018</td>\n",
       "      <td>2486.4186</td>\n",
       "      <td>125428513.0</td>\n",
       "      <td>1.065768e+08</td>\n",
       "      <td>1.465334</td>\n",
       "      <td>5.455523</td>\n",
       "      <td>5.487802</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181019</td>\n",
       "      <td>2550.4652</td>\n",
       "      <td>147323558.0</td>\n",
       "      <td>1.300959e+08</td>\n",
       "      <td>1.503079</td>\n",
       "      <td>6.407850</td>\n",
       "      <td>6.698840</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181022</td>\n",
       "      <td>2654.8762</td>\n",
       "      <td>211888449.0</td>\n",
       "      <td>1.973098e+08</td>\n",
       "      <td>1.564612</td>\n",
       "      <td>9.216105</td>\n",
       "      <td>10.159787</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181023</td>\n",
       "      <td>2594.8255</td>\n",
       "      <td>178343543.0</td>\n",
       "      <td>1.669194e+08</td>\n",
       "      <td>1.529222</td>\n",
       "      <td>7.757067</td>\n",
       "      <td>8.594939</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181024</td>\n",
       "      <td>2603.2951</td>\n",
       "      <td>160060407.0</td>\n",
       "      <td>1.420822e+08</td>\n",
       "      <td>1.534214</td>\n",
       "      <td>6.961840</td>\n",
       "      <td>7.316030</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181025</td>\n",
       "      <td>2603.7995</td>\n",
       "      <td>162032979.0</td>\n",
       "      <td>1.376302e+08</td>\n",
       "      <td>1.534511</td>\n",
       "      <td>7.047637</td>\n",
       "      <td>7.086791</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181026</td>\n",
       "      <td>2598.8468</td>\n",
       "      <td>158936119.0</td>\n",
       "      <td>1.334155e+08</td>\n",
       "      <td>1.531592</td>\n",
       "      <td>6.912939</td>\n",
       "      <td>6.869770</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181029</td>\n",
       "      <td>2542.1033</td>\n",
       "      <td>134241923.0</td>\n",
       "      <td>1.225000e+08</td>\n",
       "      <td>1.498151</td>\n",
       "      <td>5.838863</td>\n",
       "      <td>6.307716</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181030</td>\n",
       "      <td>2568.0481</td>\n",
       "      <td>166682309.0</td>\n",
       "      <td>1.523859e+08</td>\n",
       "      <td>1.513441</td>\n",
       "      <td>7.249860</td>\n",
       "      <td>7.846587</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181031</td>\n",
       "      <td>2602.7832</td>\n",
       "      <td>180550348.0</td>\n",
       "      <td>1.548947e+08</td>\n",
       "      <td>1.533912</td>\n",
       "      <td>7.853052</td>\n",
       "      <td>7.975769</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181101</td>\n",
       "      <td>2606.2372</td>\n",
       "      <td>200903932.0</td>\n",
       "      <td>1.802538e+08</td>\n",
       "      <td>1.535948</td>\n",
       "      <td>8.738333</td>\n",
       "      <td>9.281547</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181102</td>\n",
       "      <td>2676.4762</td>\n",
       "      <td>225026921.0</td>\n",
       "      <td>2.078616e+08</td>\n",
       "      <td>1.577342</td>\n",
       "      <td>9.787564</td>\n",
       "      <td>10.703114</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181105</td>\n",
       "      <td>2665.4306</td>\n",
       "      <td>193760100.0</td>\n",
       "      <td>1.685889e+08</td>\n",
       "      <td>1.570832</td>\n",
       "      <td>8.427611</td>\n",
       "      <td>8.680904</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181106</td>\n",
       "      <td>2659.3564</td>\n",
       "      <td>163669479.0</td>\n",
       "      <td>1.388009e+08</td>\n",
       "      <td>1.567253</td>\n",
       "      <td>7.118817</td>\n",
       "      <td>7.147075</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181107</td>\n",
       "      <td>2641.3420</td>\n",
       "      <td>173201986.0</td>\n",
       "      <td>1.472618e+08</td>\n",
       "      <td>1.556636</td>\n",
       "      <td>7.533434</td>\n",
       "      <td>7.582738</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3405 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ts_code trade_date      close          vol        amount   close_1  \\\n",
       "1619  000001.SH   20041108  1304.2290    7376656.0  4.398204e+06  0.768628   \n",
       "1618  000001.SH   20041109  1307.4290    7111640.0  4.270396e+06  0.770514   \n",
       "1617  000001.SH   20041110  1354.3860   23319738.0  1.410214e+07  0.798187   \n",
       "1616  000001.SH   20041111  1347.0700   27429895.0  1.680318e+07  0.793876   \n",
       "1615  000001.SH   20041112  1352.2170   15744105.0  9.490678e+06  0.796909   \n",
       "1614  000001.SH   20041115  1370.0460   14230535.0  8.929426e+06  0.807416   \n",
       "1613  000001.SH   20041116  1370.3890   14165607.0  8.616093e+06  0.807619   \n",
       "1612  000001.SH   20041117  1356.1380   12234261.0  7.593393e+06  0.799220   \n",
       "1611  000001.SH   20041118  1367.8280   11589095.0  7.315559e+06  0.806109   \n",
       "1610  000001.SH   20041119  1379.9600   14107904.0  8.670277e+06  0.813259   \n",
       "1609  000001.SH   20041122  1383.0200   16667439.0  1.027390e+07  0.815062   \n",
       "1608  000001.SH   20041123  1371.2440   16328520.0  1.003912e+07  0.808122   \n",
       "1607  000001.SH   20041124  1359.1250   16355434.0  9.764013e+06  0.800980   \n",
       "1606  000001.SH   20041125  1358.3330   13425338.0  7.758391e+06  0.800514   \n",
       "1605  000001.SH   20041126  1356.7270   10434933.0  6.121806e+06  0.799567   \n",
       "1604  000001.SH   20041129  1337.4340   10083151.0  6.080041e+06  0.788197   \n",
       "1603  000001.SH   20041130  1340.7710    9277245.0  5.475046e+06  0.790164   \n",
       "1602  000001.SH   20041201  1334.9440    9553429.0  5.797059e+06  0.786730   \n",
       "1601  000001.SH   20041202  1333.0900   12804483.0  7.714800e+06  0.785637   \n",
       "1600  000001.SH   20041203  1337.1970   13505016.0  8.201370e+06  0.788057   \n",
       "1599  000001.SH   20041206  1339.6440   10664785.0  6.326780e+06  0.789499   \n",
       "1598  000001.SH   20041207  1323.7520   10236805.0  5.960763e+06  0.780134   \n",
       "1597  000001.SH   20041208  1326.4380    8593358.0  4.929901e+06  0.781717   \n",
       "1596  000001.SH   20041209  1338.8100   15257911.0  8.334091e+06  0.789008   \n",
       "1595  000001.SH   20041210  1317.7170   11072386.0  6.280951e+06  0.776577   \n",
       "1594  000001.SH   20041213  1309.6950    8549453.0  4.859195e+06  0.771849   \n",
       "1593  000001.SH   20041214  1307.5530    8045982.0  4.652200e+06  0.770587   \n",
       "1592  000001.SH   20041215  1313.0450   11405089.0  6.514900e+06  0.773824   \n",
       "1591  000001.SH   20041216  1305.0180    9058802.0  5.060281e+06  0.769093   \n",
       "1590  000001.SH   20041217  1290.4900    7662099.0  4.229595e+06  0.760531   \n",
       "...         ...        ...        ...          ...           ...       ...   \n",
       "44    000001.SH   20180919  2730.8503  141550318.0  1.401070e+08  1.609386   \n",
       "43    000001.SH   20180920  2729.2438  111387463.0  1.048759e+08  1.608440   \n",
       "42    000001.SH   20180921  2797.4848  158016642.0  1.489764e+08  1.648656   \n",
       "41    000001.SH   20180925  2781.1385  111983179.0  1.085280e+08  1.639023   \n",
       "40    000001.SH   20180926  2806.8133  139567706.0  1.450588e+08  1.654154   \n",
       "39    000001.SH   20180927  2791.7748  123997696.0  1.234477e+08  1.645291   \n",
       "38    000001.SH   20180928  2821.3501  134290456.0  1.253700e+08  1.662721   \n",
       "37    000001.SH   20181008  2716.5104  149501388.0  1.415316e+08  1.600935   \n",
       "36    000001.SH   20181009  2721.0130  116771899.0  1.102925e+08  1.603589   \n",
       "35    000001.SH   20181010  2725.8367  113485736.0  1.113125e+08  1.606432   \n",
       "34    000001.SH   20181011  2583.4575  197150702.0  1.700578e+08  1.522523   \n",
       "33    000001.SH   20181012  2606.9125  170081635.0  1.428032e+08  1.536345   \n",
       "32    000001.SH   20181015  2568.0984  118576197.0  1.052147e+08  1.513471   \n",
       "31    000001.SH   20181016  2546.3296  119419144.0  1.067922e+08  1.500642   \n",
       "30    000001.SH   20181017  2561.6140  129953997.0  1.173884e+08  1.509649   \n",
       "29    000001.SH   20181018  2486.4186  125428513.0  1.065768e+08  1.465334   \n",
       "28    000001.SH   20181019  2550.4652  147323558.0  1.300959e+08  1.503079   \n",
       "27    000001.SH   20181022  2654.8762  211888449.0  1.973098e+08  1.564612   \n",
       "26    000001.SH   20181023  2594.8255  178343543.0  1.669194e+08  1.529222   \n",
       "25    000001.SH   20181024  2603.2951  160060407.0  1.420822e+08  1.534214   \n",
       "24    000001.SH   20181025  2603.7995  162032979.0  1.376302e+08  1.534511   \n",
       "23    000001.SH   20181026  2598.8468  158936119.0  1.334155e+08  1.531592   \n",
       "22    000001.SH   20181029  2542.1033  134241923.0  1.225000e+08  1.498151   \n",
       "21    000001.SH   20181030  2568.0481  166682309.0  1.523859e+08  1.513441   \n",
       "20    000001.SH   20181031  2602.7832  180550348.0  1.548947e+08  1.533912   \n",
       "19    000001.SH   20181101  2606.2372  200903932.0  1.802538e+08  1.535948   \n",
       "18    000001.SH   20181102  2676.4762  225026921.0  2.078616e+08  1.577342   \n",
       "17    000001.SH   20181105  2665.4306  193760100.0  1.685889e+08  1.570832   \n",
       "16    000001.SH   20181106  2659.3564  163669479.0  1.388009e+08  1.567253   \n",
       "15    000001.SH   20181107  2641.3420  173201986.0  1.472618e+08  1.556636   \n",
       "\n",
       "         vol_1   amount_1  y1  y2  \n",
       "1619  0.320848   0.226470   1   1  \n",
       "1618  0.309321   0.219889   1   0  \n",
       "1617  1.014294   0.726141   0   0  \n",
       "1616  1.193065   0.865221   0   0  \n",
       "1615  0.684791   0.488690   0   0  \n",
       "1614  0.618958   0.459790   0   0  \n",
       "1613  0.616134   0.443656   0   0  \n",
       "1612  0.532130   0.390996   0   0  \n",
       "1611  0.504069   0.376689   0   0  \n",
       "1610  0.613624   0.446446   0   0  \n",
       "1609  0.724952   0.529019   0   0  \n",
       "1608  0.710210   0.516930   0   0  \n",
       "1607  0.711381   0.502764   0   0  \n",
       "1606  0.583936   0.399492   0   0  \n",
       "1605  0.453868   0.315221   0   0  \n",
       "1604  0.438567   0.313071   0   0  \n",
       "1603  0.403515   0.281919   0   0  \n",
       "1602  0.415527   0.298500   0   0  \n",
       "1601  0.556932   0.397247   0   0  \n",
       "1600  0.587402   0.422301   0   0  \n",
       "1599  0.463866   0.325776   0   0  \n",
       "1598  0.445251   0.306929   0   0  \n",
       "1597  0.373769   0.253848   0   0  \n",
       "1596  0.663644   0.429135   0   0  \n",
       "1595  0.481594   0.323416   0   0  \n",
       "1594  0.371859   0.250207   0   0  \n",
       "1593  0.349961   0.239549   0   0  \n",
       "1592  0.496065   0.335462   0   0  \n",
       "1591  0.394013   0.260562   0   0  \n",
       "1590  0.333264   0.217788   0   0  \n",
       "...        ...        ...  ..  ..  \n",
       "44    6.156742   7.214327   0   0  \n",
       "43    4.844807   5.400221   0   0  \n",
       "42    6.872947   7.671023   0   0  \n",
       "41    4.870717   5.588277   0   0  \n",
       "40    6.070509   7.469300   0   0  \n",
       "39    5.393290   6.356513   0   0  \n",
       "38    5.840974   6.455495   0   0  \n",
       "37    6.502575   7.287678   0   0  \n",
       "36    5.079003   5.679129   0   0  \n",
       "35    4.936071   5.731650   0   0  \n",
       "34    8.575086   8.756537   0   0  \n",
       "33    7.397714   7.353158   0   0  \n",
       "32    5.157481   5.417670   0   0  \n",
       "31    5.194145   5.498898   0   0  \n",
       "30    5.652360   6.044508   0   0  \n",
       "29    5.455523   5.487802   1   1  \n",
       "28    6.407850   6.698840   0   0  \n",
       "27    9.216105  10.159787   0   0  \n",
       "26    7.757067   8.594939   0   0  \n",
       "25    6.961840   7.316030   0   0  \n",
       "24    7.047637   7.086791   0   0  \n",
       "23    6.912939   6.869770   0   0  \n",
       "22    5.838863   6.307716   1   1  \n",
       "21    7.249860   7.846587   1   0  \n",
       "20    7.853052   7.975769   0   0  \n",
       "19    8.738333   9.281547   0   0  \n",
       "18    9.787564  10.703114   0   0  \n",
       "17    8.427611   8.680904   0   0  \n",
       "16    7.118817   7.147075   0   0  \n",
       "15    7.533434   7.582738   0   0  \n",
       "\n",
       "[3405 rows x 10 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3405, 180, 3), (3405, 1), (3405, 1))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([i[0] for i in ll])\n",
    "y1 = np.array([[i[1]] for i in ll])#买点\n",
    "y2 = np.array([[i[2]] for i in ll])#最低点\n",
    "\n",
    "x.shape, y1.shape, y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x.reshape(x.shape[0],-1)\n",
    "x_train, x_test = x1[:3200], x1[3200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3405, 540)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3405, 2)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y11 = np_utils.to_categorical(y1,num_classes=2)\n",
    "y_train, y_test = y11[:3200], y11[3200:]\n",
    "y11.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用BP做预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[i[1]] for i in ll])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 100)               9100      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 500)               50500     \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 2)                 1002      \n",
      "=================================================================\n",
      "Total params: 311,102\n",
      "Trainable params: 311,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 创建模型，输入784个神经元，输出10个神经元\n",
    "model = Sequential()\n",
    "model.add(Dense(units=100,input_dim=90,bias_initializer='one',activation='tanh'))\n",
    "model.add(Dense(units=500,bias_initializer='one',activation='tanh'))\n",
    "model.add(Dense(units=500,bias_initializer='one',activation='tanh'))\n",
    "model.add(Dense(units=2,activation='tanh'))\n",
    "\n",
    "# 定义优化器\n",
    "sgd = SGD(lr=0.2)\n",
    "\n",
    "# 定义优化器，loss function，训练过程中计算准确率\n",
    "model.compile(\n",
    "    optimizer = sgd,\n",
    "    loss = 'mse',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3200/3200 [==============================] - 0s 132us/step - loss: 0.5184 - acc: 0.8687\n",
      "Epoch 2/100\n",
      "3200/3200 [==============================] - 0s 49us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 3/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 4/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 5/100\n",
      "3200/3200 [==============================] - 0s 40us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 6/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 7/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 8/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 9/100\n",
      "3200/3200 [==============================] - 0s 49us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 10/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 11/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 12/100\n",
      "3200/3200 [==============================] - 0s 50us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 13/100\n",
      "3200/3200 [==============================] - 0s 52us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 14/100\n",
      "3200/3200 [==============================] - 0s 39us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 15/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 16/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 17/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 18/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 19/100\n",
      "3200/3200 [==============================] - 0s 51us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 20/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 21/100\n",
      "3200/3200 [==============================] - 0s 49us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 22/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 23/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 24/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 25/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 26/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 27/100\n",
      "3200/3200 [==============================] - 0s 49us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 28/100\n",
      "3200/3200 [==============================] - 0s 49us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 29/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 30/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 31/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 32/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 33/100\n",
      "3200/3200 [==============================] - 0s 44us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 34/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 35/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 36/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 37/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 38/100\n",
      "3200/3200 [==============================] - 0s 45us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 39/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 40/100\n",
      "3200/3200 [==============================] - 0s 50us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 41/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 42/100\n",
      "3200/3200 [==============================] - 0s 49us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 43/100\n",
      "3200/3200 [==============================] - 0s 51us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 44/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 45/100\n",
      "3200/3200 [==============================] - 0s 50us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 46/100\n",
      "3200/3200 [==============================] - 0s 49us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 47/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 48/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 49/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 50/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 51/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 52/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 53/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 54/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 55/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 56/100\n",
      "3200/3200 [==============================] - 0s 51us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 57/100\n",
      "3200/3200 [==============================] - 0s 49us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 58/100\n",
      "3200/3200 [==============================] - 0s 51us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 59/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 60/100\n",
      "3200/3200 [==============================] - 0s 57us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 61/100\n",
      "3200/3200 [==============================] - 0s 37us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 62/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 63/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 64/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 65/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 66/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 67/100\n",
      "3200/3200 [==============================] - 0s 49us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 68/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 69/100\n",
      "3200/3200 [==============================] - 0s 49us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 70/100\n",
      "3200/3200 [==============================] - 0s 45us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 71/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 72/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 73/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 74/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 75/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 76/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 77/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 78/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 79/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 80/100\n",
      "3200/3200 [==============================] - 0s 38us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 81/100\n",
      "3200/3200 [==============================] - 0s 52us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 82/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 84/100\n",
      "3200/3200 [==============================] - 0s 51us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 85/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 86/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 87/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 88/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 89/100\n",
      "3200/3200 [==============================] - 0s 43us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 90/100\n",
      "3200/3200 [==============================] - 0s 44us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 91/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 92/100\n",
      "3200/3200 [==============================] - 0s 49us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 93/100\n",
      "3200/3200 [==============================] - 0s 49us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 94/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 95/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 96/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 97/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 98/100\n",
      "3200/3200 [==============================] - 0s 55us/step - loss: 0.5000 - acc: 0.8687: 0s - loss: 0.5000 - acc: 0.870\n",
      "Epoch 99/100\n",
      "3200/3200 [==============================] - 0s 39us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 100/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "355/355 [==============================] - 0s 252us/step\n",
      "\n",
      "test loss 0.5\n",
      "accuracy 0.9661971830985916\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "model.fit(x_train,y_train,batch_size=128,epochs=100)\n",
    "\n",
    "# 评估模型\n",
    "loss,accuracy = model.evaluate(x_test,y_test)\n",
    "\n",
    "print('\\ntest loss',loss)\n",
    "print('accuracy',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense,Dropout,Convolution2D,MaxPooling2D,Flatten\n",
    "from keras.optimizers import Adam\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 10, 9, 64)         1664      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 5, 5, 126)         201726    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 126)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 256)         806656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 2,061,696\n",
      "Trainable params: 2,061,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 定义顺序模型\n",
    "model = Sequential()\n",
    "\n",
    "# 第一个卷积层\n",
    "# input_shape 输入平面\n",
    "# filters 卷积核/滤波器个数\n",
    "# kernel_size 卷积窗口大小\n",
    "# strides 步长\n",
    "# padding padding方式 same/valid\n",
    "# activation 激活函数\n",
    "model.add(Convolution2D(\n",
    "    input_shape = (10,9,1),\n",
    "    filters = 64,\n",
    "    kernel_size = 5,\n",
    "    strides = 1,\n",
    "    padding = 'same',\n",
    "    activation = 'relu'\n",
    "))\n",
    "# 第一个池化层\n",
    "model.add(MaxPooling2D(\n",
    "    pool_size = 2,\n",
    "    strides = 2,\n",
    "    padding = 'same',\n",
    "))\n",
    "# 第二个卷积层\n",
    "model.add(Convolution2D(126,5,strides=1,padding='same',activation = 'relu'))\n",
    "# 第二个池化层\n",
    "model.add(MaxPooling2D(2,2,'same'))\n",
    "\n",
    "# 第三个卷积层\n",
    "model.add(Convolution2D(256,5,strides=1,padding='same',activation = 'relu'))\n",
    "# 第三个池化层\n",
    "model.add(MaxPooling2D(2,2,'same'))\n",
    "\n",
    "# 把第三个池化层的输出扁平化为1维\n",
    "model.add(Flatten())\n",
    "# 第一个全连接层\n",
    "model.add(Dense(1024,activation = 'relu'))\n",
    "# Dropout\n",
    "model.add(Dropout(0.5))\n",
    "# 第二个全连接层\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "# 定义优化器\n",
    "adam = Adam(lr=1e-4)\n",
    "\n",
    "# 定义优化器，loss function，训练过程中计算准确率\n",
    "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/266\n",
      "3200/3200 [==============================] - 2s 739us/step - loss: 0.5231 - acc: 0.8419\n",
      "Epoch 2/266\n",
      "3200/3200 [==============================] - 2s 534us/step - loss: 0.4764 - acc: 0.8687\n",
      "Epoch 3/266\n",
      "3200/3200 [==============================] - 2s 588us/step - loss: 0.4567 - acc: 0.8687\n",
      "Epoch 4/266\n",
      "3200/3200 [==============================] - 2s 545us/step - loss: 0.4319 - acc: 0.8694\n",
      "Epoch 5/266\n",
      "3200/3200 [==============================] - 2s 539us/step - loss: 0.4317 - acc: 0.8663\n",
      "Epoch 6/266\n",
      "3200/3200 [==============================] - 2s 545us/step - loss: 0.4137 - acc: 0.8684\n",
      "Epoch 7/266\n",
      "3200/3200 [==============================] - 2s 529us/step - loss: 0.4128 - acc: 0.8675\n",
      "Epoch 8/266\n",
      "3200/3200 [==============================] - 2s 522us/step - loss: 0.4165 - acc: 0.8678\n",
      "Epoch 9/266\n",
      "3200/3200 [==============================] - 2s 543us/step - loss: 0.4117 - acc: 0.8669\n",
      "Epoch 10/266\n",
      "3200/3200 [==============================] - 2s 527us/step - loss: 0.4056 - acc: 0.8687\n",
      "Epoch 11/266\n",
      "3200/3200 [==============================] - 2s 539us/step - loss: 0.3955 - acc: 0.8684\n",
      "Epoch 12/266\n",
      "3200/3200 [==============================] - 2s 545us/step - loss: 0.3922 - acc: 0.8678\n",
      "Epoch 13/266\n",
      "3200/3200 [==============================] - 2s 538us/step - loss: 0.3978 - acc: 0.8687\n",
      "Epoch 14/266\n",
      "3200/3200 [==============================] - 2s 529us/step - loss: 0.3887 - acc: 0.8687\n",
      "Epoch 15/266\n",
      "3200/3200 [==============================] - 2s 535us/step - loss: 0.3894 - acc: 0.8678\n",
      "Epoch 16/266\n",
      "3200/3200 [==============================] - 2s 525us/step - loss: 0.3837 - acc: 0.8687\n",
      "Epoch 17/266\n",
      "3200/3200 [==============================] - 2s 532us/step - loss: 0.3848 - acc: 0.8678\n",
      "Epoch 18/266\n",
      "3200/3200 [==============================] - 2s 537us/step - loss: 0.3762 - acc: 0.8684\n",
      "Epoch 19/266\n",
      "3200/3200 [==============================] - 2s 530us/step - loss: 0.3815 - acc: 0.8678\n",
      "Epoch 20/266\n",
      "3200/3200 [==============================] - 2s 544us/step - loss: 0.3796 - acc: 0.8703\n",
      "Epoch 21/266\n",
      "3200/3200 [==============================] - 2s 540us/step - loss: 0.3734 - acc: 0.8678\n",
      "Epoch 22/266\n",
      "3200/3200 [==============================] - 2s 547us/step - loss: 0.3757 - acc: 0.8694\n",
      "Epoch 23/266\n",
      "3200/3200 [==============================] - 2s 541us/step - loss: 0.3713 - acc: 0.8697\n",
      "Epoch 24/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.3705 - acc: 0.8703\n",
      "Epoch 25/266\n",
      "3200/3200 [==============================] - 2s 534us/step - loss: 0.3679 - acc: 0.8700\n",
      "Epoch 26/266\n",
      "3200/3200 [==============================] - 2s 555us/step - loss: 0.3664 - acc: 0.8691\n",
      "Epoch 27/266\n",
      "3200/3200 [==============================] - 2s 526us/step - loss: 0.3700 - acc: 0.8706\n",
      "Epoch 28/266\n",
      "3200/3200 [==============================] - 2s 557us/step - loss: 0.3674 - acc: 0.8712\n",
      "Epoch 29/266\n",
      "3200/3200 [==============================] - 2s 544us/step - loss: 0.3649 - acc: 0.8703\n",
      "Epoch 30/266\n",
      "3200/3200 [==============================] - 2s 517us/step - loss: 0.3616 - acc: 0.8716\n",
      "Epoch 31/266\n",
      "3200/3200 [==============================] - 2s 546us/step - loss: 0.3580 - acc: 0.8697\n",
      "Epoch 32/266\n",
      "3200/3200 [==============================] - 2s 540us/step - loss: 0.3485 - acc: 0.8719\n",
      "Epoch 33/266\n",
      "3200/3200 [==============================] - 2s 521us/step - loss: 0.3503 - acc: 0.8719\n",
      "Epoch 34/266\n",
      "3200/3200 [==============================] - 2s 555us/step - loss: 0.3474 - acc: 0.8731\n",
      "Epoch 35/266\n",
      "3200/3200 [==============================] - 2s 528us/step - loss: 0.3410 - acc: 0.8747\n",
      "Epoch 36/266\n",
      "3200/3200 [==============================] - 2s 518us/step - loss: 0.3347 - acc: 0.8756\n",
      "Epoch 37/266\n",
      "3200/3200 [==============================] - 2s 536us/step - loss: 0.3342 - acc: 0.8775\n",
      "Epoch 38/266\n",
      "3200/3200 [==============================] - 2s 532us/step - loss: 0.3268 - acc: 0.8797\n",
      "Epoch 39/266\n",
      "3200/3200 [==============================] - 2s 522us/step - loss: 0.3279 - acc: 0.8772\n",
      "Epoch 40/266\n",
      "3200/3200 [==============================] - 2s 529us/step - loss: 0.3300 - acc: 0.8778\n",
      "Epoch 41/266\n",
      "3200/3200 [==============================] - 2s 522us/step - loss: 0.3151 - acc: 0.8819\n",
      "Epoch 42/266\n",
      "3200/3200 [==============================] - 2s 524us/step - loss: 0.3125 - acc: 0.8822\n",
      "Epoch 43/266\n",
      "3200/3200 [==============================] - 2s 529us/step - loss: 0.3055 - acc: 0.8850\n",
      "Epoch 44/266\n",
      "3200/3200 [==============================] - 2s 538us/step - loss: 0.3042 - acc: 0.8866\n",
      "Epoch 45/266\n",
      "3200/3200 [==============================] - 2s 548us/step - loss: 0.2938 - acc: 0.8856\n",
      "Epoch 46/266\n",
      "3200/3200 [==============================] - 2s 536us/step - loss: 0.2926 - acc: 0.8891\n",
      "Epoch 47/266\n",
      "3200/3200 [==============================] - 2s 518us/step - loss: 0.2883 - acc: 0.8900\n",
      "Epoch 48/266\n",
      "3200/3200 [==============================] - 2s 544us/step - loss: 0.2813 - acc: 0.8922\n",
      "Epoch 49/266\n",
      "3200/3200 [==============================] - 2s 548us/step - loss: 0.2893 - acc: 0.8894\n",
      "Epoch 50/266\n",
      "3200/3200 [==============================] - 2s 542us/step - loss: 0.2834 - acc: 0.8903\n",
      "Epoch 51/266\n",
      "3200/3200 [==============================] - 2s 515us/step - loss: 0.2791 - acc: 0.8903\n",
      "Epoch 52/266\n",
      "3200/3200 [==============================] - 2s 513us/step - loss: 0.2753 - acc: 0.8959\n",
      "Epoch 53/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.2718 - acc: 0.8909\n",
      "Epoch 54/266\n",
      "3200/3200 [==============================] - 2s 528us/step - loss: 0.2486 - acc: 0.9044\n",
      "Epoch 55/266\n",
      "3200/3200 [==============================] - 2s 516us/step - loss: 0.2490 - acc: 0.9031\n",
      "Epoch 56/266\n",
      "3200/3200 [==============================] - 2s 547us/step - loss: 0.2425 - acc: 0.9103\n",
      "Epoch 57/266\n",
      "3200/3200 [==============================] - 2s 549us/step - loss: 0.2475 - acc: 0.9037\n",
      "Epoch 58/266\n",
      "3200/3200 [==============================] - 2s 525us/step - loss: 0.2367 - acc: 0.9081\n",
      "Epoch 59/266\n",
      "3200/3200 [==============================] - 2s 524us/step - loss: 0.2318 - acc: 0.9138\n",
      "Epoch 60/266\n",
      "3200/3200 [==============================] - 2s 524us/step - loss: 0.2296 - acc: 0.9097\n",
      "Epoch 61/266\n",
      "3200/3200 [==============================] - 2s 540us/step - loss: 0.2170 - acc: 0.9169\n",
      "Epoch 62/266\n",
      "3200/3200 [==============================] - 2s 552us/step - loss: 0.2070 - acc: 0.9228\n",
      "Epoch 63/266\n",
      "3200/3200 [==============================] - 2s 534us/step - loss: 0.2139 - acc: 0.9153\n",
      "Epoch 64/266\n",
      "3200/3200 [==============================] - 2s 536us/step - loss: 0.2085 - acc: 0.9209\n",
      "Epoch 65/266\n",
      "3200/3200 [==============================] - 2s 545us/step - loss: 0.2185 - acc: 0.9175\n",
      "Epoch 66/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.2062 - acc: 0.9166\n",
      "Epoch 67/266\n",
      "3200/3200 [==============================] - 2s 536us/step - loss: 0.1908 - acc: 0.9259\n",
      "Epoch 68/266\n",
      "3200/3200 [==============================] - 2s 529us/step - loss: 0.1796 - acc: 0.9303\n",
      "Epoch 69/266\n",
      "3200/3200 [==============================] - 2s 552us/step - loss: 0.1725 - acc: 0.9328\n",
      "Epoch 70/266\n",
      "3200/3200 [==============================] - 2s 547us/step - loss: 0.1907 - acc: 0.9241\n",
      "Epoch 71/266\n",
      "3200/3200 [==============================] - 2s 525us/step - loss: 0.2246 - acc: 0.9119\n",
      "Epoch 72/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.1753 - acc: 0.9331\n",
      "Epoch 73/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.1616 - acc: 0.9384\n",
      "Epoch 74/266\n",
      "3200/3200 [==============================] - 2s 513us/step - loss: 0.1580 - acc: 0.9428\n",
      "Epoch 75/266\n",
      "3200/3200 [==============================] - 2s 536us/step - loss: 0.1553 - acc: 0.9413\n",
      "Epoch 76/266\n",
      "3200/3200 [==============================] - 2s 539us/step - loss: 0.1511 - acc: 0.9434\n",
      "Epoch 77/266\n",
      "3200/3200 [==============================] - 2s 532us/step - loss: 0.1452 - acc: 0.9500\n",
      "Epoch 78/266\n",
      "3200/3200 [==============================] - 2s 542us/step - loss: 0.1487 - acc: 0.9409\n",
      "Epoch 79/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.1496 - acc: 0.9441\n",
      "Epoch 80/266\n",
      "3200/3200 [==============================] - 2s 526us/step - loss: 0.1361 - acc: 0.9491\n",
      "Epoch 81/266\n",
      "3200/3200 [==============================] - 2s 510us/step - loss: 0.1268 - acc: 0.9534\n",
      "Epoch 82/266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - 2s 532us/step - loss: 0.1221 - acc: 0.9550\n",
      "Epoch 83/266\n",
      "3200/3200 [==============================] - 2s 516us/step - loss: 0.1384 - acc: 0.9484\n",
      "Epoch 84/266\n",
      "3200/3200 [==============================] - 2s 535us/step - loss: 0.1285 - acc: 0.9528\n",
      "Epoch 85/266\n",
      "3200/3200 [==============================] - 2s 526us/step - loss: 0.1586 - acc: 0.9391\n",
      "Epoch 86/266\n",
      "3200/3200 [==============================] - 2s 533us/step - loss: 0.1285 - acc: 0.9522\n",
      "Epoch 87/266\n",
      "3200/3200 [==============================] - 2s 534us/step - loss: 0.1350 - acc: 0.9441\n",
      "Epoch 88/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.1317 - acc: 0.9528\n",
      "Epoch 89/266\n",
      "3200/3200 [==============================] - 2s 534us/step - loss: 0.1188 - acc: 0.9569\n",
      "Epoch 90/266\n",
      "3200/3200 [==============================] - 2s 524us/step - loss: 0.1049 - acc: 0.9631\n",
      "Epoch 91/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.1142 - acc: 0.9581\n",
      "Epoch 92/266\n",
      "3200/3200 [==============================] - 2s 521us/step - loss: 0.0964 - acc: 0.9672\n",
      "Epoch 93/266\n",
      "3200/3200 [==============================] - 2s 538us/step - loss: 0.0992 - acc: 0.9706\n",
      "Epoch 94/266\n",
      "3200/3200 [==============================] - 2s 532us/step - loss: 0.1057 - acc: 0.9622\n",
      "Epoch 95/266\n",
      "3200/3200 [==============================] - 2s 532us/step - loss: 0.1278 - acc: 0.9469\n",
      "Epoch 96/266\n",
      "3200/3200 [==============================] - 2s 554us/step - loss: 0.1040 - acc: 0.9631\n",
      "Epoch 97/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.0952 - acc: 0.9669\n",
      "Epoch 98/266\n",
      "3200/3200 [==============================] - 2s 547us/step - loss: 0.0929 - acc: 0.9666\n",
      "Epoch 99/266\n",
      "3200/3200 [==============================] - 2s 539us/step - loss: 0.0833 - acc: 0.9738\n",
      "Epoch 100/266\n",
      "3200/3200 [==============================] - 2s 568us/step - loss: 0.0742 - acc: 0.9791\n",
      "Epoch 101/266\n",
      "3200/3200 [==============================] - 2s 518us/step - loss: 0.0722 - acc: 0.9788\n",
      "Epoch 102/266\n",
      "3200/3200 [==============================] - 2s 547us/step - loss: 0.0692 - acc: 0.9794\n",
      "Epoch 103/266\n",
      "3200/3200 [==============================] - 2s 548us/step - loss: 0.0710 - acc: 0.9784\n",
      "Epoch 104/266\n",
      "3200/3200 [==============================] - 2s 515us/step - loss: 0.0638 - acc: 0.9819\n",
      "Epoch 105/266\n",
      "3200/3200 [==============================] - 2s 527us/step - loss: 0.0627 - acc: 0.9812\n",
      "Epoch 106/266\n",
      "3200/3200 [==============================] - 2s 532us/step - loss: 0.0603 - acc: 0.9828\n",
      "Epoch 107/266\n",
      "3200/3200 [==============================] - 2s 535us/step - loss: 0.0612 - acc: 0.9850\n",
      "Epoch 108/266\n",
      "3200/3200 [==============================] - 2s 555us/step - loss: 0.0631 - acc: 0.9816\n",
      "Epoch 109/266\n",
      "3200/3200 [==============================] - 2s 527us/step - loss: 0.0557 - acc: 0.9850\n",
      "Epoch 110/266\n",
      "3200/3200 [==============================] - 2s 519us/step - loss: 0.0559 - acc: 0.9856\n",
      "Epoch 111/266\n",
      "3200/3200 [==============================] - 2s 549us/step - loss: 0.0528 - acc: 0.9862\n",
      "Epoch 112/266\n",
      "3200/3200 [==============================] - 2s 538us/step - loss: 0.0557 - acc: 0.9847\n",
      "Epoch 113/266\n",
      "3200/3200 [==============================] - 2s 535us/step - loss: 0.0510 - acc: 0.9884\n",
      "Epoch 114/266\n",
      "3200/3200 [==============================] - 2s 558us/step - loss: 0.0506 - acc: 0.9862\n",
      "Epoch 115/266\n",
      "3200/3200 [==============================] - 2s 543us/step - loss: 0.0466 - acc: 0.9881\n",
      "Epoch 116/266\n",
      "3200/3200 [==============================] - 2s 545us/step - loss: 0.0469 - acc: 0.9881\n",
      "Epoch 117/266\n",
      "3200/3200 [==============================] - 2s 523us/step - loss: 0.0471 - acc: 0.9856\n",
      "Epoch 118/266\n",
      "3200/3200 [==============================] - 2s 527us/step - loss: 0.0776 - acc: 0.9766\n",
      "Epoch 119/266\n",
      "3200/3200 [==============================] - 2s 548us/step - loss: 0.6634 - acc: 0.8369\n",
      "Epoch 120/266\n",
      "3200/3200 [==============================] - 2s 528us/step - loss: 0.3135 - acc: 0.8766\n",
      "Epoch 121/266\n",
      "3200/3200 [==============================] - 2s 527us/step - loss: 0.2586 - acc: 0.8912\n",
      "Epoch 122/266\n",
      "3200/3200 [==============================] - 2s 522us/step - loss: 0.1858 - acc: 0.9213\n",
      "Epoch 123/266\n",
      "3200/3200 [==============================] - 2s 530us/step - loss: 0.1340 - acc: 0.9553\n",
      "Epoch 124/266\n",
      "3200/3200 [==============================] - 2s 539us/step - loss: 0.1134 - acc: 0.9647\n",
      "Epoch 125/266\n",
      "3200/3200 [==============================] - 2s 516us/step - loss: 0.0995 - acc: 0.9622\n",
      "Epoch 126/266\n",
      "3200/3200 [==============================] - 2s 547us/step - loss: 0.1399 - acc: 0.9484\n",
      "Epoch 127/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.1339 - acc: 0.9469\n",
      "Epoch 128/266\n",
      "3200/3200 [==============================] - 2s 535us/step - loss: 0.0894 - acc: 0.9697\n",
      "Epoch 129/266\n",
      "3200/3200 [==============================] - 2s 532us/step - loss: 0.0690 - acc: 0.9812\n",
      "Epoch 130/266\n",
      "3200/3200 [==============================] - 2s 530us/step - loss: 0.0616 - acc: 0.9844\n",
      "Epoch 131/266\n",
      "3200/3200 [==============================] - 2s 535us/step - loss: 0.0570 - acc: 0.9819\n",
      "Epoch 132/266\n",
      "3200/3200 [==============================] - 2s 522us/step - loss: 0.0514 - acc: 0.9884\n",
      "Epoch 133/266\n",
      "3200/3200 [==============================] - 2s 516us/step - loss: 0.0488 - acc: 0.9872\n",
      "Epoch 134/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.0443 - acc: 0.9900\n",
      "Epoch 135/266\n",
      "3200/3200 [==============================] - 2s 525us/step - loss: 0.0431 - acc: 0.9916\n",
      "Epoch 136/266\n",
      "3200/3200 [==============================] - 2s 525us/step - loss: 0.0406 - acc: 0.9916\n",
      "Epoch 137/266\n",
      "3200/3200 [==============================] - 2s 521us/step - loss: 0.0395 - acc: 0.9916\n",
      "Epoch 138/266\n",
      "3200/3200 [==============================] - 2s 552us/step - loss: 0.0390 - acc: 0.9919\n",
      "Epoch 139/266\n",
      "3200/3200 [==============================] - 2s 534us/step - loss: 0.0377 - acc: 0.9928\n",
      "Epoch 140/266\n",
      "3200/3200 [==============================] - 2s 518us/step - loss: 0.0374 - acc: 0.9913\n",
      "Epoch 141/266\n",
      "3200/3200 [==============================] - 2s 526us/step - loss: 0.0351 - acc: 0.9934\n",
      "Epoch 142/266\n",
      "3200/3200 [==============================] - 2s 539us/step - loss: 0.0327 - acc: 0.9934\n",
      "Epoch 143/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.0369 - acc: 0.9903\n",
      "Epoch 144/266\n",
      "3200/3200 [==============================] - 2s 527us/step - loss: 0.0303 - acc: 0.9947\n",
      "Epoch 145/266\n",
      "3200/3200 [==============================] - 2s 524us/step - loss: 0.0298 - acc: 0.9944\n",
      "Epoch 146/266\n",
      "3200/3200 [==============================] - 2s 522us/step - loss: 0.0296 - acc: 0.9944\n",
      "Epoch 147/266\n",
      "3200/3200 [==============================] - 2s 536us/step - loss: 0.0273 - acc: 0.9953\n",
      "Epoch 148/266\n",
      "3200/3200 [==============================] - 2s 535us/step - loss: 0.0262 - acc: 0.9959\n",
      "Epoch 149/266\n",
      "3200/3200 [==============================] - 2s 534us/step - loss: 0.0257 - acc: 0.9959\n",
      "Epoch 150/266\n",
      "3200/3200 [==============================] - 2s 523us/step - loss: 0.0273 - acc: 0.9956\n",
      "Epoch 151/266\n",
      "3200/3200 [==============================] - 2s 547us/step - loss: 0.0245 - acc: 0.9959\n",
      "Epoch 152/266\n",
      "3200/3200 [==============================] - 2s 530us/step - loss: 0.0255 - acc: 0.9950\n",
      "Epoch 153/266\n",
      "3200/3200 [==============================] - 2s 523us/step - loss: 0.0262 - acc: 0.9941\n",
      "Epoch 154/266\n",
      "3200/3200 [==============================] - 2s 536us/step - loss: 0.0240 - acc: 0.9944\n",
      "Epoch 155/266\n",
      "3200/3200 [==============================] - 2s 556us/step - loss: 0.0238 - acc: 0.9962\n",
      "Epoch 156/266\n",
      "3200/3200 [==============================] - 2s 543us/step - loss: 0.0232 - acc: 0.9959\n",
      "Epoch 157/266\n",
      "3200/3200 [==============================] - 2s 545us/step - loss: 0.0218 - acc: 0.9975\n",
      "Epoch 158/266\n",
      "3200/3200 [==============================] - 2s 546us/step - loss: 0.0207 - acc: 0.9969\n",
      "Epoch 159/266\n",
      "3200/3200 [==============================] - 2s 513us/step - loss: 0.0199 - acc: 0.9972\n",
      "Epoch 160/266\n",
      "3200/3200 [==============================] - 2s 545us/step - loss: 0.0193 - acc: 0.9972\n",
      "Epoch 161/266\n",
      "3200/3200 [==============================] - 2s 522us/step - loss: 0.0198 - acc: 0.9969\n",
      "Epoch 162/266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - 2s 544us/step - loss: 0.0197 - acc: 0.9969\n",
      "Epoch 163/266\n",
      "3200/3200 [==============================] - 2s 544us/step - loss: 0.0221 - acc: 0.9966\n",
      "Epoch 164/266\n",
      "3200/3200 [==============================] - 2s 540us/step - loss: 0.0205 - acc: 0.9966\n",
      "Epoch 165/266\n",
      "3200/3200 [==============================] - 2s 544us/step - loss: 0.0184 - acc: 0.9972\n",
      "Epoch 166/266\n",
      "3200/3200 [==============================] - 2s 547us/step - loss: 0.0168 - acc: 0.9969\n",
      "Epoch 167/266\n",
      "3200/3200 [==============================] - 2s 527us/step - loss: 0.0171 - acc: 0.9975\n",
      "Epoch 168/266\n",
      "3200/3200 [==============================] - 2s 514us/step - loss: 0.0161 - acc: 0.9981\n",
      "Epoch 169/266\n",
      "3200/3200 [==============================] - 2s 533us/step - loss: 0.0163 - acc: 0.9969\n",
      "Epoch 170/266\n",
      "3200/3200 [==============================] - 2s 555us/step - loss: 0.0162 - acc: 0.9975\n",
      "Epoch 171/266\n",
      "3200/3200 [==============================] - 2s 525us/step - loss: 0.0161 - acc: 0.9978\n",
      "Epoch 172/266\n",
      "3200/3200 [==============================] - 2s 544us/step - loss: 0.0159 - acc: 0.9981\n",
      "Epoch 173/266\n",
      "3200/3200 [==============================] - 2s 534us/step - loss: 0.0148 - acc: 0.9981\n",
      "Epoch 174/266\n",
      "3200/3200 [==============================] - 2s 530us/step - loss: 0.0148 - acc: 0.9975\n",
      "Epoch 175/266\n",
      "3200/3200 [==============================] - 2s 530us/step - loss: 0.0139 - acc: 0.9972\n",
      "Epoch 176/266\n",
      "3200/3200 [==============================] - 2s 547us/step - loss: 0.0145 - acc: 0.9988\n",
      "Epoch 177/266\n",
      "3200/3200 [==============================] - 2s 536us/step - loss: 0.0136 - acc: 0.9972\n",
      "Epoch 178/266\n",
      "3200/3200 [==============================] - 2s 526us/step - loss: 0.0132 - acc: 0.9984\n",
      "Epoch 179/266\n",
      "3200/3200 [==============================] - 2s 550us/step - loss: 0.0128 - acc: 0.9984\n",
      "Epoch 180/266\n",
      "3200/3200 [==============================] - 2s 522us/step - loss: 0.0117 - acc: 0.9981\n",
      "Epoch 181/266\n",
      "3200/3200 [==============================] - 2s 490us/step - loss: 0.0126 - acc: 0.9984\n",
      "Epoch 182/266\n",
      "3200/3200 [==============================] - 2s 519us/step - loss: 0.0119 - acc: 0.9988\n",
      "Epoch 183/266\n",
      "3200/3200 [==============================] - 2s 541us/step - loss: 0.0119 - acc: 0.9978\n",
      "Epoch 184/266\n",
      "3200/3200 [==============================] - 2s 524us/step - loss: 0.0118 - acc: 0.9984\n",
      "Epoch 185/266\n",
      "3200/3200 [==============================] - 2s 527us/step - loss: 0.0116 - acc: 0.9994\n",
      "Epoch 186/266\n",
      "3200/3200 [==============================] - 2s 525us/step - loss: 0.0104 - acc: 0.9984\n",
      "Epoch 187/266\n",
      "3200/3200 [==============================] - 2s 538us/step - loss: 0.0105 - acc: 0.9988\n",
      "Epoch 188/266\n",
      "3200/3200 [==============================] - 2s 520us/step - loss: 0.0097 - acc: 0.9994\n",
      "Epoch 189/266\n",
      "3200/3200 [==============================] - 2s 509us/step - loss: 0.0096 - acc: 0.9984\n",
      "Epoch 190/266\n",
      "3200/3200 [==============================] - 2s 544us/step - loss: 0.0091 - acc: 0.9994\n",
      "Epoch 191/266\n",
      "3200/3200 [==============================] - 2s 541us/step - loss: 0.0090 - acc: 0.9991\n",
      "Epoch 192/266\n",
      "3200/3200 [==============================] - 2s 549us/step - loss: 0.0094 - acc: 0.9991\n",
      "Epoch 193/266\n",
      "3200/3200 [==============================] - 2s 555us/step - loss: 0.0087 - acc: 0.9991\n",
      "Epoch 194/266\n",
      "3200/3200 [==============================] - 2s 540us/step - loss: 0.0094 - acc: 0.9988\n",
      "Epoch 195/266\n",
      "3200/3200 [==============================] - 2s 549us/step - loss: 0.0091 - acc: 0.9991\n",
      "Epoch 196/266\n",
      "3200/3200 [==============================] - 2s 529us/step - loss: 0.0079 - acc: 0.9997\n",
      "Epoch 197/266\n",
      "3200/3200 [==============================] - 2s 550us/step - loss: 0.0087 - acc: 0.9994\n",
      "Epoch 198/266\n",
      "3200/3200 [==============================] - 2s 537us/step - loss: 0.0083 - acc: 0.9991\n",
      "Epoch 199/266\n",
      "3200/3200 [==============================] - 2s 538us/step - loss: 0.0073 - acc: 0.9994\n",
      "Epoch 200/266\n",
      "3200/3200 [==============================] - 2s 550us/step - loss: 0.0079 - acc: 0.9991\n",
      "Epoch 201/266\n",
      "3200/3200 [==============================] - 2s 527us/step - loss: 0.0069 - acc: 0.9994\n",
      "Epoch 202/266\n",
      "3200/3200 [==============================] - 2s 547us/step - loss: 0.0068 - acc: 0.9994\n",
      "Epoch 203/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.0060 - acc: 0.9997\n",
      "Epoch 204/266\n",
      "3200/3200 [==============================] - 2s 539us/step - loss: 0.0061 - acc: 0.9994\n",
      "Epoch 205/266\n",
      "3200/3200 [==============================] - 2s 547us/step - loss: 0.0063 - acc: 0.9994\n",
      "Epoch 206/266\n",
      "3200/3200 [==============================] - 2s 541us/step - loss: 0.0071 - acc: 0.9991\n",
      "Epoch 207/266\n",
      "3200/3200 [==============================] - 2s 514us/step - loss: 0.0098 - acc: 0.9988\n",
      "Epoch 208/266\n",
      "3200/3200 [==============================] - 2s 533us/step - loss: 0.0136 - acc: 0.9966\n",
      "Epoch 209/266\n",
      "3200/3200 [==============================] - 2s 527us/step - loss: 0.0686 - acc: 0.9834\n",
      "Epoch 210/266\n",
      "3200/3200 [==============================] - 2s 544us/step - loss: 1.6539 - acc: 0.8603\n",
      "Epoch 211/266\n",
      "3200/3200 [==============================] - 2s 528us/step - loss: 1.3773 - acc: 0.8141\n",
      "Epoch 212/266\n",
      "3200/3200 [==============================] - 2s 546us/step - loss: 0.4386 - acc: 0.8663\n",
      "Epoch 213/266\n",
      "3200/3200 [==============================] - 2s 550us/step - loss: 0.3825 - acc: 0.8684\n",
      "Epoch 214/266\n",
      "3200/3200 [==============================] - 2s 526us/step - loss: 0.3691 - acc: 0.8659\n",
      "Epoch 215/266\n",
      "3200/3200 [==============================] - 2s 526us/step - loss: 0.3598 - acc: 0.8691\n",
      "Epoch 216/266\n",
      "3200/3200 [==============================] - 2s 535us/step - loss: 0.3434 - acc: 0.8716\n",
      "Epoch 217/266\n",
      "3200/3200 [==============================] - 2s 536us/step - loss: 0.3305 - acc: 0.8706\n",
      "Epoch 218/266\n",
      "3200/3200 [==============================] - 2s 557us/step - loss: 0.3044 - acc: 0.8756\n",
      "Epoch 219/266\n",
      "3200/3200 [==============================] - 2s 527us/step - loss: 0.2909 - acc: 0.8791\n",
      "Epoch 220/266\n",
      "3200/3200 [==============================] - 2s 525us/step - loss: 0.2781 - acc: 0.8844\n",
      "Epoch 221/266\n",
      "3200/3200 [==============================] - 2s 510us/step - loss: 0.2246 - acc: 0.8972\n",
      "Epoch 222/266\n",
      "3200/3200 [==============================] - 2s 525us/step - loss: 0.2048 - acc: 0.9091\n",
      "Epoch 223/266\n",
      "3200/3200 [==============================] - 2s 519us/step - loss: 0.3149 - acc: 0.8916\n",
      "Epoch 224/266\n",
      "3200/3200 [==============================] - 2s 519us/step - loss: 0.2417 - acc: 0.8944\n",
      "Epoch 225/266\n",
      "3200/3200 [==============================] - 2s 559us/step - loss: 0.1688 - acc: 0.9266\n",
      "Epoch 226/266\n",
      "3200/3200 [==============================] - 2s 507us/step - loss: 0.1256 - acc: 0.9563\n",
      "Epoch 227/266\n",
      "3200/3200 [==============================] - 2s 542us/step - loss: 0.0897 - acc: 0.9728\n",
      "Epoch 228/266\n",
      "3200/3200 [==============================] - 2s 520us/step - loss: 0.1315 - acc: 0.9528\n",
      "Epoch 229/266\n",
      "3200/3200 [==============================] - 2s 541us/step - loss: 0.1745 - acc: 0.9400\n",
      "Epoch 230/266\n",
      "3200/3200 [==============================] - 2s 524us/step - loss: 0.0964 - acc: 0.9681\n",
      "Epoch 231/266\n",
      "3200/3200 [==============================] - 2s 538us/step - loss: 0.0719 - acc: 0.9791\n",
      "Epoch 232/266\n",
      "3200/3200 [==============================] - 2s 518us/step - loss: 0.0561 - acc: 0.9850\n",
      "Epoch 233/266\n",
      "3200/3200 [==============================] - 2s 546us/step - loss: 0.0563 - acc: 0.9828\n",
      "Epoch 234/266\n",
      "3200/3200 [==============================] - 2s 519us/step - loss: 0.0394 - acc: 0.9903\n",
      "Epoch 235/266\n",
      "3200/3200 [==============================] - 2s 545us/step - loss: 0.0354 - acc: 0.9928\n",
      "Epoch 236/266\n",
      "3200/3200 [==============================] - 2s 528us/step - loss: 0.0324 - acc: 0.9925\n",
      "Epoch 237/266\n",
      "3200/3200 [==============================] - 2s 544us/step - loss: 0.0296 - acc: 0.9953\n",
      "Epoch 238/266\n",
      "3200/3200 [==============================] - 2s 532us/step - loss: 0.0249 - acc: 0.9962\n",
      "Epoch 239/266\n",
      "3200/3200 [==============================] - 2s 539us/step - loss: 0.0343 - acc: 0.9906\n",
      "Epoch 240/266\n",
      "3200/3200 [==============================] - 2s 538us/step - loss: 0.0364 - acc: 0.9909\n",
      "Epoch 241/266\n",
      "3200/3200 [==============================] - 2s 524us/step - loss: 0.0534 - acc: 0.9825\n",
      "Epoch 242/266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - 2s 527us/step - loss: 0.1391 - acc: 0.9519\n",
      "Epoch 243/266\n",
      "3200/3200 [==============================] - 2s 539us/step - loss: 0.1439 - acc: 0.9409\n",
      "Epoch 244/266\n",
      "3200/3200 [==============================] - 2s 514us/step - loss: 0.0880 - acc: 0.9731\n",
      "Epoch 245/266\n",
      "3200/3200 [==============================] - 2s 530us/step - loss: 0.0376 - acc: 0.9875\n",
      "Epoch 246/266\n",
      "3200/3200 [==============================] - 2s 524us/step - loss: 0.0225 - acc: 0.9969\n",
      "Epoch 247/266\n",
      "3200/3200 [==============================] - 2s 544us/step - loss: 0.0236 - acc: 0.9953\n",
      "Epoch 248/266\n",
      "3200/3200 [==============================] - 2s 535us/step - loss: 0.0189 - acc: 0.9972\n",
      "Epoch 249/266\n",
      "3200/3200 [==============================] - 2s 536us/step - loss: 0.0133 - acc: 0.9991\n",
      "Epoch 250/266\n",
      "3200/3200 [==============================] - 2s 544us/step - loss: 0.0105 - acc: 0.9997\n",
      "Epoch 251/266\n",
      "3200/3200 [==============================] - 2s 526us/step - loss: 0.0102 - acc: 0.9991\n",
      "Epoch 252/266\n",
      "3200/3200 [==============================] - 2s 533us/step - loss: 0.0107 - acc: 0.9988\n",
      "Epoch 253/266\n",
      "3200/3200 [==============================] - 2s 560us/step - loss: 0.0101 - acc: 0.9997\n",
      "Epoch 254/266\n",
      "3200/3200 [==============================] - 2s 539us/step - loss: 0.0103 - acc: 0.9994\n",
      "Epoch 255/266\n",
      "3200/3200 [==============================] - 2s 547us/step - loss: 0.0088 - acc: 0.9997\n",
      "Epoch 256/266\n",
      "3200/3200 [==============================] - 2s 539us/step - loss: 0.0078 - acc: 0.9997\n",
      "Epoch 257/266\n",
      "3200/3200 [==============================] - 2s 538us/step - loss: 0.0079 - acc: 0.9991\n",
      "Epoch 258/266\n",
      "3200/3200 [==============================] - 2s 535us/step - loss: 0.0072 - acc: 0.9997\n",
      "Epoch 259/266\n",
      "3200/3200 [==============================] - 2s 529us/step - loss: 0.0071 - acc: 0.9997\n",
      "Epoch 260/266\n",
      "3200/3200 [==============================] - 2s 542us/step - loss: 0.0062 - acc: 0.9997\n",
      "Epoch 261/266\n",
      "3200/3200 [==============================] - 2s 512us/step - loss: 0.0059 - acc: 0.9997\n",
      "Epoch 262/266\n",
      "3200/3200 [==============================] - 2s 532us/step - loss: 0.0061 - acc: 0.9997\n",
      "Epoch 263/266\n",
      "3200/3200 [==============================] - 2s 529us/step - loss: 0.0058 - acc: 0.9997\n",
      "Epoch 264/266\n",
      "3200/3200 [==============================] - 2s 551us/step - loss: 0.0055 - acc: 0.9997\n",
      "Epoch 265/266\n",
      "3200/3200 [==============================] - 2s 546us/step - loss: 0.0050 - acc: 1.0000\n",
      "Epoch 266/266\n",
      "3200/3200 [==============================] - 2s 539us/step - loss: 0.0050 - acc: 0.9997\n",
      "@ Total Time Spent: 456.21 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# 训练模型\n",
    "model.fit(x_train.reshape((x_train.shape[0],10,9,1)),y_train,batch_size=256,epochs=266)\n",
    "print('@ Total Time Spent: %.2f seconds' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict(x_test.reshape((x_test.shape[0],10,9,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9998593e-01, 1.4124693e-05],\n",
       "       [1.0000000e+00, 4.3917570e-10],\n",
       "       [1.0000000e+00, 8.7278396e-10],\n",
       "       [9.9999309e-01, 6.8770369e-06],\n",
       "       [9.9999988e-01, 7.6666971e-08],\n",
       "       [1.0000000e+00, 5.8134241e-08],\n",
       "       [9.9999738e-01, 2.6630350e-06],\n",
       "       [9.9998093e-01, 1.9049217e-05],\n",
       "       [1.0000000e+00, 2.8458558e-08],\n",
       "       [9.9926537e-01, 7.3465967e-04]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/355 [==============================] - 0s 386us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.582860738884114, 0.9492957746478873]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test.reshape((x_test.shape[0],10,9,1)),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用SimpleRNN预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 50)                2700      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 2,802\n",
      "Trainable params: 2,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 创建模型\n",
    "model = Sequential()\n",
    "\n",
    "# 循环神经网络\n",
    "model.add(SimpleRNN(\n",
    "    units = 50, # 输出\n",
    "    input_shape = (30,3), #输入\n",
    "))\n",
    "\n",
    "# 输出层\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "# 定义优化器\n",
    "adam = Adam(lr=1e-4)\n",
    "\n",
    "# 定义优化器，loss function，训练过程中计算准确率\n",
    "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 1s 263us/step - loss: 0.4818 - acc: 0.8672\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.4001 - acc: 0.8688\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.3906 - acc: 0.8688\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.3893 - acc: 0.8688\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.3888 - acc: 0.8688\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.3884 - acc: 0.8688\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.3878 - acc: 0.8688\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 1s 183us/step - loss: 0.3874 - acc: 0.8688\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.3869 - acc: 0.8688\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.3864 - acc: 0.8688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd140392160>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "model.fit(x[:3200],y_train[:3200],batch_size=64,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "3200/3200 [==============================] - 0s 150us/step - loss: 0.3267 - acc: 0.8759\n",
      "Epoch 2/200\n",
      "3200/3200 [==============================] - 0s 155us/step - loss: 0.3344 - acc: 0.8709\n",
      "Epoch 3/200\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.3281 - acc: 0.8744\n",
      "Epoch 4/200\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.3265 - acc: 0.8744\n",
      "Epoch 5/200\n",
      "3200/3200 [==============================] - 1s 161us/step - loss: 0.3263 - acc: 0.8747\n",
      "Epoch 6/200\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.3314 - acc: 0.8728\n",
      "Epoch 7/200\n",
      "3200/3200 [==============================] - 0s 151us/step - loss: 0.3254 - acc: 0.8769\n",
      "Epoch 8/200\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.3251 - acc: 0.8759\n",
      "Epoch 9/200\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.3254 - acc: 0.8769\n",
      "Epoch 10/200\n",
      "3200/3200 [==============================] - 1s 180us/step - loss: 0.3298 - acc: 0.8762\n",
      "Epoch 11/200\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.3274 - acc: 0.8784\n",
      "Epoch 12/200\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.3310 - acc: 0.8759\n",
      "Epoch 13/200\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.3288 - acc: 0.8731\n",
      "Epoch 14/200\n",
      "3200/3200 [==============================] - 1s 159us/step - loss: 0.3266 - acc: 0.8741\n",
      "Epoch 15/200\n",
      "3200/3200 [==============================] - 1s 160us/step - loss: 0.3270 - acc: 0.8769\n",
      "Epoch 16/200\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.3312 - acc: 0.8738\n",
      "Epoch 17/200\n",
      "3200/3200 [==============================] - 1s 157us/step - loss: 0.3260 - acc: 0.8756\n",
      "Epoch 18/200\n",
      "3200/3200 [==============================] - 1s 179us/step - loss: 0.3301 - acc: 0.8744\n",
      "Epoch 19/200\n",
      "3200/3200 [==============================] - 0s 154us/step - loss: 0.3260 - acc: 0.8766\n",
      "Epoch 20/200\n",
      "3200/3200 [==============================] - 0s 154us/step - loss: 0.3251 - acc: 0.8759\n",
      "Epoch 21/200\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.3258 - acc: 0.8769\n",
      "Epoch 22/200\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.3284 - acc: 0.8731\n",
      "Epoch 23/200\n",
      "3200/3200 [==============================] - 1s 161us/step - loss: 0.3270 - acc: 0.8722\n",
      "Epoch 24/200\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.3246 - acc: 0.8756\n",
      "Epoch 25/200\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.3261 - acc: 0.8747\n",
      "Epoch 26/200\n",
      "3200/3200 [==============================] - 1s 159us/step - loss: 0.3285 - acc: 0.8706\n",
      "Epoch 27/200\n",
      "3200/3200 [==============================] - 0s 156us/step - loss: 0.3251 - acc: 0.8766\n",
      "Epoch 28/200\n",
      "3200/3200 [==============================] - 0s 150us/step - loss: 0.3236 - acc: 0.8759\n",
      "Epoch 29/200\n",
      "3200/3200 [==============================] - 1s 158us/step - loss: 0.3243 - acc: 0.8762\n",
      "Epoch 30/200\n",
      "3200/3200 [==============================] - 1s 157us/step - loss: 0.3230 - acc: 0.8797\n",
      "Epoch 31/200\n",
      "3200/3200 [==============================] - 0s 153us/step - loss: 0.3282 - acc: 0.8787\n",
      "Epoch 32/200\n",
      "3200/3200 [==============================] - 1s 160us/step - loss: 0.3294 - acc: 0.8728\n",
      "Epoch 33/200\n",
      "3200/3200 [==============================] - 1s 158us/step - loss: 0.3278 - acc: 0.8731\n",
      "Epoch 34/200\n",
      "3200/3200 [==============================] - 0s 153us/step - loss: 0.3268 - acc: 0.8759\n",
      "Epoch 35/200\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.3258 - acc: 0.8716\n",
      "Epoch 36/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3249 - acc: 0.8775\n",
      "Epoch 37/200\n",
      "3200/3200 [==============================] - 1s 160us/step - loss: 0.3239 - acc: 0.8769\n",
      "Epoch 38/200\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.3271 - acc: 0.8738\n",
      "Epoch 39/200\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.3260 - acc: 0.8738\n",
      "Epoch 40/200\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.3257 - acc: 0.8750\n",
      "Epoch 41/200\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.3240 - acc: 0.8766\n",
      "Epoch 42/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3272 - acc: 0.8775\n",
      "Epoch 43/200\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.3276 - acc: 0.8738\n",
      "Epoch 44/200\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.3252 - acc: 0.8738\n",
      "Epoch 45/200\n",
      "3200/3200 [==============================] - 0s 150us/step - loss: 0.3248 - acc: 0.8769\n",
      "Epoch 46/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3227 - acc: 0.8747\n",
      "Epoch 47/200\n",
      "3200/3200 [==============================] - 0s 153us/step - loss: 0.3242 - acc: 0.8756\n",
      "Epoch 48/200\n",
      "3200/3200 [==============================] - 0s 140us/step - loss: 0.3270 - acc: 0.8741\n",
      "Epoch 49/200\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.3251 - acc: 0.8747\n",
      "Epoch 50/200\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.3246 - acc: 0.8775\n",
      "Epoch 51/200\n",
      "3200/3200 [==============================] - 0s 142us/step - loss: 0.3250 - acc: 0.8759\n",
      "Epoch 52/200\n",
      "3200/3200 [==============================] - 1s 158us/step - loss: 0.3236 - acc: 0.8772\n",
      "Epoch 53/200\n",
      "3200/3200 [==============================] - 0s 156us/step - loss: 0.3223 - acc: 0.8778\n",
      "Epoch 54/200\n",
      "3200/3200 [==============================] - 0s 148us/step - loss: 0.3224 - acc: 0.8772\n",
      "Epoch 55/200\n",
      "3200/3200 [==============================] - 0s 155us/step - loss: 0.3219 - acc: 0.8769\n",
      "Epoch 56/200\n",
      "3200/3200 [==============================] - 0s 156us/step - loss: 0.3256 - acc: 0.8753\n",
      "Epoch 57/200\n",
      "3200/3200 [==============================] - 0s 143us/step - loss: 0.3234 - acc: 0.8775\n",
      "Epoch 58/200\n",
      "3200/3200 [==============================] - 0s 156us/step - loss: 0.3249 - acc: 0.8762\n",
      "Epoch 59/200\n",
      "3200/3200 [==============================] - 0s 154us/step - loss: 0.3223 - acc: 0.8762\n",
      "Epoch 60/200\n",
      "3200/3200 [==============================] - 0s 150us/step - loss: 0.3242 - acc: 0.8756\n",
      "Epoch 61/200\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.3236 - acc: 0.8778\n",
      "Epoch 62/200\n",
      "3200/3200 [==============================] - 1s 159us/step - loss: 0.3262 - acc: 0.8759\n",
      "Epoch 63/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3254 - acc: 0.8734\n",
      "Epoch 64/200\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.3218 - acc: 0.8762\n",
      "Epoch 65/200\n",
      "3200/3200 [==============================] - 0s 147us/step - loss: 0.3213 - acc: 0.8803\n",
      "Epoch 66/200\n",
      "3200/3200 [==============================] - 0s 151us/step - loss: 0.3205 - acc: 0.8787\n",
      "Epoch 67/200\n",
      "3200/3200 [==============================] - 0s 149us/step - loss: 0.3203 - acc: 0.8781\n",
      "Epoch 68/200\n",
      "3200/3200 [==============================] - 0s 146us/step - loss: 0.3256 - acc: 0.8775\n",
      "Epoch 69/200\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.3220 - acc: 0.8800\n",
      "Epoch 70/200\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.3221 - acc: 0.8781\n",
      "Epoch 71/200\n",
      "3200/3200 [==============================] - 0s 156us/step - loss: 0.3225 - acc: 0.8759\n",
      "Epoch 72/200\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.3286 - acc: 0.8756\n",
      "Epoch 73/200\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.3301 - acc: 0.8756\n",
      "Epoch 74/200\n",
      "3200/3200 [==============================] - 0s 152us/step - loss: 0.3245 - acc: 0.8750\n",
      "Epoch 75/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3321 - acc: 0.8703\n",
      "Epoch 76/200\n",
      "3200/3200 [==============================] - 1s 160us/step - loss: 0.3285 - acc: 0.8762\n",
      "Epoch 77/200\n",
      "3200/3200 [==============================] - 0s 151us/step - loss: 0.3261 - acc: 0.8741\n",
      "Epoch 78/200\n",
      "3200/3200 [==============================] - 0s 144us/step - loss: 0.3247 - acc: 0.8769\n",
      "Epoch 79/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3299 - acc: 0.8759\n",
      "Epoch 80/200\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.3294 - acc: 0.8747\n",
      "Epoch 81/200\n",
      "3200/3200 [==============================] - 0s 141us/step - loss: 0.3220 - acc: 0.8781\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.3234 - acc: 0.8738\n",
      "Epoch 83/200\n",
      "3200/3200 [==============================] - 0s 155us/step - loss: 0.3224 - acc: 0.8756\n",
      "Epoch 84/200\n",
      "3200/3200 [==============================] - 0s 149us/step - loss: 0.3280 - acc: 0.8738\n",
      "Epoch 85/200\n",
      "3200/3200 [==============================] - 0s 155us/step - loss: 0.3217 - acc: 0.8784\n",
      "Epoch 86/200\n",
      "3200/3200 [==============================] - 0s 153us/step - loss: 0.3245 - acc: 0.8775\n",
      "Epoch 87/200\n",
      "3200/3200 [==============================] - 0s 150us/step - loss: 0.3239 - acc: 0.8772\n",
      "Epoch 88/200\n",
      "3200/3200 [==============================] - 0s 151us/step - loss: 0.3207 - acc: 0.8766\n",
      "Epoch 89/200\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.3215 - acc: 0.8794\n",
      "Epoch 90/200\n",
      "3200/3200 [==============================] - 0s 152us/step - loss: 0.3229 - acc: 0.8784\n",
      "Epoch 91/200\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.3239 - acc: 0.8778\n",
      "Epoch 92/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3219 - acc: 0.8784\n",
      "Epoch 93/200\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.3221 - acc: 0.8769\n",
      "Epoch 94/200\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.3216 - acc: 0.8800\n",
      "Epoch 95/200\n",
      "3200/3200 [==============================] - 0s 150us/step - loss: 0.3315 - acc: 0.8741\n",
      "Epoch 96/200\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.3216 - acc: 0.8769\n",
      "Epoch 97/200\n",
      "3200/3200 [==============================] - 1s 161us/step - loss: 0.3252 - acc: 0.8738\n",
      "Epoch 98/200\n",
      "3200/3200 [==============================] - 0s 145us/step - loss: 0.3224 - acc: 0.8769\n",
      "Epoch 99/200\n",
      "3200/3200 [==============================] - 1s 158us/step - loss: 0.3234 - acc: 0.8753\n",
      "Epoch 100/200\n",
      "3200/3200 [==============================] - 0s 155us/step - loss: 0.3256 - acc: 0.8766\n",
      "Epoch 101/200\n",
      "3200/3200 [==============================] - 0s 152us/step - loss: 0.3237 - acc: 0.8784\n",
      "Epoch 102/200\n",
      "3200/3200 [==============================] - 0s 155us/step - loss: 0.3212 - acc: 0.8775\n",
      "Epoch 103/200\n",
      "3200/3200 [==============================] - 1s 158us/step - loss: 0.3252 - acc: 0.8741\n",
      "Epoch 104/200\n",
      "3200/3200 [==============================] - 0s 150us/step - loss: 0.3279 - acc: 0.8725\n",
      "Epoch 105/200\n",
      "3200/3200 [==============================] - 0s 153us/step - loss: 0.3231 - acc: 0.8769\n",
      "Epoch 106/200\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.3200 - acc: 0.8762\n",
      "Epoch 107/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3213 - acc: 0.8775\n",
      "Epoch 108/200\n",
      "3200/3200 [==============================] - 1s 194us/step - loss: 0.3230 - acc: 0.8769\n",
      "Epoch 109/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3210 - acc: 0.8772\n",
      "Epoch 110/200\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.3208 - acc: 0.8803\n",
      "Epoch 111/200\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.3224 - acc: 0.8753\n",
      "Epoch 112/200\n",
      "3200/3200 [==============================] - 0s 155us/step - loss: 0.3254 - acc: 0.8741\n",
      "Epoch 113/200\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.3258 - acc: 0.8781\n",
      "Epoch 114/200\n",
      "3200/3200 [==============================] - 1s 160us/step - loss: 0.3202 - acc: 0.8787\n",
      "Epoch 115/200\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.3315 - acc: 0.8744\n",
      "Epoch 116/200\n",
      "3200/3200 [==============================] - 0s 155us/step - loss: 0.3233 - acc: 0.8759\n",
      "Epoch 117/200\n",
      "3200/3200 [==============================] - 1s 160us/step - loss: 0.3208 - acc: 0.8772\n",
      "Epoch 118/200\n",
      "3200/3200 [==============================] - 0s 151us/step - loss: 0.3191 - acc: 0.8769\n",
      "Epoch 119/200\n",
      "3200/3200 [==============================] - 1s 161us/step - loss: 0.3200 - acc: 0.8772\n",
      "Epoch 120/200\n",
      "3200/3200 [==============================] - 0s 152us/step - loss: 0.3226 - acc: 0.8762\n",
      "Epoch 121/200\n",
      "3200/3200 [==============================] - 1s 159us/step - loss: 0.3239 - acc: 0.8741\n",
      "Epoch 122/200\n",
      "3200/3200 [==============================] - 1s 158us/step - loss: 0.3217 - acc: 0.8781\n",
      "Epoch 123/200\n",
      "3200/3200 [==============================] - 0s 151us/step - loss: 0.3206 - acc: 0.8769\n",
      "Epoch 124/200\n",
      "3200/3200 [==============================] - 1s 158us/step - loss: 0.3206 - acc: 0.8759\n",
      "Epoch 125/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3286 - acc: 0.8744\n",
      "Epoch 126/200\n",
      "3200/3200 [==============================] - 0s 152us/step - loss: 0.3216 - acc: 0.8769\n",
      "Epoch 127/200\n",
      "3200/3200 [==============================] - 1s 161us/step - loss: 0.3196 - acc: 0.8784\n",
      "Epoch 128/200\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.3193 - acc: 0.8769\n",
      "Epoch 129/200\n",
      "3200/3200 [==============================] - 0s 141us/step - loss: 0.3260 - acc: 0.8750\n",
      "Epoch 130/200\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.3215 - acc: 0.8766\n",
      "Epoch 131/200\n",
      "3200/3200 [==============================] - 0s 153us/step - loss: 0.3192 - acc: 0.8775\n",
      "Epoch 132/200\n",
      "3200/3200 [==============================] - 1s 157us/step - loss: 0.3196 - acc: 0.8800\n",
      "Epoch 133/200\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.3231 - acc: 0.8797\n",
      "Epoch 134/200\n",
      "3200/3200 [==============================] - 1s 159us/step - loss: 0.3208 - acc: 0.8762\n",
      "Epoch 135/200\n",
      "3200/3200 [==============================] - 1s 159us/step - loss: 0.3229 - acc: 0.8747\n",
      "Epoch 136/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3246 - acc: 0.8731\n",
      "Epoch 137/200\n",
      "3200/3200 [==============================] - 1s 161us/step - loss: 0.3298 - acc: 0.8738\n",
      "Epoch 138/200\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.3211 - acc: 0.8769\n",
      "Epoch 139/200\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.3193 - acc: 0.8762\n",
      "Epoch 140/200\n",
      "3200/3200 [==============================] - 0s 148us/step - loss: 0.3205 - acc: 0.8778\n",
      "Epoch 141/200\n",
      "3200/3200 [==============================] - 0s 155us/step - loss: 0.3215 - acc: 0.8784\n",
      "Epoch 142/200\n",
      "3200/3200 [==============================] - 0s 151us/step - loss: 0.3245 - acc: 0.8759\n",
      "Epoch 143/200\n",
      "3200/3200 [==============================] - 0s 147us/step - loss: 0.3206 - acc: 0.8769\n",
      "Epoch 144/200\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.3210 - acc: 0.8759\n",
      "Epoch 145/200\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.3194 - acc: 0.8769\n",
      "Epoch 146/200\n",
      "3200/3200 [==============================] - 1s 160us/step - loss: 0.3175 - acc: 0.8797\n",
      "Epoch 147/200\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.3185 - acc: 0.8791\n",
      "Epoch 148/200\n",
      "3200/3200 [==============================] - 0s 156us/step - loss: 0.3198 - acc: 0.8794\n",
      "Epoch 149/200\n",
      "3200/3200 [==============================] - 0s 153us/step - loss: 0.3208 - acc: 0.8800\n",
      "Epoch 150/200\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.3265 - acc: 0.8744\n",
      "Epoch 151/200\n",
      "3200/3200 [==============================] - 1s 161us/step - loss: 0.3234 - acc: 0.8772\n",
      "Epoch 152/200\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.3244 - acc: 0.8794\n",
      "Epoch 153/200\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.3176 - acc: 0.8775\n",
      "Epoch 154/200\n",
      "3200/3200 [==============================] - 0s 153us/step - loss: 0.3182 - acc: 0.8787\n",
      "Epoch 155/200\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.3184 - acc: 0.8772\n",
      "Epoch 156/200\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.3184 - acc: 0.8800\n",
      "Epoch 157/200\n",
      "3200/3200 [==============================] - 1s 161us/step - loss: 0.3222 - acc: 0.8759\n",
      "Epoch 158/200\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.3224 - acc: 0.8738\n",
      "Epoch 159/200\n",
      "3200/3200 [==============================] - 0s 152us/step - loss: 0.3312 - acc: 0.8753\n",
      "Epoch 160/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3265 - acc: 0.8762\n",
      "Epoch 161/200\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.3201 - acc: 0.8787\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - 0s 153us/step - loss: 0.3223 - acc: 0.8803\n",
      "Epoch 163/200\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.3325 - acc: 0.8694\n",
      "Epoch 164/200\n",
      "3200/3200 [==============================] - 1s 158us/step - loss: 0.3228 - acc: 0.8778\n",
      "Epoch 165/200\n",
      "3200/3200 [==============================] - 0s 146us/step - loss: 0.3192 - acc: 0.8787\n",
      "Epoch 166/200\n",
      "3200/3200 [==============================] - 1s 191us/step - loss: 0.3177 - acc: 0.8784\n",
      "Epoch 167/200\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.3185 - acc: 0.8781\n",
      "Epoch 168/200\n",
      "3200/3200 [==============================] - 0s 154us/step - loss: 0.3177 - acc: 0.8791\n",
      "Epoch 169/200\n",
      "3200/3200 [==============================] - 1s 157us/step - loss: 0.3210 - acc: 0.8791\n",
      "Epoch 170/200\n",
      "3200/3200 [==============================] - 0s 155us/step - loss: 0.3232 - acc: 0.8778\n",
      "Epoch 171/200\n",
      "3200/3200 [==============================] - 1s 161us/step - loss: 0.3222 - acc: 0.8756\n",
      "Epoch 172/200\n",
      "3200/3200 [==============================] - 0s 153us/step - loss: 0.3198 - acc: 0.8778\n",
      "Epoch 173/200\n",
      "3200/3200 [==============================] - 0s 152us/step - loss: 0.3253 - acc: 0.8791\n",
      "Epoch 174/200\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.3222 - acc: 0.8750\n",
      "Epoch 175/200\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.3195 - acc: 0.8775\n",
      "Epoch 176/200\n",
      "3200/3200 [==============================] - 1s 157us/step - loss: 0.3184 - acc: 0.8784\n",
      "Epoch 177/200\n",
      "3200/3200 [==============================] - 1s 156us/step - loss: 0.3198 - acc: 0.8784\n",
      "Epoch 178/200\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.3197 - acc: 0.8781\n",
      "Epoch 179/200\n",
      "3200/3200 [==============================] - 1s 160us/step - loss: 0.3174 - acc: 0.8797\n",
      "Epoch 180/200\n",
      "3200/3200 [==============================] - 1s 160us/step - loss: 0.3215 - acc: 0.8766\n",
      "Epoch 181/200\n",
      "3200/3200 [==============================] - 0s 154us/step - loss: 0.3234 - acc: 0.8781\n",
      "Epoch 182/200\n",
      "3200/3200 [==============================] - 0s 147us/step - loss: 0.3196 - acc: 0.8781\n",
      "Epoch 183/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3181 - acc: 0.8775\n",
      "Epoch 184/200\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.3193 - acc: 0.8787\n",
      "Epoch 185/200\n",
      "3200/3200 [==============================] - 1s 158us/step - loss: 0.3265 - acc: 0.8766\n",
      "Epoch 186/200\n",
      "3200/3200 [==============================] - 1s 161us/step - loss: 0.3222 - acc: 0.8762\n",
      "Epoch 187/200\n",
      "3200/3200 [==============================] - 0s 140us/step - loss: 0.3173 - acc: 0.8803\n",
      "Epoch 188/200\n",
      "3200/3200 [==============================] - 0s 155us/step - loss: 0.3179 - acc: 0.8787\n",
      "Epoch 189/200\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.3208 - acc: 0.8766\n",
      "Epoch 190/200\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.3269 - acc: 0.8713\n",
      "Epoch 191/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3170 - acc: 0.8797\n",
      "Epoch 192/200\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.3256 - acc: 0.8713\n",
      "Epoch 193/200\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.3197 - acc: 0.8769\n",
      "Epoch 194/200\n",
      "3200/3200 [==============================] - 1s 181us/step - loss: 0.3234 - acc: 0.8759\n",
      "Epoch 195/200\n",
      "3200/3200 [==============================] - 0s 154us/step - loss: 0.3206 - acc: 0.8738\n",
      "Epoch 196/200\n",
      "3200/3200 [==============================] - 0s 154us/step - loss: 0.3196 - acc: 0.8766\n",
      "Epoch 197/200\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.3168 - acc: 0.8775\n",
      "Epoch 198/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3210 - acc: 0.8766\n",
      "Epoch 199/200\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.3238 - acc: 0.8744\n",
      "Epoch 200/200\n",
      "3200/3200 [==============================] - 1s 158us/step - loss: 0.3179 - acc: 0.8787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd140388c50>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "model.fit(x[:3200],y_train[:3200],batch_size=64,epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/355 [==============================] - 0s 337us/step\n",
      "test loss 0.2047892114226247\n",
      "test accuracy 0.9183098591549296\n"
     ]
    }
   ],
   "source": [
    "# 评估模型\n",
    "loss,accuracy = model.evaluate(x[3200:],y_test)\n",
    "\n",
    "print('test loss',loss)\n",
    "print('test accuracy',accuracy)\n",
    "\n",
    "p = model.predict(x[3200:])\n",
    "\n",
    "p[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 256)               266240    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 200)               51400     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 318,042\n",
      "Trainable params: 318,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 创建模型\n",
    "model = Sequential()\n",
    "\n",
    "# 循环神经网络\n",
    "model.add(LSTM(\n",
    "    units = 256, # 输出\n",
    "    input_shape = (180,3), #输入\n",
    "))\n",
    "model.add(Dense(200,activation='tanh'))\n",
    "# 输出层\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "# 定义优化器\n",
    "adam = Adam(lr=1e-4)\n",
    "\n",
    "# 定义优化器，loss function，训练过程中计算准确率\n",
    "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "3000/3000 [==============================] - 17s 6ms/step - loss: 0.5505 - acc: 0.7250\n",
      "Epoch 2/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4307 - acc: 0.8620\n",
      "Epoch 3/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4145 - acc: 0.8620\n",
      "Epoch 4/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4102 - acc: 0.8620\n",
      "Epoch 5/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4057 - acc: 0.8620\n",
      "Epoch 6/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4021 - acc: 0.8620\n",
      "Epoch 7/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3996 - acc: 0.8620\n",
      "Epoch 8/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3972 - acc: 0.8620\n",
      "Epoch 9/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3955 - acc: 0.8620\n",
      "Epoch 10/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3932 - acc: 0.8620\n",
      "Epoch 11/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3908 - acc: 0.8620\n",
      "Epoch 12/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3893 - acc: 0.8620\n",
      "Epoch 13/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3891 - acc: 0.8620\n",
      "Epoch 14/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3931 - acc: 0.8620\n",
      "Epoch 15/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3903 - acc: 0.8620\n",
      "Epoch 16/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3901 - acc: 0.8620\n",
      "Epoch 17/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3894 - acc: 0.8620\n",
      "Epoch 18/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3856 - acc: 0.8620\n",
      "Epoch 19/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3866 - acc: 0.8620\n",
      "Epoch 20/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3833 - acc: 0.8620\n",
      "Epoch 21/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3776 - acc: 0.8620\n",
      "Epoch 22/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3763 - acc: 0.8620\n",
      "Epoch 23/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3699 - acc: 0.8620\n",
      "Epoch 24/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3699 - acc: 0.8617\n",
      "Epoch 25/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3614 - acc: 0.8617\n",
      "Epoch 26/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3631 - acc: 0.8620\n",
      "Epoch 27/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3649 - acc: 0.8610\n",
      "Epoch 28/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3610 - acc: 0.8593\n",
      "Epoch 29/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3763 - acc: 0.8607\n",
      "Epoch 30/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3628 - acc: 0.8617\n",
      "Epoch 31/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3535 - acc: 0.8613\n",
      "Epoch 32/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3565 - acc: 0.8627\n",
      "Epoch 33/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3591 - acc: 0.8627\n",
      "Epoch 34/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3532 - acc: 0.8627\n",
      "Epoch 35/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3595 - acc: 0.8610\n",
      "Epoch 36/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3575 - acc: 0.8633\n",
      "Epoch 37/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3536 - acc: 0.8613\n",
      "Epoch 38/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3498 - acc: 0.8637\n",
      "Epoch 39/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3478 - acc: 0.8620\n",
      "Epoch 40/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3467 - acc: 0.8617\n",
      "Epoch 41/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3494 - acc: 0.8633\n",
      "Epoch 42/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3465 - acc: 0.8643\n",
      "Epoch 43/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3446 - acc: 0.8623\n",
      "Epoch 44/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3437 - acc: 0.8653\n",
      "Epoch 45/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3434 - acc: 0.8650\n",
      "Epoch 46/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3474 - acc: 0.8667\n",
      "Epoch 47/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3514 - acc: 0.8663\n",
      "Epoch 48/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3456 - acc: 0.8650\n",
      "Epoch 49/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3412 - acc: 0.8677\n",
      "Epoch 50/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3414 - acc: 0.8663\n",
      "Epoch 51/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3388 - acc: 0.8667\n",
      "Epoch 52/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3374 - acc: 0.8667\n",
      "Epoch 53/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3412 - acc: 0.8693\n",
      "Epoch 54/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3405 - acc: 0.8683\n",
      "Epoch 55/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3371 - acc: 0.8687\n",
      "Epoch 56/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3360 - acc: 0.8700\n",
      "Epoch 57/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3355 - acc: 0.8670\n",
      "Epoch 58/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3443 - acc: 0.8663\n",
      "Epoch 59/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3396 - acc: 0.8687\n",
      "Epoch 60/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3375 - acc: 0.8687\n",
      "Epoch 61/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3333 - acc: 0.8690\n",
      "Epoch 62/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3312 - acc: 0.8700\n",
      "Epoch 63/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3279 - acc: 0.8733\n",
      "Epoch 64/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3381 - acc: 0.8637\n",
      "Epoch 65/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3334 - acc: 0.8707\n",
      "Epoch 66/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3287 - acc: 0.8687\n",
      "Epoch 67/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3351 - acc: 0.8727\n",
      "Epoch 68/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3627 - acc: 0.8643\n",
      "Epoch 69/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3439 - acc: 0.8687\n",
      "Epoch 70/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3299 - acc: 0.8657\n",
      "Epoch 71/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3305 - acc: 0.8730\n",
      "Epoch 72/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3239 - acc: 0.8737\n",
      "Epoch 73/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3306 - acc: 0.8673\n",
      "Epoch 74/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3376 - acc: 0.8673\n",
      "Epoch 75/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3252 - acc: 0.8717\n",
      "Epoch 76/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3323 - acc: 0.8737\n",
      "Epoch 77/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3225 - acc: 0.8707\n",
      "Epoch 78/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3211 - acc: 0.8690\n",
      "Epoch 79/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3228 - acc: 0.8703\n",
      "Epoch 80/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3160 - acc: 0.8750\n",
      "Epoch 81/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3116 - acc: 0.8750\n",
      "Epoch 82/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3266 - acc: 0.8637\n",
      "Epoch 83/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3177 - acc: 0.8747\n",
      "Epoch 84/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3107 - acc: 0.8753\n",
      "Epoch 85/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3103 - acc: 0.8710\n",
      "Epoch 86/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3100 - acc: 0.8730\n",
      "Epoch 87/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3143 - acc: 0.8730\n",
      "Epoch 88/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3212 - acc: 0.8693\n",
      "Epoch 89/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3398 - acc: 0.8610\n",
      "Epoch 90/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3390 - acc: 0.8670\n",
      "Epoch 91/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3566 - acc: 0.8627\n",
      "Epoch 92/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3418 - acc: 0.8707\n",
      "Epoch 93/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3375 - acc: 0.8713\n",
      "Epoch 94/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3367 - acc: 0.8703\n",
      "Epoch 95/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3290 - acc: 0.8710\n",
      "Epoch 96/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3194 - acc: 0.8743\n",
      "Epoch 97/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3206 - acc: 0.8707\n",
      "Epoch 98/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3168 - acc: 0.8720\n",
      "Epoch 99/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3097 - acc: 0.8730\n",
      "Epoch 100/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3259 - acc: 0.8740\n",
      "Epoch 101/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3195 - acc: 0.8720\n",
      "Epoch 102/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3116 - acc: 0.8790\n",
      "Epoch 103/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3083 - acc: 0.8730\n",
      "Epoch 104/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2999 - acc: 0.8793\n",
      "Epoch 105/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3119 - acc: 0.8717\n",
      "Epoch 106/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3108 - acc: 0.8760\n",
      "Epoch 107/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3006 - acc: 0.8770\n",
      "Epoch 108/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3006 - acc: 0.8753\n",
      "Epoch 109/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2952 - acc: 0.8800\n",
      "Epoch 110/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3151 - acc: 0.8757\n",
      "Epoch 111/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3020 - acc: 0.8737\n",
      "Epoch 112/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2953 - acc: 0.8797\n",
      "Epoch 113/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2970 - acc: 0.8803\n",
      "Epoch 114/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3029 - acc: 0.8780\n",
      "Epoch 115/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2986 - acc: 0.8763\n",
      "Epoch 116/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2917 - acc: 0.8813\n",
      "Epoch 117/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2915 - acc: 0.8813\n",
      "Epoch 118/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2854 - acc: 0.8837\n",
      "Epoch 119/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2877 - acc: 0.8807\n",
      "Epoch 120/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2943 - acc: 0.8783\n",
      "Epoch 121/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2844 - acc: 0.8833\n",
      "Epoch 122/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2916 - acc: 0.8763\n",
      "Epoch 123/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2869 - acc: 0.8793\n",
      "Epoch 124/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2995 - acc: 0.8760\n",
      "Epoch 125/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2892 - acc: 0.8787\n",
      "Epoch 126/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2872 - acc: 0.8790\n",
      "Epoch 127/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2798 - acc: 0.8823\n",
      "Epoch 128/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2794 - acc: 0.8813\n",
      "Epoch 129/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2793 - acc: 0.8820\n",
      "Epoch 130/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2834 - acc: 0.8807\n",
      "Epoch 131/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2789 - acc: 0.8817\n",
      "Epoch 132/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2817 - acc: 0.8787\n",
      "Epoch 133/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2834 - acc: 0.8833\n",
      "Epoch 134/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2835 - acc: 0.8813\n",
      "Epoch 135/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2735 - acc: 0.8893\n",
      "Epoch 136/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2791 - acc: 0.8830\n",
      "Epoch 137/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2738 - acc: 0.8867\n",
      "Epoch 138/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2741 - acc: 0.8833\n",
      "Epoch 139/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2747 - acc: 0.8850\n",
      "Epoch 140/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2668 - acc: 0.8890\n",
      "Epoch 141/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2677 - acc: 0.8883\n",
      "Epoch 142/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2714 - acc: 0.8853\n",
      "Epoch 143/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2616 - acc: 0.8880\n",
      "Epoch 144/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2680 - acc: 0.8853\n",
      "Epoch 145/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2702 - acc: 0.8843\n",
      "Epoch 146/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2635 - acc: 0.8897\n",
      "Epoch 147/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2640 - acc: 0.8903\n",
      "Epoch 148/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2636 - acc: 0.8900\n",
      "Epoch 149/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2591 - acc: 0.8887\n",
      "Epoch 150/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2610 - acc: 0.8870\n",
      "Epoch 151/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2727 - acc: 0.8813\n",
      "Epoch 152/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2626 - acc: 0.8900\n",
      "Epoch 153/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2633 - acc: 0.8883\n",
      "Epoch 154/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2599 - acc: 0.8890\n",
      "Epoch 155/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2567 - acc: 0.8893\n",
      "Epoch 156/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2668 - acc: 0.8817\n",
      "Epoch 157/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2615 - acc: 0.8870\n",
      "Epoch 158/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2575 - acc: 0.8910\n",
      "Epoch 159/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2634 - acc: 0.8840\n",
      "Epoch 160/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2599 - acc: 0.8873\n",
      "Epoch 161/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2572 - acc: 0.8897\n",
      "Epoch 162/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2657 - acc: 0.8807\n",
      "Epoch 163/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2623 - acc: 0.8883\n",
      "Epoch 164/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2593 - acc: 0.8930\n",
      "Epoch 165/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2566 - acc: 0.8897\n",
      "Epoch 166/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2516 - acc: 0.8897\n",
      "Epoch 167/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2464 - acc: 0.8940\n",
      "Epoch 168/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2489 - acc: 0.8867\n",
      "Epoch 169/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2503 - acc: 0.8910\n",
      "Epoch 170/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2621 - acc: 0.8870\n",
      "Epoch 171/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2522 - acc: 0.8930\n",
      "Epoch 172/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2531 - acc: 0.8907\n",
      "Epoch 173/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2470 - acc: 0.8923\n",
      "Epoch 174/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2461 - acc: 0.8950\n",
      "Epoch 175/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2492 - acc: 0.8880\n",
      "Epoch 176/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2481 - acc: 0.8953\n",
      "Epoch 177/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2492 - acc: 0.8937\n",
      "Epoch 178/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2482 - acc: 0.8867\n",
      "Epoch 179/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2394 - acc: 0.8947\n",
      "Epoch 180/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2613 - acc: 0.8920\n",
      "Epoch 181/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2567 - acc: 0.8893\n",
      "Epoch 182/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2468 - acc: 0.8933\n",
      "Epoch 183/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2430 - acc: 0.8913\n",
      "Epoch 184/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2391 - acc: 0.8940\n",
      "Epoch 185/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2469 - acc: 0.8913\n",
      "Epoch 186/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2471 - acc: 0.8933\n",
      "Epoch 187/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2470 - acc: 0.8910\n",
      "Epoch 188/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2457 - acc: 0.8933\n",
      "Epoch 189/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2389 - acc: 0.8937\n",
      "Epoch 190/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2352 - acc: 0.8960\n",
      "Epoch 191/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2404 - acc: 0.8890\n",
      "Epoch 192/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2381 - acc: 0.8943\n",
      "Epoch 193/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2422 - acc: 0.8983\n",
      "Epoch 194/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2386 - acc: 0.8977\n",
      "Epoch 195/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2358 - acc: 0.8943\n",
      "Epoch 196/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2399 - acc: 0.8930\n",
      "Epoch 197/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2423 - acc: 0.8903\n",
      "Epoch 198/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2295 - acc: 0.8957\n",
      "Epoch 199/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2339 - acc: 0.8953\n",
      "Epoch 200/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2336 - acc: 0.8977\n",
      "Epoch 201/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2330 - acc: 0.8987\n",
      "Epoch 202/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2360 - acc: 0.8983\n",
      "Epoch 203/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2374 - acc: 0.8987\n",
      "Epoch 204/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2320 - acc: 0.8987\n",
      "Epoch 205/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2297 - acc: 0.8987\n",
      "Epoch 206/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2421 - acc: 0.8893\n",
      "Epoch 207/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2406 - acc: 0.8907\n",
      "Epoch 208/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2392 - acc: 0.8933\n",
      "Epoch 209/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2334 - acc: 0.8973\n",
      "Epoch 210/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2269 - acc: 0.8967\n",
      "Epoch 211/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2298 - acc: 0.9000\n",
      "Epoch 212/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2260 - acc: 0.8960\n",
      "Epoch 213/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2272 - acc: 0.8990\n",
      "Epoch 214/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2306 - acc: 0.8970\n",
      "Epoch 215/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2352 - acc: 0.8940\n",
      "Epoch 216/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2280 - acc: 0.8977\n",
      "Epoch 217/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2232 - acc: 0.8967\n",
      "Epoch 218/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2278 - acc: 0.8970\n",
      "Epoch 219/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2284 - acc: 0.8960\n",
      "Epoch 220/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2327 - acc: 0.8977\n",
      "Epoch 221/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2315 - acc: 0.8953\n",
      "Epoch 222/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2247 - acc: 0.8990\n",
      "Epoch 223/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2219 - acc: 0.8987\n",
      "Epoch 224/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2212 - acc: 0.9003\n",
      "Epoch 225/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2271 - acc: 0.8993\n",
      "Epoch 226/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2259 - acc: 0.9033\n",
      "Epoch 227/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2321 - acc: 0.8973\n",
      "Epoch 228/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2325 - acc: 0.8920\n",
      "Epoch 229/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2243 - acc: 0.9000\n",
      "Epoch 230/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2174 - acc: 0.9050\n",
      "Epoch 231/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2194 - acc: 0.9063\n",
      "Epoch 232/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2194 - acc: 0.9043\n",
      "Epoch 233/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2210 - acc: 0.9000\n",
      "Epoch 234/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2287 - acc: 0.8973\n",
      "Epoch 235/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2261 - acc: 0.8970\n",
      "Epoch 236/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2290 - acc: 0.8953\n",
      "Epoch 237/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2233 - acc: 0.9033\n",
      "Epoch 238/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2233 - acc: 0.8990\n",
      "Epoch 239/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2182 - acc: 0.9020\n",
      "Epoch 240/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2220 - acc: 0.9003\n",
      "Epoch 241/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2228 - acc: 0.9000\n",
      "Epoch 242/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2247 - acc: 0.9007\n",
      "Epoch 243/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2276 - acc: 0.9043\n",
      "Epoch 244/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2321 - acc: 0.8983\n",
      "Epoch 245/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2293 - acc: 0.8993\n",
      "Epoch 246/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2323 - acc: 0.9003\n",
      "Epoch 247/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2233 - acc: 0.9003\n",
      "Epoch 248/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2194 - acc: 0.9057\n",
      "Epoch 249/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2221 - acc: 0.9013\n",
      "Epoch 250/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2169 - acc: 0.8990\n",
      "Epoch 251/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2133 - acc: 0.9040\n",
      "Epoch 252/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2185 - acc: 0.9000\n",
      "Epoch 253/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2176 - acc: 0.8997\n",
      "Epoch 254/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2182 - acc: 0.9013\n",
      "Epoch 255/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2308 - acc: 0.8967\n",
      "Epoch 256/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2259 - acc: 0.8980\n",
      "Epoch 257/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2239 - acc: 0.9027\n",
      "Epoch 258/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2267 - acc: 0.8993\n",
      "Epoch 259/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2256 - acc: 0.9020\n",
      "Epoch 260/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2178 - acc: 0.9017\n",
      "Epoch 261/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2131 - acc: 0.9053\n",
      "Epoch 262/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2166 - acc: 0.9030\n",
      "Epoch 263/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2146 - acc: 0.9033\n",
      "Epoch 264/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2087 - acc: 0.9060\n",
      "Epoch 265/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2096 - acc: 0.9050\n",
      "Epoch 266/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2127 - acc: 0.9053\n",
      "Epoch 267/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2130 - acc: 0.9067\n",
      "Epoch 268/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2119 - acc: 0.9027\n",
      "Epoch 269/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2083 - acc: 0.8987\n",
      "Epoch 270/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2117 - acc: 0.9057\n",
      "Epoch 271/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2117 - acc: 0.8997\n",
      "Epoch 272/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2069 - acc: 0.9083\n",
      "Epoch 273/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2052 - acc: 0.9027\n",
      "Epoch 274/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2094 - acc: 0.9063\n",
      "Epoch 275/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2102 - acc: 0.9030\n",
      "Epoch 276/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2079 - acc: 0.9030\n",
      "Epoch 277/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2044 - acc: 0.9063\n",
      "Epoch 278/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2080 - acc: 0.9027\n",
      "Epoch 279/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2047 - acc: 0.9050\n",
      "Epoch 280/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2141 - acc: 0.9030\n",
      "Epoch 281/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2183 - acc: 0.9047\n",
      "Epoch 282/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2103 - acc: 0.9050\n",
      "Epoch 283/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2127 - acc: 0.9063\n",
      "Epoch 284/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2058 - acc: 0.9103\n",
      "Epoch 285/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2083 - acc: 0.9040\n",
      "Epoch 286/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2034 - acc: 0.9017\n",
      "Epoch 287/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2130 - acc: 0.9053\n",
      "Epoch 288/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2192 - acc: 0.9003\n",
      "Epoch 289/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2122 - acc: 0.9073\n",
      "Epoch 290/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2139 - acc: 0.8990\n",
      "Epoch 291/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2010 - acc: 0.9057\n",
      "Epoch 292/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2018 - acc: 0.9080\n",
      "Epoch 293/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2041 - acc: 0.9093\n",
      "Epoch 294/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2052 - acc: 0.9093\n",
      "Epoch 295/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.1984 - acc: 0.9103\n",
      "Epoch 296/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.1977 - acc: 0.9107\n",
      "Epoch 297/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2025 - acc: 0.9077\n",
      "Epoch 298/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2036 - acc: 0.9060\n",
      "Epoch 299/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2055 - acc: 0.9047\n",
      "Epoch 300/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2024 - acc: 0.9050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcfbc6f9198>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "model.fit(x[:3000],y_train[:3000],batch_size=128,epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405/405 [==============================] - 1s 3ms/step\n",
      "test loss 0.30328865814192313\n",
      "test accuracy 0.9703703708118863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.9999690e-01, 3.0603428e-06],\n",
       "       [9.9999666e-01, 3.3833219e-06],\n",
       "       [9.9999499e-01, 4.9805617e-06],\n",
       "       [9.9998999e-01, 9.9705348e-06],\n",
       "       [9.9999225e-01, 7.7104933e-06],\n",
       "       [9.9999189e-01, 8.0543050e-06],\n",
       "       [9.9999452e-01, 5.5148798e-06],\n",
       "       [9.9999678e-01, 3.2015703e-06],\n",
       "       [9.9999368e-01, 6.3325760e-06],\n",
       "       [9.9998081e-01, 1.9225445e-05]], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 评估模型\n",
    "loss,accuracy = model.evaluate(x[3000:],y11[3000:])\n",
    "\n",
    "print('test loss',loss)\n",
    "print('test accuracy',accuracy)\n",
    "\n",
    "p = model.predict(x[3000:])\n",
    "\n",
    "p[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
