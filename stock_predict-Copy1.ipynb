{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "做未来做预测\n",
    "\n",
    "现在比较流行的方式是预测股价，我觉得不是很靠谱。\n",
    "\n",
    "## 我想到的一个是找最低点。\n",
    "这样的话，问题就由一个回归问题变为一个分类问题。  \n",
    "如以过去N天的(股价，成交量，时间（季度、月份），其它股票、指数、汇率、油价，等等)组成一个特征序列，来预测次日的股价是否是最低点。\n",
    "最低点的判断依据：未来T天的股价收盘价均不小于预测序列最后一日的收盘价，且未来T天的收盘价最高点大于预测序列最后一日的收盘价的Z%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/installed/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ian/installed/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tushare as ts\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.optimizers import Adam\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tt:\n",
    "    def __init__(self):\n",
    "        ts.set_token('5fd1639100f8a22b7f86e882e03192009faa72bae1ae93803e1172d5')\n",
    "        self._pro = ts.pro_api()\n",
    "        \n",
    "    def index_daily(self, ts_code, start_date, end_date):\n",
    "        '''\n",
    "        获取行情数据\n",
    "        由于ts的接口一次只能获取1800个交易日（一年大概有250个交易日。约7年）的数据\n",
    "        '''\n",
    "        startdate = datetime.strptime(start_date, '%Y%m%d')\n",
    "        enddate = datetime.strptime(end_date, '%Y%m%d')\n",
    "        df = pd.DataFrame()\n",
    "        while enddate.year-startdate.year>6:      \n",
    "            print(startdate.strftime('%Y%m%d'), (startdate.replace(year=(startdate.year+6))-timedelta(days=1)).strftime('%Y%m%d'))\n",
    "            t = self._pro.index_daily(ts_code=ts_code, start_date=startdate.strftime('%Y%m%d'), end_date=(startdate.replace(year=(startdate.year+6))-timedelta(days=1)).strftime('%Y%m%d'))\n",
    "            if not df.empty:\n",
    "                df = pd.concat([df,t], axis=0)\n",
    "            else:\n",
    "                df = t\n",
    "            startdate = startdate.replace(year=(startdate.year+6))\n",
    "        else:\n",
    "            print(startdate.strftime('%Y%m%d'),end_date)\n",
    "            t = self._pro.index_daily(ts_code=ts_code, start_date=startdate.strftime('%Y%m%d'), end_date=end_date)\n",
    "            if not df.empty:\n",
    "                df = pd.concat([df,t], axis=0)\n",
    "            else:\n",
    "                df = t\n",
    "        return df.sort_values('trade_date')\n",
    "    def normlize_field(self, df, fieldnamelist):\n",
    "        '''\n",
    "        #设df中某一字段 第一日净值为1\n",
    "        '''\n",
    "        df1 = pd.DataFrame()\n",
    "        df1['ts_code'] = df['ts_code']\n",
    "        df1['trade_date'] = df['trade_date']\n",
    "        for f in fieldnamelist:\n",
    "            df1[f+'_1'] = df.apply(lambda x: x[f]/df.iloc[0][f], axis=1)        \n",
    "        return df1\n",
    "    def build_flag(self, df,series_len,pro_len, fieldnamelist):\n",
    "        '''\n",
    "        构建训练集\n",
    "\n",
    "        series_len: 参考的之前的序列范围。如以之前的series_len个序列预测下一个序列，则series_len=series_len\n",
    "        pro_len: 预测日以后的天数（含预测日）\n",
    "        '''\n",
    "        r = list()\n",
    "        l1 = list(df['close_1'])\n",
    "        for i in range(series_len,(df.shape[0]-pro_len)):\n",
    "            final_list = list()\n",
    "            laa = l1[i:i+pro_len]\n",
    "            #最低点和买点的关系：最低点一定是买点，买点不一定是最低点\n",
    "            #买点特征\n",
    "            #买点日之后14日最高收盘价涨幅超过0.05，最低价不得低于买点日收盘价\n",
    "            #最低点的特征：在买点特征的基础上\n",
    "            #最低日收盘价低于前一日收盘价\n",
    "\n",
    "            f1 = 0 # 买点标志 1表示买点\n",
    "            f2 = 0 # 最低点标志 1表示最低点\n",
    "            if (max(laa) - l1[i])/l1[i] > 0.05 and min(laa[1:]) > l1[i]:#未来pro_len日最高收盘价涨幅超过0.05\n",
    "                f1 = 1\n",
    "                if l1[i] < l1[i-1]:\n",
    "                    f2 = 1\n",
    "            final_list.append(df[fieldnamelist].values[(i-series_len) : i])\n",
    "            final_list.append(f1)\n",
    "            final_list.append(f2)\n",
    "            r.append(final_list)    \n",
    "        return r\n",
    "    def build_x(self, df,series_len, start):\n",
    "        '''\n",
    "        构建预测序列\n",
    "\n",
    "        series_len: 参考的之前的序列范围。如以之前的series_len个序列预测下一个序列，则series_len=series_len\n",
    "        '''\n",
    "        ll = list()\n",
    "        l1 = list(df['close_1'])\n",
    "        l4 = list(df['vol_1'])\n",
    "        l5 = list(df['amount_1'])\n",
    "        for i in range(df.shape[0]-start,df.shape[0]):\n",
    "            final_list = list()\n",
    "            l2 = l1[i-series_len : i]\n",
    "            ll.append(list(zip(l1[i-series_len : i],l4[i-series_len : i],l5[i-series_len : i])))    \n",
    "        return ll\n",
    "    def getNum(self, ll):\n",
    "        '''\n",
    "        获取买点、最低点的个数\n",
    "        '''\n",
    "        y = [(i[-2],i[-1]) for i in ll]\n",
    "        return Counter(y)\n",
    "    def preprocess(self,ll):\n",
    "        '''\n",
    "        数据预处理，获得可用于训练的set\n",
    "        '''\n",
    "        ll1 = [i for i in ll if i[1]==1]#买点数据\n",
    "        #均衡数据\n",
    "        ll2 = ll + ll1* (round(len(ll)/len(ll1))-1)\n",
    "        ll3 = [(i[1],i[2]) for i in ll2]\n",
    "        x = np.array([i[0] for i in ll2])\n",
    "        y1 = np.array([[i[1]] for i in ll2])#买点\n",
    "        y2 = np.array([[i[2]] for i in ll2])#最低点\n",
    "        print(Counter(ll3))\n",
    "        return x,np_utils.to_categorical(y1,num_classes=2),np_utils.to_categorical(y2,num_classes=2)\n",
    "    def splitData(self,x,y):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "        return x_train, x_test, y_train, y_test\n",
    "    def buildModel(self,):\n",
    "        # 创建模型\n",
    "        model = Sequential()\n",
    "\n",
    "        # 循环神经网络\n",
    "        model.add(GRU(\n",
    "            units = 256, # 输出\n",
    "            input_shape = (180,9), #输入\n",
    "        ))\n",
    "        model.add(Dense(200,activation='tanh'))\n",
    "        # 输出层\n",
    "        model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "        # 定义优化器\n",
    "        adam = Adam(lr=1e-4)\n",
    "\n",
    "        # 定义优化器，loss function，训练过程中计算准确率\n",
    "        model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "        print(model.summary())\n",
    "        self._model = model\n",
    "        return model\n",
    "    def train(self,x_train,y_train,batch_size=128,epochs=300):\n",
    "        # 训练模型\n",
    "        start = time.time()\n",
    "        self._model.fit(x_train,y_train,batch_size,epochs)\n",
    "        print('@ Total Time Spent: %.2f seconds' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20010101 20061231\n",
      "20070101 20121231\n",
      "20130101 20181130\n"
     ]
    }
   ],
   "source": [
    "df = t.index_daily('000001.SH',start_date='20010101', end_date='20181130')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_code</th>\n",
       "      <th>trade_date</th>\n",
       "      <th>close</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>pre_close</th>\n",
       "      <th>change</th>\n",
       "      <th>pct_chg</th>\n",
       "      <th>vol</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010102</td>\n",
       "      <td>2103.4690</td>\n",
       "      <td>2077.0760</td>\n",
       "      <td>2103.5380</td>\n",
       "      <td>2074.8820</td>\n",
       "      <td>2073.4770</td>\n",
       "      <td>29.9920</td>\n",
       "      <td>1.4465</td>\n",
       "      <td>18650746.0</td>\n",
       "      <td>1.340509e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010103</td>\n",
       "      <td>2123.8990</td>\n",
       "      <td>2108.2540</td>\n",
       "      <td>2123.9000</td>\n",
       "      <td>2105.4970</td>\n",
       "      <td>2103.4690</td>\n",
       "      <td>20.4300</td>\n",
       "      <td>0.9713</td>\n",
       "      <td>17157673.0</td>\n",
       "      <td>1.751683e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010104</td>\n",
       "      <td>2117.4050</td>\n",
       "      <td>2125.4850</td>\n",
       "      <td>2127.4190</td>\n",
       "      <td>2113.1790</td>\n",
       "      <td>2123.8990</td>\n",
       "      <td>-6.4940</td>\n",
       "      <td>-0.3058</td>\n",
       "      <td>13423541.0</td>\n",
       "      <td>1.429525e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010105</td>\n",
       "      <td>2125.3000</td>\n",
       "      <td>2117.6640</td>\n",
       "      <td>2125.3810</td>\n",
       "      <td>2114.0560</td>\n",
       "      <td>2117.4050</td>\n",
       "      <td>7.8950</td>\n",
       "      <td>0.3729</td>\n",
       "      <td>13288175.0</td>\n",
       "      <td>1.484424e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010108</td>\n",
       "      <td>2102.0640</td>\n",
       "      <td>2128.6020</td>\n",
       "      <td>2131.9800</td>\n",
       "      <td>2092.0970</td>\n",
       "      <td>2125.3000</td>\n",
       "      <td>-23.2360</td>\n",
       "      <td>-1.0933</td>\n",
       "      <td>14787085.0</td>\n",
       "      <td>1.588766e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010109</td>\n",
       "      <td>2101.1350</td>\n",
       "      <td>2099.3230</td>\n",
       "      <td>2111.1000</td>\n",
       "      <td>2082.8410</td>\n",
       "      <td>2102.0640</td>\n",
       "      <td>-0.9290</td>\n",
       "      <td>-0.0442</td>\n",
       "      <td>11649338.0</td>\n",
       "      <td>1.158177e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010110</td>\n",
       "      <td>2125.6150</td>\n",
       "      <td>2102.0850</td>\n",
       "      <td>2125.6700</td>\n",
       "      <td>2102.0660</td>\n",
       "      <td>2101.1350</td>\n",
       "      <td>24.4800</td>\n",
       "      <td>1.1651</td>\n",
       "      <td>12916883.0</td>\n",
       "      <td>1.302625e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010111</td>\n",
       "      <td>2119.1410</td>\n",
       "      <td>2129.6100</td>\n",
       "      <td>2131.3680</td>\n",
       "      <td>2117.0290</td>\n",
       "      <td>2125.6150</td>\n",
       "      <td>-6.4740</td>\n",
       "      <td>-0.3046</td>\n",
       "      <td>14072209.0</td>\n",
       "      <td>1.461571e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010112</td>\n",
       "      <td>2104.7470</td>\n",
       "      <td>2116.8460</td>\n",
       "      <td>2117.7270</td>\n",
       "      <td>2100.2730</td>\n",
       "      <td>2119.1410</td>\n",
       "      <td>-14.3940</td>\n",
       "      <td>-0.6792</td>\n",
       "      <td>10660882.0</td>\n",
       "      <td>1.101729e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010115</td>\n",
       "      <td>2032.4430</td>\n",
       "      <td>2097.0910</td>\n",
       "      <td>2097.0920</td>\n",
       "      <td>2030.2880</td>\n",
       "      <td>2104.7470</td>\n",
       "      <td>-72.3040</td>\n",
       "      <td>-3.4353</td>\n",
       "      <td>14998588.0</td>\n",
       "      <td>1.341256e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010116</td>\n",
       "      <td>2045.8870</td>\n",
       "      <td>2018.2980</td>\n",
       "      <td>2045.9460</td>\n",
       "      <td>2007.7060</td>\n",
       "      <td>2032.4430</td>\n",
       "      <td>13.4440</td>\n",
       "      <td>0.6615</td>\n",
       "      <td>11583722.0</td>\n",
       "      <td>1.040591e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010117</td>\n",
       "      <td>2034.5840</td>\n",
       "      <td>2046.3770</td>\n",
       "      <td>2048.6950</td>\n",
       "      <td>2031.7560</td>\n",
       "      <td>2045.8870</td>\n",
       "      <td>-11.3030</td>\n",
       "      <td>-0.5525</td>\n",
       "      <td>7324134.0</td>\n",
       "      <td>6.609476e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010118</td>\n",
       "      <td>2043.1100</td>\n",
       "      <td>2033.6860</td>\n",
       "      <td>2049.3550</td>\n",
       "      <td>2033.5760</td>\n",
       "      <td>2034.5840</td>\n",
       "      <td>8.5260</td>\n",
       "      <td>0.4191</td>\n",
       "      <td>10341571.0</td>\n",
       "      <td>8.381613e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010119</td>\n",
       "      <td>2065.6060</td>\n",
       "      <td>2046.5810</td>\n",
       "      <td>2067.8770</td>\n",
       "      <td>2046.5410</td>\n",
       "      <td>2043.1100</td>\n",
       "      <td>22.4960</td>\n",
       "      <td>1.1011</td>\n",
       "      <td>11831264.0</td>\n",
       "      <td>8.601583e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010205</td>\n",
       "      <td>2008.0320</td>\n",
       "      <td>2069.8560</td>\n",
       "      <td>2072.5470</td>\n",
       "      <td>2005.6830</td>\n",
       "      <td>2065.6060</td>\n",
       "      <td>-57.5740</td>\n",
       "      <td>-2.7873</td>\n",
       "      <td>8974243.0</td>\n",
       "      <td>7.400294e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010206</td>\n",
       "      <td>1995.3150</td>\n",
       "      <td>1999.7230</td>\n",
       "      <td>1999.7250</td>\n",
       "      <td>1956.1780</td>\n",
       "      <td>2008.0320</td>\n",
       "      <td>-12.7170</td>\n",
       "      <td>-0.6333</td>\n",
       "      <td>9528427.0</td>\n",
       "      <td>7.974362e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010207</td>\n",
       "      <td>1979.9390</td>\n",
       "      <td>1995.6030</td>\n",
       "      <td>1998.2500</td>\n",
       "      <td>1978.5990</td>\n",
       "      <td>1995.3150</td>\n",
       "      <td>-15.3760</td>\n",
       "      <td>-0.7706</td>\n",
       "      <td>5882229.0</td>\n",
       "      <td>5.171536e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010208</td>\n",
       "      <td>1930.1380</td>\n",
       "      <td>1977.5590</td>\n",
       "      <td>1986.8980</td>\n",
       "      <td>1926.4000</td>\n",
       "      <td>1979.9390</td>\n",
       "      <td>-49.8010</td>\n",
       "      <td>-2.5153</td>\n",
       "      <td>8885737.0</td>\n",
       "      <td>7.517260e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010209</td>\n",
       "      <td>1956.9700</td>\n",
       "      <td>1935.1880</td>\n",
       "      <td>1963.3290</td>\n",
       "      <td>1935.1880</td>\n",
       "      <td>1930.1380</td>\n",
       "      <td>26.8320</td>\n",
       "      <td>1.3902</td>\n",
       "      <td>11237019.0</td>\n",
       "      <td>7.748484e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010212</td>\n",
       "      <td>1961.2210</td>\n",
       "      <td>1958.6550</td>\n",
       "      <td>1961.8100</td>\n",
       "      <td>1935.5870</td>\n",
       "      <td>1956.9700</td>\n",
       "      <td>4.2510</td>\n",
       "      <td>0.2172</td>\n",
       "      <td>7203075.0</td>\n",
       "      <td>6.524706e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010213</td>\n",
       "      <td>1963.5280</td>\n",
       "      <td>1962.5270</td>\n",
       "      <td>1973.9990</td>\n",
       "      <td>1958.9910</td>\n",
       "      <td>1961.2210</td>\n",
       "      <td>2.3070</td>\n",
       "      <td>0.1176</td>\n",
       "      <td>5403514.0</td>\n",
       "      <td>5.135050e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010214</td>\n",
       "      <td>1963.2000</td>\n",
       "      <td>1962.7380</td>\n",
       "      <td>1966.8400</td>\n",
       "      <td>1956.4230</td>\n",
       "      <td>1963.5280</td>\n",
       "      <td>-0.3280</td>\n",
       "      <td>-0.0167</td>\n",
       "      <td>4382153.0</td>\n",
       "      <td>4.177529e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010215</td>\n",
       "      <td>1946.4790</td>\n",
       "      <td>1963.4490</td>\n",
       "      <td>1966.9230</td>\n",
       "      <td>1944.5600</td>\n",
       "      <td>1963.2000</td>\n",
       "      <td>-16.7210</td>\n",
       "      <td>-0.8517</td>\n",
       "      <td>4506756.0</td>\n",
       "      <td>4.262383e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010216</td>\n",
       "      <td>1941.9640</td>\n",
       "      <td>1942.0490</td>\n",
       "      <td>1942.1230</td>\n",
       "      <td>1911.8710</td>\n",
       "      <td>1946.4790</td>\n",
       "      <td>-4.5150</td>\n",
       "      <td>-0.2320</td>\n",
       "      <td>6750209.0</td>\n",
       "      <td>6.112809e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010219</td>\n",
       "      <td>1967.5500</td>\n",
       "      <td>1942.5790</td>\n",
       "      <td>1969.3470</td>\n",
       "      <td>1935.6510</td>\n",
       "      <td>1941.9640</td>\n",
       "      <td>25.5860</td>\n",
       "      <td>1.3175</td>\n",
       "      <td>5846083.0</td>\n",
       "      <td>5.220088e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010220</td>\n",
       "      <td>1953.2180</td>\n",
       "      <td>1969.7770</td>\n",
       "      <td>1980.3860</td>\n",
       "      <td>1947.2550</td>\n",
       "      <td>1967.5500</td>\n",
       "      <td>-14.3320</td>\n",
       "      <td>-0.7284</td>\n",
       "      <td>6435934.0</td>\n",
       "      <td>6.328305e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010221</td>\n",
       "      <td>1909.3260</td>\n",
       "      <td>1948.4170</td>\n",
       "      <td>1948.4170</td>\n",
       "      <td>1901.2050</td>\n",
       "      <td>1953.2180</td>\n",
       "      <td>-43.8920</td>\n",
       "      <td>-2.2472</td>\n",
       "      <td>7617817.0</td>\n",
       "      <td>7.402123e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010222</td>\n",
       "      <td>1907.2600</td>\n",
       "      <td>1901.9930</td>\n",
       "      <td>1922.4570</td>\n",
       "      <td>1893.7830</td>\n",
       "      <td>1909.3260</td>\n",
       "      <td>-2.0660</td>\n",
       "      <td>-0.1082</td>\n",
       "      <td>5054734.0</td>\n",
       "      <td>5.184100e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010223</td>\n",
       "      <td>1936.3460</td>\n",
       "      <td>1906.9140</td>\n",
       "      <td>1936.6340</td>\n",
       "      <td>1906.5510</td>\n",
       "      <td>1907.2600</td>\n",
       "      <td>29.0860</td>\n",
       "      <td>1.5250</td>\n",
       "      <td>6209740.0</td>\n",
       "      <td>5.555909e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010226</td>\n",
       "      <td>1953.3410</td>\n",
       "      <td>1940.7700</td>\n",
       "      <td>1953.7710</td>\n",
       "      <td>1935.5470</td>\n",
       "      <td>1936.3460</td>\n",
       "      <td>16.9950</td>\n",
       "      <td>0.8777</td>\n",
       "      <td>7302811.0</td>\n",
       "      <td>6.871464e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181022</td>\n",
       "      <td>2654.8762</td>\n",
       "      <td>2565.6444</td>\n",
       "      <td>2675.4063</td>\n",
       "      <td>2565.6444</td>\n",
       "      <td>2550.4652</td>\n",
       "      <td>104.4110</td>\n",
       "      <td>4.0938</td>\n",
       "      <td>211888449.0</td>\n",
       "      <td>1.973098e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181023</td>\n",
       "      <td>2594.8255</td>\n",
       "      <td>2652.6476</td>\n",
       "      <td>2659.8123</td>\n",
       "      <td>2583.2605</td>\n",
       "      <td>2654.8762</td>\n",
       "      <td>-60.0507</td>\n",
       "      <td>-2.2619</td>\n",
       "      <td>178343543.0</td>\n",
       "      <td>1.669194e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181024</td>\n",
       "      <td>2603.2951</td>\n",
       "      <td>2579.9715</td>\n",
       "      <td>2640.4017</td>\n",
       "      <td>2577.7725</td>\n",
       "      <td>2594.8255</td>\n",
       "      <td>8.4696</td>\n",
       "      <td>0.3264</td>\n",
       "      <td>160060407.0</td>\n",
       "      <td>1.420822e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181025</td>\n",
       "      <td>2603.7995</td>\n",
       "      <td>2540.9347</td>\n",
       "      <td>2606.1007</td>\n",
       "      <td>2531.5349</td>\n",
       "      <td>2603.2950</td>\n",
       "      <td>0.5045</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>162032979.0</td>\n",
       "      <td>1.376302e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181026</td>\n",
       "      <td>2598.8468</td>\n",
       "      <td>2610.8982</td>\n",
       "      <td>2626.1160</td>\n",
       "      <td>2580.8452</td>\n",
       "      <td>2603.7995</td>\n",
       "      <td>-4.9527</td>\n",
       "      <td>-0.1902</td>\n",
       "      <td>158936119.0</td>\n",
       "      <td>1.334155e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181029</td>\n",
       "      <td>2542.1033</td>\n",
       "      <td>2593.5908</td>\n",
       "      <td>2595.5365</td>\n",
       "      <td>2529.1883</td>\n",
       "      <td>2598.8467</td>\n",
       "      <td>-56.7434</td>\n",
       "      <td>-2.1834</td>\n",
       "      <td>134241923.0</td>\n",
       "      <td>1.225000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181030</td>\n",
       "      <td>2568.0481</td>\n",
       "      <td>2538.5737</td>\n",
       "      <td>2586.9188</td>\n",
       "      <td>2521.7836</td>\n",
       "      <td>2542.1033</td>\n",
       "      <td>25.9448</td>\n",
       "      <td>1.0206</td>\n",
       "      <td>166682309.0</td>\n",
       "      <td>1.523859e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181031</td>\n",
       "      <td>2602.7832</td>\n",
       "      <td>2573.0146</td>\n",
       "      <td>2612.9221</td>\n",
       "      <td>2567.0274</td>\n",
       "      <td>2568.0481</td>\n",
       "      <td>34.7351</td>\n",
       "      <td>1.3526</td>\n",
       "      <td>180550348.0</td>\n",
       "      <td>1.548947e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181101</td>\n",
       "      <td>2606.2372</td>\n",
       "      <td>2617.0325</td>\n",
       "      <td>2636.7992</td>\n",
       "      <td>2603.6514</td>\n",
       "      <td>2602.7832</td>\n",
       "      <td>3.4540</td>\n",
       "      <td>0.1327</td>\n",
       "      <td>200903932.0</td>\n",
       "      <td>1.802538e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181102</td>\n",
       "      <td>2676.4762</td>\n",
       "      <td>2649.2512</td>\n",
       "      <td>2676.4762</td>\n",
       "      <td>2628.8697</td>\n",
       "      <td>2606.2372</td>\n",
       "      <td>70.2390</td>\n",
       "      <td>2.6950</td>\n",
       "      <td>225026921.0</td>\n",
       "      <td>2.078616e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181105</td>\n",
       "      <td>2665.4306</td>\n",
       "      <td>2665.4270</td>\n",
       "      <td>2673.1879</td>\n",
       "      <td>2639.2707</td>\n",
       "      <td>2676.4762</td>\n",
       "      <td>-11.0456</td>\n",
       "      <td>-0.4127</td>\n",
       "      <td>193760100.0</td>\n",
       "      <td>1.685889e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181106</td>\n",
       "      <td>2659.3564</td>\n",
       "      <td>2660.7193</td>\n",
       "      <td>2660.8627</td>\n",
       "      <td>2635.3207</td>\n",
       "      <td>2665.4306</td>\n",
       "      <td>-6.0742</td>\n",
       "      <td>-0.2279</td>\n",
       "      <td>163669479.0</td>\n",
       "      <td>1.388009e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181107</td>\n",
       "      <td>2641.3420</td>\n",
       "      <td>2659.8446</td>\n",
       "      <td>2675.6772</td>\n",
       "      <td>2639.2463</td>\n",
       "      <td>2659.3564</td>\n",
       "      <td>-18.0144</td>\n",
       "      <td>-0.6774</td>\n",
       "      <td>173201986.0</td>\n",
       "      <td>1.472618e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181108</td>\n",
       "      <td>2635.6322</td>\n",
       "      <td>2660.0873</td>\n",
       "      <td>2662.3688</td>\n",
       "      <td>2632.3012</td>\n",
       "      <td>2641.3420</td>\n",
       "      <td>-5.7098</td>\n",
       "      <td>-0.2162</td>\n",
       "      <td>159635487.0</td>\n",
       "      <td>1.286688e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181109</td>\n",
       "      <td>2598.8715</td>\n",
       "      <td>2621.2380</td>\n",
       "      <td>2621.2380</td>\n",
       "      <td>2598.1609</td>\n",
       "      <td>2635.6322</td>\n",
       "      <td>-36.7607</td>\n",
       "      <td>-1.3948</td>\n",
       "      <td>152446307.0</td>\n",
       "      <td>1.212605e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181112</td>\n",
       "      <td>2630.5195</td>\n",
       "      <td>2593.2004</td>\n",
       "      <td>2631.1696</td>\n",
       "      <td>2590.2106</td>\n",
       "      <td>2598.8715</td>\n",
       "      <td>31.6480</td>\n",
       "      <td>1.2178</td>\n",
       "      <td>175031308.0</td>\n",
       "      <td>1.423514e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181113</td>\n",
       "      <td>2654.8795</td>\n",
       "      <td>2600.5004</td>\n",
       "      <td>2666.4853</td>\n",
       "      <td>2597.3477</td>\n",
       "      <td>2630.5195</td>\n",
       "      <td>24.3600</td>\n",
       "      <td>0.9261</td>\n",
       "      <td>249282447.0</td>\n",
       "      <td>1.896072e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181114</td>\n",
       "      <td>2632.2425</td>\n",
       "      <td>2648.3091</td>\n",
       "      <td>2658.3072</td>\n",
       "      <td>2627.9565</td>\n",
       "      <td>2654.8795</td>\n",
       "      <td>-22.6370</td>\n",
       "      <td>-0.8527</td>\n",
       "      <td>238449351.0</td>\n",
       "      <td>1.769704e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181115</td>\n",
       "      <td>2668.1704</td>\n",
       "      <td>2632.1379</td>\n",
       "      <td>2668.1704</td>\n",
       "      <td>2631.8875</td>\n",
       "      <td>2632.2424</td>\n",
       "      <td>35.9280</td>\n",
       "      <td>1.3649</td>\n",
       "      <td>207848721.0</td>\n",
       "      <td>1.718825e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181116</td>\n",
       "      <td>2679.1097</td>\n",
       "      <td>2669.7799</td>\n",
       "      <td>2695.5689</td>\n",
       "      <td>2657.0341</td>\n",
       "      <td>2668.1704</td>\n",
       "      <td>10.9393</td>\n",
       "      <td>0.4100</td>\n",
       "      <td>242198849.0</td>\n",
       "      <td>2.049836e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181119</td>\n",
       "      <td>2703.5116</td>\n",
       "      <td>2681.8988</td>\n",
       "      <td>2703.5116</td>\n",
       "      <td>2674.1781</td>\n",
       "      <td>2679.1097</td>\n",
       "      <td>24.4019</td>\n",
       "      <td>0.9108</td>\n",
       "      <td>231662827.0</td>\n",
       "      <td>1.965627e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181120</td>\n",
       "      <td>2645.8545</td>\n",
       "      <td>2684.2874</td>\n",
       "      <td>2690.8299</td>\n",
       "      <td>2643.3588</td>\n",
       "      <td>2703.5116</td>\n",
       "      <td>-57.6571</td>\n",
       "      <td>-2.1327</td>\n",
       "      <td>215891309.0</td>\n",
       "      <td>1.808704e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181121</td>\n",
       "      <td>2651.5053</td>\n",
       "      <td>2619.8211</td>\n",
       "      <td>2653.7960</td>\n",
       "      <td>2617.7759</td>\n",
       "      <td>2645.8545</td>\n",
       "      <td>5.6508</td>\n",
       "      <td>0.2136</td>\n",
       "      <td>182596447.0</td>\n",
       "      <td>1.493758e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181122</td>\n",
       "      <td>2645.4339</td>\n",
       "      <td>2655.8964</td>\n",
       "      <td>2658.0011</td>\n",
       "      <td>2634.4827</td>\n",
       "      <td>2651.5052</td>\n",
       "      <td>-6.0713</td>\n",
       "      <td>-0.2290</td>\n",
       "      <td>149309063.0</td>\n",
       "      <td>1.255702e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181123</td>\n",
       "      <td>2579.4831</td>\n",
       "      <td>2640.6674</td>\n",
       "      <td>2642.0356</td>\n",
       "      <td>2577.3511</td>\n",
       "      <td>2645.4339</td>\n",
       "      <td>-65.9508</td>\n",
       "      <td>-2.4930</td>\n",
       "      <td>191474549.0</td>\n",
       "      <td>1.499756e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181126</td>\n",
       "      <td>2575.8101</td>\n",
       "      <td>2580.8424</td>\n",
       "      <td>2594.9966</td>\n",
       "      <td>2568.0352</td>\n",
       "      <td>2579.4831</td>\n",
       "      <td>-3.6730</td>\n",
       "      <td>-0.1424</td>\n",
       "      <td>134314540.0</td>\n",
       "      <td>1.081581e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181127</td>\n",
       "      <td>2574.6792</td>\n",
       "      <td>2585.8261</td>\n",
       "      <td>2592.5353</td>\n",
       "      <td>2566.1663</td>\n",
       "      <td>2575.8101</td>\n",
       "      <td>-1.1309</td>\n",
       "      <td>-0.0439</td>\n",
       "      <td>123658436.0</td>\n",
       "      <td>1.022266e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181128</td>\n",
       "      <td>2601.7365</td>\n",
       "      <td>2575.4541</td>\n",
       "      <td>2601.9626</td>\n",
       "      <td>2561.5618</td>\n",
       "      <td>2574.6792</td>\n",
       "      <td>27.0573</td>\n",
       "      <td>1.0509</td>\n",
       "      <td>145963473.0</td>\n",
       "      <td>1.195785e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181129</td>\n",
       "      <td>2567.4434</td>\n",
       "      <td>2613.7805</td>\n",
       "      <td>2617.5479</td>\n",
       "      <td>2567.4434</td>\n",
       "      <td>2601.7365</td>\n",
       "      <td>-34.2931</td>\n",
       "      <td>-1.3181</td>\n",
       "      <td>157168738.0</td>\n",
       "      <td>1.294724e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181130</td>\n",
       "      <td>2588.1875</td>\n",
       "      <td>2564.5644</td>\n",
       "      <td>2590.2102</td>\n",
       "      <td>2555.3223</td>\n",
       "      <td>2567.4434</td>\n",
       "      <td>20.7441</td>\n",
       "      <td>0.8080</td>\n",
       "      <td>139329021.0</td>\n",
       "      <td>1.122198e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4343 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ts_code trade_date      ...               vol        amount\n",
       "1443  000001.SH   20010102      ...        18650746.0  1.340509e+07\n",
       "1442  000001.SH   20010103      ...        17157673.0  1.751683e+07\n",
       "1441  000001.SH   20010104      ...        13423541.0  1.429525e+07\n",
       "1440  000001.SH   20010105      ...        13288175.0  1.484424e+07\n",
       "1439  000001.SH   20010108      ...        14787085.0  1.588766e+07\n",
       "1438  000001.SH   20010109      ...        11649338.0  1.158177e+07\n",
       "1437  000001.SH   20010110      ...        12916883.0  1.302625e+07\n",
       "1436  000001.SH   20010111      ...        14072209.0  1.461571e+07\n",
       "1435  000001.SH   20010112      ...        10660882.0  1.101729e+07\n",
       "1434  000001.SH   20010115      ...        14998588.0  1.341256e+07\n",
       "1433  000001.SH   20010116      ...        11583722.0  1.040591e+07\n",
       "1432  000001.SH   20010117      ...         7324134.0  6.609476e+06\n",
       "1431  000001.SH   20010118      ...        10341571.0  8.381613e+06\n",
       "1430  000001.SH   20010119      ...        11831264.0  8.601583e+06\n",
       "1429  000001.SH   20010205      ...         8974243.0  7.400294e+06\n",
       "1428  000001.SH   20010206      ...         9528427.0  7.974362e+06\n",
       "1427  000001.SH   20010207      ...         5882229.0  5.171536e+06\n",
       "1426  000001.SH   20010208      ...         8885737.0  7.517260e+06\n",
       "1425  000001.SH   20010209      ...        11237019.0  7.748484e+06\n",
       "1424  000001.SH   20010212      ...         7203075.0  6.524706e+06\n",
       "1423  000001.SH   20010213      ...         5403514.0  5.135050e+06\n",
       "1422  000001.SH   20010214      ...         4382153.0  4.177529e+06\n",
       "1421  000001.SH   20010215      ...         4506756.0  4.262383e+06\n",
       "1420  000001.SH   20010216      ...         6750209.0  6.112809e+06\n",
       "1419  000001.SH   20010219      ...         5846083.0  5.220088e+06\n",
       "1418  000001.SH   20010220      ...         6435934.0  6.328305e+06\n",
       "1417  000001.SH   20010221      ...         7617817.0  7.402123e+06\n",
       "1416  000001.SH   20010222      ...         5054734.0  5.184100e+06\n",
       "1415  000001.SH   20010223      ...         6209740.0  5.555909e+06\n",
       "1414  000001.SH   20010226      ...         7302811.0  6.871464e+06\n",
       "...         ...        ...      ...               ...           ...\n",
       "29    000001.SH   20181022      ...       211888449.0  1.973098e+08\n",
       "28    000001.SH   20181023      ...       178343543.0  1.669194e+08\n",
       "27    000001.SH   20181024      ...       160060407.0  1.420822e+08\n",
       "26    000001.SH   20181025      ...       162032979.0  1.376302e+08\n",
       "25    000001.SH   20181026      ...       158936119.0  1.334155e+08\n",
       "24    000001.SH   20181029      ...       134241923.0  1.225000e+08\n",
       "23    000001.SH   20181030      ...       166682309.0  1.523859e+08\n",
       "22    000001.SH   20181031      ...       180550348.0  1.548947e+08\n",
       "21    000001.SH   20181101      ...       200903932.0  1.802538e+08\n",
       "20    000001.SH   20181102      ...       225026921.0  2.078616e+08\n",
       "19    000001.SH   20181105      ...       193760100.0  1.685889e+08\n",
       "18    000001.SH   20181106      ...       163669479.0  1.388009e+08\n",
       "17    000001.SH   20181107      ...       173201986.0  1.472618e+08\n",
       "16    000001.SH   20181108      ...       159635487.0  1.286688e+08\n",
       "15    000001.SH   20181109      ...       152446307.0  1.212605e+08\n",
       "14    000001.SH   20181112      ...       175031308.0  1.423514e+08\n",
       "13    000001.SH   20181113      ...       249282447.0  1.896072e+08\n",
       "12    000001.SH   20181114      ...       238449351.0  1.769704e+08\n",
       "11    000001.SH   20181115      ...       207848721.0  1.718825e+08\n",
       "10    000001.SH   20181116      ...       242198849.0  2.049836e+08\n",
       "9     000001.SH   20181119      ...       231662827.0  1.965627e+08\n",
       "8     000001.SH   20181120      ...       215891309.0  1.808704e+08\n",
       "7     000001.SH   20181121      ...       182596447.0  1.493758e+08\n",
       "6     000001.SH   20181122      ...       149309063.0  1.255702e+08\n",
       "5     000001.SH   20181123      ...       191474549.0  1.499756e+08\n",
       "4     000001.SH   20181126      ...       134314540.0  1.081581e+08\n",
       "3     000001.SH   20181127      ...       123658436.0  1.022266e+08\n",
       "2     000001.SH   20181128      ...       145963473.0  1.195785e+08\n",
       "1     000001.SH   20181129      ...       157168738.0  1.294724e+08\n",
       "0     000001.SH   20181130      ...       139329021.0  1.122198e+08\n",
       "\n",
       "[4343 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_code</th>\n",
       "      <th>trade_date</th>\n",
       "      <th>close_1</th>\n",
       "      <th>open_1</th>\n",
       "      <th>high_1</th>\n",
       "      <th>low_1</th>\n",
       "      <th>pre_close_1</th>\n",
       "      <th>change_1</th>\n",
       "      <th>pct_chg_1</th>\n",
       "      <th>vol_1</th>\n",
       "      <th>amount_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010102</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010103</td>\n",
       "      <td>1.009713</td>\n",
       "      <td>1.015011</td>\n",
       "      <td>1.009680</td>\n",
       "      <td>1.014755</td>\n",
       "      <td>1.014465</td>\n",
       "      <td>0.681182</td>\n",
       "      <td>0.671483</td>\n",
       "      <td>0.919946</td>\n",
       "      <td>1.306729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010104</td>\n",
       "      <td>1.006625</td>\n",
       "      <td>1.023306</td>\n",
       "      <td>1.011353</td>\n",
       "      <td>1.018457</td>\n",
       "      <td>1.024318</td>\n",
       "      <td>-0.216524</td>\n",
       "      <td>-0.211407</td>\n",
       "      <td>0.719732</td>\n",
       "      <td>1.066404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010105</td>\n",
       "      <td>1.010379</td>\n",
       "      <td>1.019541</td>\n",
       "      <td>1.010384</td>\n",
       "      <td>1.018880</td>\n",
       "      <td>1.021186</td>\n",
       "      <td>0.263237</td>\n",
       "      <td>0.257795</td>\n",
       "      <td>0.712474</td>\n",
       "      <td>1.107358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010108</td>\n",
       "      <td>0.999332</td>\n",
       "      <td>1.024807</td>\n",
       "      <td>1.013521</td>\n",
       "      <td>1.008297</td>\n",
       "      <td>1.024993</td>\n",
       "      <td>-0.774740</td>\n",
       "      <td>-0.755824</td>\n",
       "      <td>0.792841</td>\n",
       "      <td>1.185196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010109</td>\n",
       "      <td>0.998890</td>\n",
       "      <td>1.010711</td>\n",
       "      <td>1.003595</td>\n",
       "      <td>1.003836</td>\n",
       "      <td>1.013787</td>\n",
       "      <td>-0.030975</td>\n",
       "      <td>-0.030557</td>\n",
       "      <td>0.624604</td>\n",
       "      <td>0.863982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010110</td>\n",
       "      <td>1.010528</td>\n",
       "      <td>1.012040</td>\n",
       "      <td>1.010521</td>\n",
       "      <td>1.013101</td>\n",
       "      <td>1.013339</td>\n",
       "      <td>0.816218</td>\n",
       "      <td>0.805461</td>\n",
       "      <td>0.692567</td>\n",
       "      <td>0.971739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010111</td>\n",
       "      <td>1.007451</td>\n",
       "      <td>1.025292</td>\n",
       "      <td>1.013230</td>\n",
       "      <td>1.020313</td>\n",
       "      <td>1.025145</td>\n",
       "      <td>-0.215858</td>\n",
       "      <td>-0.210577</td>\n",
       "      <td>0.754512</td>\n",
       "      <td>1.090310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010112</td>\n",
       "      <td>1.000608</td>\n",
       "      <td>1.019147</td>\n",
       "      <td>1.006745</td>\n",
       "      <td>1.012237</td>\n",
       "      <td>1.022023</td>\n",
       "      <td>-0.479928</td>\n",
       "      <td>-0.469547</td>\n",
       "      <td>0.571606</td>\n",
       "      <td>0.821873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010115</td>\n",
       "      <td>0.966234</td>\n",
       "      <td>1.009636</td>\n",
       "      <td>0.996936</td>\n",
       "      <td>0.978508</td>\n",
       "      <td>1.015081</td>\n",
       "      <td>-2.410776</td>\n",
       "      <td>-2.374905</td>\n",
       "      <td>0.804182</td>\n",
       "      <td>1.000557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010116</td>\n",
       "      <td>0.972625</td>\n",
       "      <td>0.971702</td>\n",
       "      <td>0.972621</td>\n",
       "      <td>0.967624</td>\n",
       "      <td>0.980210</td>\n",
       "      <td>0.448253</td>\n",
       "      <td>0.457311</td>\n",
       "      <td>0.621086</td>\n",
       "      <td>0.776265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010117</td>\n",
       "      <td>0.967252</td>\n",
       "      <td>0.985220</td>\n",
       "      <td>0.973928</td>\n",
       "      <td>0.979215</td>\n",
       "      <td>0.986694</td>\n",
       "      <td>-0.376867</td>\n",
       "      <td>-0.381956</td>\n",
       "      <td>0.392699</td>\n",
       "      <td>0.493057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010118</td>\n",
       "      <td>0.971305</td>\n",
       "      <td>0.979110</td>\n",
       "      <td>0.974242</td>\n",
       "      <td>0.980092</td>\n",
       "      <td>0.981243</td>\n",
       "      <td>0.284276</td>\n",
       "      <td>0.289734</td>\n",
       "      <td>0.554486</td>\n",
       "      <td>0.625256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010119</td>\n",
       "      <td>0.982000</td>\n",
       "      <td>0.985318</td>\n",
       "      <td>0.983047</td>\n",
       "      <td>0.986341</td>\n",
       "      <td>0.985355</td>\n",
       "      <td>0.750067</td>\n",
       "      <td>0.761217</td>\n",
       "      <td>0.634359</td>\n",
       "      <td>0.641665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010205</td>\n",
       "      <td>0.954629</td>\n",
       "      <td>0.996524</td>\n",
       "      <td>0.985267</td>\n",
       "      <td>0.966649</td>\n",
       "      <td>0.996204</td>\n",
       "      <td>-1.919645</td>\n",
       "      <td>-1.926927</td>\n",
       "      <td>0.481173</td>\n",
       "      <td>0.552051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010206</td>\n",
       "      <td>0.948583</td>\n",
       "      <td>0.962759</td>\n",
       "      <td>0.950648</td>\n",
       "      <td>0.942790</td>\n",
       "      <td>0.968437</td>\n",
       "      <td>-0.424013</td>\n",
       "      <td>-0.437815</td>\n",
       "      <td>0.510887</td>\n",
       "      <td>0.594875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010207</td>\n",
       "      <td>0.941273</td>\n",
       "      <td>0.960775</td>\n",
       "      <td>0.949947</td>\n",
       "      <td>0.953596</td>\n",
       "      <td>0.962304</td>\n",
       "      <td>-0.512670</td>\n",
       "      <td>-0.532734</td>\n",
       "      <td>0.315388</td>\n",
       "      <td>0.385789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010208</td>\n",
       "      <td>0.917598</td>\n",
       "      <td>0.952088</td>\n",
       "      <td>0.944551</td>\n",
       "      <td>0.928438</td>\n",
       "      <td>0.954888</td>\n",
       "      <td>-1.660476</td>\n",
       "      <td>-1.738887</td>\n",
       "      <td>0.476428</td>\n",
       "      <td>0.560776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010209</td>\n",
       "      <td>0.930354</td>\n",
       "      <td>0.931689</td>\n",
       "      <td>0.933346</td>\n",
       "      <td>0.932674</td>\n",
       "      <td>0.930870</td>\n",
       "      <td>0.894639</td>\n",
       "      <td>0.961078</td>\n",
       "      <td>0.602497</td>\n",
       "      <td>0.578025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010212</td>\n",
       "      <td>0.932375</td>\n",
       "      <td>0.942987</td>\n",
       "      <td>0.932624</td>\n",
       "      <td>0.932866</td>\n",
       "      <td>0.943811</td>\n",
       "      <td>0.141738</td>\n",
       "      <td>0.150156</td>\n",
       "      <td>0.386208</td>\n",
       "      <td>0.486733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010213</td>\n",
       "      <td>0.933471</td>\n",
       "      <td>0.944851</td>\n",
       "      <td>0.938419</td>\n",
       "      <td>0.944146</td>\n",
       "      <td>0.945861</td>\n",
       "      <td>0.076921</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>0.289721</td>\n",
       "      <td>0.383067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010214</td>\n",
       "      <td>0.933315</td>\n",
       "      <td>0.944952</td>\n",
       "      <td>0.935015</td>\n",
       "      <td>0.942908</td>\n",
       "      <td>0.946974</td>\n",
       "      <td>-0.010936</td>\n",
       "      <td>-0.011545</td>\n",
       "      <td>0.234959</td>\n",
       "      <td>0.311637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010215</td>\n",
       "      <td>0.925366</td>\n",
       "      <td>0.945295</td>\n",
       "      <td>0.935055</td>\n",
       "      <td>0.937191</td>\n",
       "      <td>0.946815</td>\n",
       "      <td>-0.557515</td>\n",
       "      <td>-0.588801</td>\n",
       "      <td>0.241639</td>\n",
       "      <td>0.317967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010216</td>\n",
       "      <td>0.923220</td>\n",
       "      <td>0.934992</td>\n",
       "      <td>0.923265</td>\n",
       "      <td>0.921436</td>\n",
       "      <td>0.938751</td>\n",
       "      <td>-0.150540</td>\n",
       "      <td>-0.160387</td>\n",
       "      <td>0.361927</td>\n",
       "      <td>0.456006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010219</td>\n",
       "      <td>0.935383</td>\n",
       "      <td>0.935247</td>\n",
       "      <td>0.936207</td>\n",
       "      <td>0.932897</td>\n",
       "      <td>0.936574</td>\n",
       "      <td>0.853094</td>\n",
       "      <td>0.910819</td>\n",
       "      <td>0.313450</td>\n",
       "      <td>0.389411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010220</td>\n",
       "      <td>0.928570</td>\n",
       "      <td>0.948341</td>\n",
       "      <td>0.941455</td>\n",
       "      <td>0.938490</td>\n",
       "      <td>0.948913</td>\n",
       "      <td>-0.477861</td>\n",
       "      <td>-0.503560</td>\n",
       "      <td>0.345076</td>\n",
       "      <td>0.472082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010221</td>\n",
       "      <td>0.907703</td>\n",
       "      <td>0.938058</td>\n",
       "      <td>0.926257</td>\n",
       "      <td>0.916295</td>\n",
       "      <td>0.942001</td>\n",
       "      <td>-1.463457</td>\n",
       "      <td>-1.553543</td>\n",
       "      <td>0.408446</td>\n",
       "      <td>0.552187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010222</td>\n",
       "      <td>0.906721</td>\n",
       "      <td>0.915707</td>\n",
       "      <td>0.913916</td>\n",
       "      <td>0.912718</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>-0.068885</td>\n",
       "      <td>-0.074801</td>\n",
       "      <td>0.271020</td>\n",
       "      <td>0.386726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010223</td>\n",
       "      <td>0.920549</td>\n",
       "      <td>0.918076</td>\n",
       "      <td>0.920656</td>\n",
       "      <td>0.918872</td>\n",
       "      <td>0.919837</td>\n",
       "      <td>0.969792</td>\n",
       "      <td>1.054269</td>\n",
       "      <td>0.332949</td>\n",
       "      <td>0.414462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20010226</td>\n",
       "      <td>0.928628</td>\n",
       "      <td>0.934376</td>\n",
       "      <td>0.928802</td>\n",
       "      <td>0.932847</td>\n",
       "      <td>0.933864</td>\n",
       "      <td>0.566651</td>\n",
       "      <td>0.606775</td>\n",
       "      <td>0.391556</td>\n",
       "      <td>0.512601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181022</td>\n",
       "      <td>1.262142</td>\n",
       "      <td>1.235219</td>\n",
       "      <td>1.271860</td>\n",
       "      <td>1.236525</td>\n",
       "      <td>1.230043</td>\n",
       "      <td>3.481295</td>\n",
       "      <td>2.830142</td>\n",
       "      <td>11.360857</td>\n",
       "      <td>14.719017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181023</td>\n",
       "      <td>1.233593</td>\n",
       "      <td>1.277107</td>\n",
       "      <td>1.264447</td>\n",
       "      <td>1.245016</td>\n",
       "      <td>1.280398</td>\n",
       "      <td>-2.002224</td>\n",
       "      <td>-1.563705</td>\n",
       "      <td>9.562274</td>\n",
       "      <td>12.451938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181024</td>\n",
       "      <td>1.237620</td>\n",
       "      <td>1.242117</td>\n",
       "      <td>1.255219</td>\n",
       "      <td>1.242371</td>\n",
       "      <td>1.251437</td>\n",
       "      <td>0.282395</td>\n",
       "      <td>0.225648</td>\n",
       "      <td>8.581984</td>\n",
       "      <td>10.599117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181025</td>\n",
       "      <td>1.237860</td>\n",
       "      <td>1.223323</td>\n",
       "      <td>1.238913</td>\n",
       "      <td>1.220086</td>\n",
       "      <td>1.255522</td>\n",
       "      <td>0.016821</td>\n",
       "      <td>0.013412</td>\n",
       "      <td>8.687748</td>\n",
       "      <td>10.267005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181026</td>\n",
       "      <td>1.235505</td>\n",
       "      <td>1.257007</td>\n",
       "      <td>1.248428</td>\n",
       "      <td>1.243852</td>\n",
       "      <td>1.255765</td>\n",
       "      <td>-0.165134</td>\n",
       "      <td>-0.131490</td>\n",
       "      <td>8.521703</td>\n",
       "      <td>9.952596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181029</td>\n",
       "      <td>1.208529</td>\n",
       "      <td>1.248674</td>\n",
       "      <td>1.233891</td>\n",
       "      <td>1.218955</td>\n",
       "      <td>1.253376</td>\n",
       "      <td>-1.891951</td>\n",
       "      <td>-1.509437</td>\n",
       "      <td>7.197670</td>\n",
       "      <td>9.138320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181030</td>\n",
       "      <td>1.220863</td>\n",
       "      <td>1.222186</td>\n",
       "      <td>1.229794</td>\n",
       "      <td>1.215387</td>\n",
       "      <td>1.226010</td>\n",
       "      <td>0.865057</td>\n",
       "      <td>0.705565</td>\n",
       "      <td>8.937032</td>\n",
       "      <td>11.367763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181031</td>\n",
       "      <td>1.237377</td>\n",
       "      <td>1.238768</td>\n",
       "      <td>1.242156</td>\n",
       "      <td>1.237192</td>\n",
       "      <td>1.238523</td>\n",
       "      <td>1.158146</td>\n",
       "      <td>0.935085</td>\n",
       "      <td>9.680597</td>\n",
       "      <td>11.554915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181101</td>\n",
       "      <td>1.239019</td>\n",
       "      <td>1.259960</td>\n",
       "      <td>1.253507</td>\n",
       "      <td>1.254843</td>\n",
       "      <td>1.255275</td>\n",
       "      <td>0.115164</td>\n",
       "      <td>0.091739</td>\n",
       "      <td>10.771898</td>\n",
       "      <td>13.446664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181102</td>\n",
       "      <td>1.272411</td>\n",
       "      <td>1.275471</td>\n",
       "      <td>1.272369</td>\n",
       "      <td>1.266997</td>\n",
       "      <td>1.256940</td>\n",
       "      <td>2.341925</td>\n",
       "      <td>1.863118</td>\n",
       "      <td>12.065304</td>\n",
       "      <td>15.506162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181105</td>\n",
       "      <td>1.267159</td>\n",
       "      <td>1.283259</td>\n",
       "      <td>1.270806</td>\n",
       "      <td>1.272010</td>\n",
       "      <td>1.290815</td>\n",
       "      <td>-0.368285</td>\n",
       "      <td>-0.285309</td>\n",
       "      <td>10.388866</td>\n",
       "      <td>12.576481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181106</td>\n",
       "      <td>1.264272</td>\n",
       "      <td>1.280993</td>\n",
       "      <td>1.264946</td>\n",
       "      <td>1.270106</td>\n",
       "      <td>1.285488</td>\n",
       "      <td>-0.202527</td>\n",
       "      <td>-0.157553</td>\n",
       "      <td>8.775492</td>\n",
       "      <td>10.354342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181107</td>\n",
       "      <td>1.255708</td>\n",
       "      <td>1.280572</td>\n",
       "      <td>1.271989</td>\n",
       "      <td>1.271998</td>\n",
       "      <td>1.282559</td>\n",
       "      <td>-0.600640</td>\n",
       "      <td>-0.468303</td>\n",
       "      <td>9.286598</td>\n",
       "      <td>10.985510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181108</td>\n",
       "      <td>1.252993</td>\n",
       "      <td>1.280688</td>\n",
       "      <td>1.265662</td>\n",
       "      <td>1.268651</td>\n",
       "      <td>1.273871</td>\n",
       "      <td>-0.190377</td>\n",
       "      <td>-0.149464</td>\n",
       "      <td>8.559201</td>\n",
       "      <td>9.598501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181109</td>\n",
       "      <td>1.235517</td>\n",
       "      <td>1.261985</td>\n",
       "      <td>1.246109</td>\n",
       "      <td>1.252197</td>\n",
       "      <td>1.271117</td>\n",
       "      <td>-1.225684</td>\n",
       "      <td>-0.964259</td>\n",
       "      <td>8.173738</td>\n",
       "      <td>9.045851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181112</td>\n",
       "      <td>1.250563</td>\n",
       "      <td>1.248486</td>\n",
       "      <td>1.250831</td>\n",
       "      <td>1.248365</td>\n",
       "      <td>1.253388</td>\n",
       "      <td>1.055215</td>\n",
       "      <td>0.841894</td>\n",
       "      <td>9.384681</td>\n",
       "      <td>10.619203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181113</td>\n",
       "      <td>1.262143</td>\n",
       "      <td>1.252001</td>\n",
       "      <td>1.267619</td>\n",
       "      <td>1.251805</td>\n",
       "      <td>1.268651</td>\n",
       "      <td>0.812217</td>\n",
       "      <td>0.640235</td>\n",
       "      <td>13.365816</td>\n",
       "      <td>14.144415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181114</td>\n",
       "      <td>1.251382</td>\n",
       "      <td>1.275018</td>\n",
       "      <td>1.263731</td>\n",
       "      <td>1.266557</td>\n",
       "      <td>1.280400</td>\n",
       "      <td>-0.754768</td>\n",
       "      <td>-0.589492</td>\n",
       "      <td>12.784977</td>\n",
       "      <td>13.201729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181115</td>\n",
       "      <td>1.268462</td>\n",
       "      <td>1.267232</td>\n",
       "      <td>1.268420</td>\n",
       "      <td>1.268452</td>\n",
       "      <td>1.269482</td>\n",
       "      <td>1.197919</td>\n",
       "      <td>0.943588</td>\n",
       "      <td>11.144258</td>\n",
       "      <td>12.822177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181116</td>\n",
       "      <td>1.273663</td>\n",
       "      <td>1.285355</td>\n",
       "      <td>1.281445</td>\n",
       "      <td>1.280571</td>\n",
       "      <td>1.286810</td>\n",
       "      <td>0.364741</td>\n",
       "      <td>0.283443</td>\n",
       "      <td>12.986014</td>\n",
       "      <td>15.291470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181119</td>\n",
       "      <td>1.285263</td>\n",
       "      <td>1.291190</td>\n",
       "      <td>1.285221</td>\n",
       "      <td>1.288834</td>\n",
       "      <td>1.292086</td>\n",
       "      <td>0.813614</td>\n",
       "      <td>0.629658</td>\n",
       "      <td>12.421102</td>\n",
       "      <td>14.663284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181120</td>\n",
       "      <td>1.257853</td>\n",
       "      <td>1.292340</td>\n",
       "      <td>1.279192</td>\n",
       "      <td>1.273980</td>\n",
       "      <td>1.303854</td>\n",
       "      <td>-1.922416</td>\n",
       "      <td>-1.474386</td>\n",
       "      <td>11.575478</td>\n",
       "      <td>13.492662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181121</td>\n",
       "      <td>1.260539</td>\n",
       "      <td>1.261302</td>\n",
       "      <td>1.261587</td>\n",
       "      <td>1.261650</td>\n",
       "      <td>1.276047</td>\n",
       "      <td>0.188410</td>\n",
       "      <td>0.147667</td>\n",
       "      <td>9.790303</td>\n",
       "      <td>11.143210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181122</td>\n",
       "      <td>1.257653</td>\n",
       "      <td>1.278671</td>\n",
       "      <td>1.263586</td>\n",
       "      <td>1.269702</td>\n",
       "      <td>1.278772</td>\n",
       "      <td>-0.202431</td>\n",
       "      <td>-0.158313</td>\n",
       "      <td>8.005528</td>\n",
       "      <td>9.367347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181123</td>\n",
       "      <td>1.226300</td>\n",
       "      <td>1.271339</td>\n",
       "      <td>1.255996</td>\n",
       "      <td>1.242168</td>\n",
       "      <td>1.275844</td>\n",
       "      <td>-2.198946</td>\n",
       "      <td>-1.723470</td>\n",
       "      <td>10.266321</td>\n",
       "      <td>11.187959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181126</td>\n",
       "      <td>1.224553</td>\n",
       "      <td>1.242536</td>\n",
       "      <td>1.233634</td>\n",
       "      <td>1.237678</td>\n",
       "      <td>1.244037</td>\n",
       "      <td>-0.122466</td>\n",
       "      <td>-0.098445</td>\n",
       "      <td>7.201564</td>\n",
       "      <td>8.068432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181127</td>\n",
       "      <td>1.224016</td>\n",
       "      <td>1.244936</td>\n",
       "      <td>1.232464</td>\n",
       "      <td>1.236777</td>\n",
       "      <td>1.242266</td>\n",
       "      <td>-0.037707</td>\n",
       "      <td>-0.030349</td>\n",
       "      <td>6.630214</td>\n",
       "      <td>7.625948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181128</td>\n",
       "      <td>1.236879</td>\n",
       "      <td>1.239942</td>\n",
       "      <td>1.236946</td>\n",
       "      <td>1.234558</td>\n",
       "      <td>1.241721</td>\n",
       "      <td>0.902151</td>\n",
       "      <td>0.726512</td>\n",
       "      <td>7.826147</td>\n",
       "      <td>8.920376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181129</td>\n",
       "      <td>1.220576</td>\n",
       "      <td>1.258394</td>\n",
       "      <td>1.244355</td>\n",
       "      <td>1.237392</td>\n",
       "      <td>1.254770</td>\n",
       "      <td>-1.143408</td>\n",
       "      <td>-0.911234</td>\n",
       "      <td>8.426941</td>\n",
       "      <td>9.658451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181130</td>\n",
       "      <td>1.230438</td>\n",
       "      <td>1.234699</td>\n",
       "      <td>1.231359</td>\n",
       "      <td>1.231551</td>\n",
       "      <td>1.238231</td>\n",
       "      <td>0.691654</td>\n",
       "      <td>0.558590</td>\n",
       "      <td>7.470426</td>\n",
       "      <td>8.371431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4343 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ts_code trade_date   close_1    ...      pct_chg_1      vol_1   amount_1\n",
       "1443  000001.SH   20010102  1.000000    ...       1.000000   1.000000   1.000000\n",
       "1442  000001.SH   20010103  1.009713    ...       0.671483   0.919946   1.306729\n",
       "1441  000001.SH   20010104  1.006625    ...      -0.211407   0.719732   1.066404\n",
       "1440  000001.SH   20010105  1.010379    ...       0.257795   0.712474   1.107358\n",
       "1439  000001.SH   20010108  0.999332    ...      -0.755824   0.792841   1.185196\n",
       "1438  000001.SH   20010109  0.998890    ...      -0.030557   0.624604   0.863982\n",
       "1437  000001.SH   20010110  1.010528    ...       0.805461   0.692567   0.971739\n",
       "1436  000001.SH   20010111  1.007451    ...      -0.210577   0.754512   1.090310\n",
       "1435  000001.SH   20010112  1.000608    ...      -0.469547   0.571606   0.821873\n",
       "1434  000001.SH   20010115  0.966234    ...      -2.374905   0.804182   1.000557\n",
       "1433  000001.SH   20010116  0.972625    ...       0.457311   0.621086   0.776265\n",
       "1432  000001.SH   20010117  0.967252    ...      -0.381956   0.392699   0.493057\n",
       "1431  000001.SH   20010118  0.971305    ...       0.289734   0.554486   0.625256\n",
       "1430  000001.SH   20010119  0.982000    ...       0.761217   0.634359   0.641665\n",
       "1429  000001.SH   20010205  0.954629    ...      -1.926927   0.481173   0.552051\n",
       "1428  000001.SH   20010206  0.948583    ...      -0.437815   0.510887   0.594875\n",
       "1427  000001.SH   20010207  0.941273    ...      -0.532734   0.315388   0.385789\n",
       "1426  000001.SH   20010208  0.917598    ...      -1.738887   0.476428   0.560776\n",
       "1425  000001.SH   20010209  0.930354    ...       0.961078   0.602497   0.578025\n",
       "1424  000001.SH   20010212  0.932375    ...       0.150156   0.386208   0.486733\n",
       "1423  000001.SH   20010213  0.933471    ...       0.081300   0.289721   0.383067\n",
       "1422  000001.SH   20010214  0.933315    ...      -0.011545   0.234959   0.311637\n",
       "1421  000001.SH   20010215  0.925366    ...      -0.588801   0.241639   0.317967\n",
       "1420  000001.SH   20010216  0.923220    ...      -0.160387   0.361927   0.456006\n",
       "1419  000001.SH   20010219  0.935383    ...       0.910819   0.313450   0.389411\n",
       "1418  000001.SH   20010220  0.928570    ...      -0.503560   0.345076   0.472082\n",
       "1417  000001.SH   20010221  0.907703    ...      -1.553543   0.408446   0.552187\n",
       "1416  000001.SH   20010222  0.906721    ...      -0.074801   0.271020   0.386726\n",
       "1415  000001.SH   20010223  0.920549    ...       1.054269   0.332949   0.414462\n",
       "1414  000001.SH   20010226  0.928628    ...       0.606775   0.391556   0.512601\n",
       "...         ...        ...       ...    ...            ...        ...        ...\n",
       "29    000001.SH   20181022  1.262142    ...       2.830142  11.360857  14.719017\n",
       "28    000001.SH   20181023  1.233593    ...      -1.563705   9.562274  12.451938\n",
       "27    000001.SH   20181024  1.237620    ...       0.225648   8.581984  10.599117\n",
       "26    000001.SH   20181025  1.237860    ...       0.013412   8.687748  10.267005\n",
       "25    000001.SH   20181026  1.235505    ...      -0.131490   8.521703   9.952596\n",
       "24    000001.SH   20181029  1.208529    ...      -1.509437   7.197670   9.138320\n",
       "23    000001.SH   20181030  1.220863    ...       0.705565   8.937032  11.367763\n",
       "22    000001.SH   20181031  1.237377    ...       0.935085   9.680597  11.554915\n",
       "21    000001.SH   20181101  1.239019    ...       0.091739  10.771898  13.446664\n",
       "20    000001.SH   20181102  1.272411    ...       1.863118  12.065304  15.506162\n",
       "19    000001.SH   20181105  1.267159    ...      -0.285309  10.388866  12.576481\n",
       "18    000001.SH   20181106  1.264272    ...      -0.157553   8.775492  10.354342\n",
       "17    000001.SH   20181107  1.255708    ...      -0.468303   9.286598  10.985510\n",
       "16    000001.SH   20181108  1.252993    ...      -0.149464   8.559201   9.598501\n",
       "15    000001.SH   20181109  1.235517    ...      -0.964259   8.173738   9.045851\n",
       "14    000001.SH   20181112  1.250563    ...       0.841894   9.384681  10.619203\n",
       "13    000001.SH   20181113  1.262143    ...       0.640235  13.365816  14.144415\n",
       "12    000001.SH   20181114  1.251382    ...      -0.589492  12.784977  13.201729\n",
       "11    000001.SH   20181115  1.268462    ...       0.943588  11.144258  12.822177\n",
       "10    000001.SH   20181116  1.273663    ...       0.283443  12.986014  15.291470\n",
       "9     000001.SH   20181119  1.285263    ...       0.629658  12.421102  14.663284\n",
       "8     000001.SH   20181120  1.257853    ...      -1.474386  11.575478  13.492662\n",
       "7     000001.SH   20181121  1.260539    ...       0.147667   9.790303  11.143210\n",
       "6     000001.SH   20181122  1.257653    ...      -0.158313   8.005528   9.367347\n",
       "5     000001.SH   20181123  1.226300    ...      -1.723470  10.266321  11.187959\n",
       "4     000001.SH   20181126  1.224553    ...      -0.098445   7.201564   8.068432\n",
       "3     000001.SH   20181127  1.224016    ...      -0.030349   6.630214   7.625948\n",
       "2     000001.SH   20181128  1.236879    ...       0.726512   7.826147   8.920376\n",
       "1     000001.SH   20181129  1.220576    ...      -0.911234   8.426941   9.658451\n",
       "0     000001.SH   20181130  1.230438    ...       0.558590   7.470426   8.371431\n",
       "\n",
       "[4343 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = t.normlize_field(df, ['close', 'open', 'high', 'low', 'pre_close','change', 'pct_chg', 'vol', 'amount'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = t.build_flag(df1,180,15,['close_1', 'open_1', 'high_1', 'low_1',\\\n",
    "       'pre_close_1', 'change_1', 'pct_chg_1', 'vol_1', 'amount_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.        ,  1.        ,  1.        , ...,  1.        ,\n",
       "          1.        ,  1.        ],\n",
       "        [ 1.00971253,  1.01501052,  1.00967988, ...,  0.67148289,\n",
       "          0.91994567,  1.30672949],\n",
       "        [ 1.00662525,  1.02330632,  1.01135278, ..., -0.21140684,\n",
       "          0.71973212,  1.06640391],\n",
       "        ...,\n",
       "        [ 0.83905777,  0.85005171,  0.84578125, ..., -0.11268579,\n",
       "          0.17010049,  0.28127828],\n",
       "        [ 0.83902686,  0.85029628,  0.84348227, ..., -0.0025579 ,\n",
       "          0.1834339 ,  0.30188096],\n",
       "        [ 0.82080268,  0.85029773,  0.84324505, ..., -1.50162461,\n",
       "          0.14575948,  0.22990788]]), 0, 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [(i[-2],i[-1]) for i in ll]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(0, 0): 3662, (1, 1): 217, (1, 0): 269})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/installed/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/ian/installed/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.iloc[180:(df1.shape[0]-15)]\n",
    "df2['y1'] = [i[0] for i in y]\n",
    "df2['y2'] = [i[1] for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_code</th>\n",
       "      <th>trade_date</th>\n",
       "      <th>close_1</th>\n",
       "      <th>open_1</th>\n",
       "      <th>high_1</th>\n",
       "      <th>low_1</th>\n",
       "      <th>pre_close_1</th>\n",
       "      <th>change_1</th>\n",
       "      <th>pct_chg_1</th>\n",
       "      <th>vol_1</th>\n",
       "      <th>amount_1</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011009</td>\n",
       "      <td>0.829411</td>\n",
       "      <td>0.830096</td>\n",
       "      <td>0.832438</td>\n",
       "      <td>0.830034</td>\n",
       "      <td>0.832675</td>\n",
       "      <td>0.603728</td>\n",
       "      <td>0.724991</td>\n",
       "      <td>0.145891</td>\n",
       "      <td>0.235083</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011010</td>\n",
       "      <td>0.801824</td>\n",
       "      <td>0.839662</td>\n",
       "      <td>0.829382</td>\n",
       "      <td>0.811471</td>\n",
       "      <td>0.841408</td>\n",
       "      <td>-1.934783</td>\n",
       "      <td>-2.299412</td>\n",
       "      <td>0.210788</td>\n",
       "      <td>0.348804</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011011</td>\n",
       "      <td>0.778869</td>\n",
       "      <td>0.809945</td>\n",
       "      <td>0.804937</td>\n",
       "      <td>0.787615</td>\n",
       "      <td>0.813422</td>\n",
       "      <td>-1.609963</td>\n",
       "      <td>-1.979191</td>\n",
       "      <td>0.234323</td>\n",
       "      <td>0.368340</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011012</td>\n",
       "      <td>0.804065</td>\n",
       "      <td>0.788488</td>\n",
       "      <td>0.808499</td>\n",
       "      <td>0.770546</td>\n",
       "      <td>0.790135</td>\n",
       "      <td>1.767138</td>\n",
       "      <td>2.236433</td>\n",
       "      <td>0.388148</td>\n",
       "      <td>0.605245</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011015</td>\n",
       "      <td>0.802142</td>\n",
       "      <td>0.814690</td>\n",
       "      <td>0.809868</td>\n",
       "      <td>0.804993</td>\n",
       "      <td>0.815696</td>\n",
       "      <td>-0.134903</td>\n",
       "      <td>-0.165365</td>\n",
       "      <td>0.246036</td>\n",
       "      <td>0.384142</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011016</td>\n",
       "      <td>0.787224</td>\n",
       "      <td>0.813252</td>\n",
       "      <td>0.804193</td>\n",
       "      <td>0.794841</td>\n",
       "      <td>0.813744</td>\n",
       "      <td>-1.046212</td>\n",
       "      <td>-1.285655</td>\n",
       "      <td>0.181316</td>\n",
       "      <td>0.281191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011017</td>\n",
       "      <td>0.781122</td>\n",
       "      <td>0.796759</td>\n",
       "      <td>0.790041</td>\n",
       "      <td>0.788438</td>\n",
       "      <td>0.798611</td>\n",
       "      <td>-0.427981</td>\n",
       "      <td>-0.535914</td>\n",
       "      <td>0.147104</td>\n",
       "      <td>0.229858</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011018</td>\n",
       "      <td>0.768055</td>\n",
       "      <td>0.790540</td>\n",
       "      <td>0.784920</td>\n",
       "      <td>0.775650</td>\n",
       "      <td>0.792421</td>\n",
       "      <td>-0.916478</td>\n",
       "      <td>-1.156516</td>\n",
       "      <td>0.176715</td>\n",
       "      <td>0.263443</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011019</td>\n",
       "      <td>0.747552</td>\n",
       "      <td>0.775342</td>\n",
       "      <td>0.765589</td>\n",
       "      <td>0.747470</td>\n",
       "      <td>0.779164</td>\n",
       "      <td>-1.437917</td>\n",
       "      <td>-1.845420</td>\n",
       "      <td>0.317957</td>\n",
       "      <td>0.459681</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011022</td>\n",
       "      <td>0.722934</td>\n",
       "      <td>0.750318</td>\n",
       "      <td>0.743223</td>\n",
       "      <td>0.730095</td>\n",
       "      <td>0.758365</td>\n",
       "      <td>-1.726594</td>\n",
       "      <td>-2.276668</td>\n",
       "      <td>0.226833</td>\n",
       "      <td>0.319451</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011023</td>\n",
       "      <td>0.794194</td>\n",
       "      <td>0.790640</td>\n",
       "      <td>0.794251</td>\n",
       "      <td>0.783249</td>\n",
       "      <td>0.733391</td>\n",
       "      <td>4.997766</td>\n",
       "      <td>6.814380</td>\n",
       "      <td>0.762403</td>\n",
       "      <td>1.127082</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011024</td>\n",
       "      <td>0.816775</td>\n",
       "      <td>0.834653</td>\n",
       "      <td>0.829518</td>\n",
       "      <td>0.812439</td>\n",
       "      <td>0.805681</td>\n",
       "      <td>1.583689</td>\n",
       "      <td>1.965572</td>\n",
       "      <td>1.090954</td>\n",
       "      <td>1.746596</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011025</td>\n",
       "      <td>0.797016</td>\n",
       "      <td>0.822323</td>\n",
       "      <td>0.811979</td>\n",
       "      <td>0.806336</td>\n",
       "      <td>0.828589</td>\n",
       "      <td>-1.385736</td>\n",
       "      <td>-1.672382</td>\n",
       "      <td>0.521445</td>\n",
       "      <td>0.890591</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011026</td>\n",
       "      <td>0.797672</td>\n",
       "      <td>0.803609</td>\n",
       "      <td>0.799560</td>\n",
       "      <td>0.795960</td>\n",
       "      <td>0.808545</td>\n",
       "      <td>0.045979</td>\n",
       "      <td>0.056896</td>\n",
       "      <td>0.347903</td>\n",
       "      <td>0.555807</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011029</td>\n",
       "      <td>0.808416</td>\n",
       "      <td>0.808215</td>\n",
       "      <td>0.808697</td>\n",
       "      <td>0.809070</td>\n",
       "      <td>0.809210</td>\n",
       "      <td>0.753501</td>\n",
       "      <td>0.931144</td>\n",
       "      <td>0.294291</td>\n",
       "      <td>0.471984</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011030</td>\n",
       "      <td>0.799997</td>\n",
       "      <td>0.819836</td>\n",
       "      <td>0.810293</td>\n",
       "      <td>0.809895</td>\n",
       "      <td>0.820109</td>\n",
       "      <td>-0.590424</td>\n",
       "      <td>-0.719945</td>\n",
       "      <td>0.214293</td>\n",
       "      <td>0.340980</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011031</td>\n",
       "      <td>0.803040</td>\n",
       "      <td>0.808312</td>\n",
       "      <td>0.803014</td>\n",
       "      <td>0.798063</td>\n",
       "      <td>0.811569</td>\n",
       "      <td>0.213424</td>\n",
       "      <td>0.262980</td>\n",
       "      <td>0.217223</td>\n",
       "      <td>0.337326</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011101</td>\n",
       "      <td>0.811875</td>\n",
       "      <td>0.814776</td>\n",
       "      <td>0.816594</td>\n",
       "      <td>0.813586</td>\n",
       "      <td>0.814656</td>\n",
       "      <td>0.619632</td>\n",
       "      <td>0.760595</td>\n",
       "      <td>0.338852</td>\n",
       "      <td>0.518484</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011102</td>\n",
       "      <td>0.804077</td>\n",
       "      <td>0.820347</td>\n",
       "      <td>0.813744</td>\n",
       "      <td>0.813816</td>\n",
       "      <td>0.823618</td>\n",
       "      <td>-0.546913</td>\n",
       "      <td>-0.664017</td>\n",
       "      <td>0.230290</td>\n",
       "      <td>0.345025</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011105</td>\n",
       "      <td>0.798477</td>\n",
       "      <td>0.812604</td>\n",
       "      <td>0.802604</td>\n",
       "      <td>0.805458</td>\n",
       "      <td>0.815708</td>\n",
       "      <td>-0.392738</td>\n",
       "      <td>-0.481438</td>\n",
       "      <td>0.186302</td>\n",
       "      <td>0.279762</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011106</td>\n",
       "      <td>0.794534</td>\n",
       "      <td>0.807950</td>\n",
       "      <td>0.801278</td>\n",
       "      <td>0.804218</td>\n",
       "      <td>0.810027</td>\n",
       "      <td>-0.276540</td>\n",
       "      <td>-0.341376</td>\n",
       "      <td>0.205229</td>\n",
       "      <td>0.288812</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011107</td>\n",
       "      <td>0.757820</td>\n",
       "      <td>0.803739</td>\n",
       "      <td>0.793644</td>\n",
       "      <td>0.767161</td>\n",
       "      <td>0.806027</td>\n",
       "      <td>-2.574953</td>\n",
       "      <td>-3.194539</td>\n",
       "      <td>0.318802</td>\n",
       "      <td>0.466018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011108</td>\n",
       "      <td>0.763439</td>\n",
       "      <td>0.754195</td>\n",
       "      <td>0.763428</td>\n",
       "      <td>0.747255</td>\n",
       "      <td>0.768781</td>\n",
       "      <td>0.394138</td>\n",
       "      <td>0.512686</td>\n",
       "      <td>0.280917</td>\n",
       "      <td>0.386775</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011109</td>\n",
       "      <td>0.775081</td>\n",
       "      <td>0.772757</td>\n",
       "      <td>0.778642</td>\n",
       "      <td>0.762863</td>\n",
       "      <td>0.774482</td>\n",
       "      <td>0.816484</td>\n",
       "      <td>1.054200</td>\n",
       "      <td>0.310850</td>\n",
       "      <td>0.449318</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011112</td>\n",
       "      <td>0.770724</td>\n",
       "      <td>0.785623</td>\n",
       "      <td>0.778367</td>\n",
       "      <td>0.780338</td>\n",
       "      <td>0.786292</td>\n",
       "      <td>-0.305548</td>\n",
       "      <td>-0.388593</td>\n",
       "      <td>0.216537</td>\n",
       "      <td>0.318237</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011113</td>\n",
       "      <td>0.768222</td>\n",
       "      <td>0.775105</td>\n",
       "      <td>0.771772</td>\n",
       "      <td>0.770218</td>\n",
       "      <td>0.781873</td>\n",
       "      <td>-0.175513</td>\n",
       "      <td>-0.224473</td>\n",
       "      <td>0.184470</td>\n",
       "      <td>0.261682</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011114</td>\n",
       "      <td>0.768779</td>\n",
       "      <td>0.777897</td>\n",
       "      <td>0.773921</td>\n",
       "      <td>0.778161</td>\n",
       "      <td>0.779334</td>\n",
       "      <td>0.039044</td>\n",
       "      <td>0.050121</td>\n",
       "      <td>0.148733</td>\n",
       "      <td>0.220050</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011115</td>\n",
       "      <td>0.770795</td>\n",
       "      <td>0.779588</td>\n",
       "      <td>0.772419</td>\n",
       "      <td>0.777348</td>\n",
       "      <td>0.779899</td>\n",
       "      <td>0.141438</td>\n",
       "      <td>0.181334</td>\n",
       "      <td>0.135029</td>\n",
       "      <td>0.203414</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011116</td>\n",
       "      <td>0.782880</td>\n",
       "      <td>0.830710</td>\n",
       "      <td>0.824645</td>\n",
       "      <td>0.792627</td>\n",
       "      <td>0.781945</td>\n",
       "      <td>0.847526</td>\n",
       "      <td>1.083858</td>\n",
       "      <td>0.591455</td>\n",
       "      <td>0.949532</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20011119</td>\n",
       "      <td>0.793876</td>\n",
       "      <td>0.792023</td>\n",
       "      <td>0.793894</td>\n",
       "      <td>0.792727</td>\n",
       "      <td>0.794204</td>\n",
       "      <td>0.771239</td>\n",
       "      <td>0.971034</td>\n",
       "      <td>0.245270</td>\n",
       "      <td>0.377600</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20180921</td>\n",
       "      <td>1.329939</td>\n",
       "      <td>1.316213</td>\n",
       "      <td>1.329895</td>\n",
       "      <td>1.311897</td>\n",
       "      <td>1.316264</td>\n",
       "      <td>2.275307</td>\n",
       "      <td>1.728586</td>\n",
       "      <td>8.472403</td>\n",
       "      <td>11.113413</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20180925</td>\n",
       "      <td>1.322168</td>\n",
       "      <td>1.336045</td>\n",
       "      <td>1.326352</td>\n",
       "      <td>1.335575</td>\n",
       "      <td>1.349176</td>\n",
       "      <td>-0.545022</td>\n",
       "      <td>-0.403941</td>\n",
       "      <td>6.004220</td>\n",
       "      <td>8.096030</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20180926</td>\n",
       "      <td>1.334374</td>\n",
       "      <td>1.340980</td>\n",
       "      <td>1.344089</td>\n",
       "      <td>1.341446</td>\n",
       "      <td>1.341292</td>\n",
       "      <td>0.856055</td>\n",
       "      <td>0.638230</td>\n",
       "      <td>7.483224</td>\n",
       "      <td>10.821167</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20180927</td>\n",
       "      <td>1.327224</td>\n",
       "      <td>1.350838</td>\n",
       "      <td>1.336085</td>\n",
       "      <td>1.342882</td>\n",
       "      <td>1.353675</td>\n",
       "      <td>-0.501417</td>\n",
       "      <td>-0.370411</td>\n",
       "      <td>6.648404</td>\n",
       "      <td>9.209014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20180928</td>\n",
       "      <td>1.341284</td>\n",
       "      <td>1.345288</td>\n",
       "      <td>1.341433</td>\n",
       "      <td>1.345540</td>\n",
       "      <td>1.346422</td>\n",
       "      <td>0.986106</td>\n",
       "      <td>0.732389</td>\n",
       "      <td>7.200273</td>\n",
       "      <td>9.352414</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181008</td>\n",
       "      <td>1.291443</td>\n",
       "      <td>1.332743</td>\n",
       "      <td>1.317751</td>\n",
       "      <td>1.306184</td>\n",
       "      <td>1.360686</td>\n",
       "      <td>-3.495589</td>\n",
       "      <td>-2.568890</td>\n",
       "      <td>8.015840</td>\n",
       "      <td>10.558042</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181009</td>\n",
       "      <td>1.293584</td>\n",
       "      <td>1.306515</td>\n",
       "      <td>1.299864</td>\n",
       "      <td>1.306675</td>\n",
       "      <td>1.310123</td>\n",
       "      <td>0.150127</td>\n",
       "      <td>0.114552</td>\n",
       "      <td>6.260977</td>\n",
       "      <td>8.227652</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181010</td>\n",
       "      <td>1.295877</td>\n",
       "      <td>1.311326</td>\n",
       "      <td>1.304254</td>\n",
       "      <td>1.302755</td>\n",
       "      <td>1.312295</td>\n",
       "      <td>0.160833</td>\n",
       "      <td>0.122572</td>\n",
       "      <td>6.084783</td>\n",
       "      <td>8.303743</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181011</td>\n",
       "      <td>1.228189</td>\n",
       "      <td>1.272497</td>\n",
       "      <td>1.265148</td>\n",
       "      <td>1.233958</td>\n",
       "      <td>1.314621</td>\n",
       "      <td>-4.747239</td>\n",
       "      <td>-3.610992</td>\n",
       "      <td>10.570660</td>\n",
       "      <td>12.686055</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181012</td>\n",
       "      <td>1.239340</td>\n",
       "      <td>1.239262</td>\n",
       "      <td>1.243580</td>\n",
       "      <td>1.222559</td>\n",
       "      <td>1.245954</td>\n",
       "      <td>0.782042</td>\n",
       "      <td>0.627653</td>\n",
       "      <td>9.119294</td>\n",
       "      <td>10.652906</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181015</td>\n",
       "      <td>1.220887</td>\n",
       "      <td>1.254606</td>\n",
       "      <td>1.241703</td>\n",
       "      <td>1.236088</td>\n",
       "      <td>1.257266</td>\n",
       "      <td>-1.294148</td>\n",
       "      <td>-1.029312</td>\n",
       "      <td>6.357719</td>\n",
       "      <td>7.848862</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181016</td>\n",
       "      <td>1.210538</td>\n",
       "      <td>1.236240</td>\n",
       "      <td>1.234927</td>\n",
       "      <td>1.222451</td>\n",
       "      <td>1.238547</td>\n",
       "      <td>-0.725820</td>\n",
       "      <td>-0.586035</td>\n",
       "      <td>6.402915</td>\n",
       "      <td>7.966542</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181017</td>\n",
       "      <td>1.217804</td>\n",
       "      <td>1.239393</td>\n",
       "      <td>1.227717</td>\n",
       "      <td>1.213357</td>\n",
       "      <td>1.228048</td>\n",
       "      <td>0.509616</td>\n",
       "      <td>0.415002</td>\n",
       "      <td>6.967764</td>\n",
       "      <td>8.756996</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181018</td>\n",
       "      <td>1.182056</td>\n",
       "      <td>1.225237</td>\n",
       "      <td>1.209824</td>\n",
       "      <td>1.197957</td>\n",
       "      <td>1.235420</td>\n",
       "      <td>-2.507182</td>\n",
       "      <td>-2.029381</td>\n",
       "      <td>6.725120</td>\n",
       "      <td>7.950467</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181019</td>\n",
       "      <td>1.212504</td>\n",
       "      <td>1.184396</td>\n",
       "      <td>1.213853</td>\n",
       "      <td>1.180403</td>\n",
       "      <td>1.199154</td>\n",
       "      <td>2.135456</td>\n",
       "      <td>1.780781</td>\n",
       "      <td>7.899071</td>\n",
       "      <td>9.704961</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181022</td>\n",
       "      <td>1.262142</td>\n",
       "      <td>1.235219</td>\n",
       "      <td>1.271860</td>\n",
       "      <td>1.236525</td>\n",
       "      <td>1.230043</td>\n",
       "      <td>3.481295</td>\n",
       "      <td>2.830142</td>\n",
       "      <td>11.360857</td>\n",
       "      <td>14.719017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181023</td>\n",
       "      <td>1.233593</td>\n",
       "      <td>1.277107</td>\n",
       "      <td>1.264447</td>\n",
       "      <td>1.245016</td>\n",
       "      <td>1.280398</td>\n",
       "      <td>-2.002224</td>\n",
       "      <td>-1.563705</td>\n",
       "      <td>9.562274</td>\n",
       "      <td>12.451938</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181024</td>\n",
       "      <td>1.237620</td>\n",
       "      <td>1.242117</td>\n",
       "      <td>1.255219</td>\n",
       "      <td>1.242371</td>\n",
       "      <td>1.251437</td>\n",
       "      <td>0.282395</td>\n",
       "      <td>0.225648</td>\n",
       "      <td>8.581984</td>\n",
       "      <td>10.599117</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181025</td>\n",
       "      <td>1.237860</td>\n",
       "      <td>1.223323</td>\n",
       "      <td>1.238913</td>\n",
       "      <td>1.220086</td>\n",
       "      <td>1.255522</td>\n",
       "      <td>0.016821</td>\n",
       "      <td>0.013412</td>\n",
       "      <td>8.687748</td>\n",
       "      <td>10.267005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181026</td>\n",
       "      <td>1.235505</td>\n",
       "      <td>1.257007</td>\n",
       "      <td>1.248428</td>\n",
       "      <td>1.243852</td>\n",
       "      <td>1.255765</td>\n",
       "      <td>-0.165134</td>\n",
       "      <td>-0.131490</td>\n",
       "      <td>8.521703</td>\n",
       "      <td>9.952596</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181029</td>\n",
       "      <td>1.208529</td>\n",
       "      <td>1.248674</td>\n",
       "      <td>1.233891</td>\n",
       "      <td>1.218955</td>\n",
       "      <td>1.253376</td>\n",
       "      <td>-1.891951</td>\n",
       "      <td>-1.509437</td>\n",
       "      <td>7.197670</td>\n",
       "      <td>9.138320</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181030</td>\n",
       "      <td>1.220863</td>\n",
       "      <td>1.222186</td>\n",
       "      <td>1.229794</td>\n",
       "      <td>1.215387</td>\n",
       "      <td>1.226010</td>\n",
       "      <td>0.865057</td>\n",
       "      <td>0.705565</td>\n",
       "      <td>8.937032</td>\n",
       "      <td>11.367763</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181031</td>\n",
       "      <td>1.237377</td>\n",
       "      <td>1.238768</td>\n",
       "      <td>1.242156</td>\n",
       "      <td>1.237192</td>\n",
       "      <td>1.238523</td>\n",
       "      <td>1.158146</td>\n",
       "      <td>0.935085</td>\n",
       "      <td>9.680597</td>\n",
       "      <td>11.554915</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181101</td>\n",
       "      <td>1.239019</td>\n",
       "      <td>1.259960</td>\n",
       "      <td>1.253507</td>\n",
       "      <td>1.254843</td>\n",
       "      <td>1.255275</td>\n",
       "      <td>0.115164</td>\n",
       "      <td>0.091739</td>\n",
       "      <td>10.771898</td>\n",
       "      <td>13.446664</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181102</td>\n",
       "      <td>1.272411</td>\n",
       "      <td>1.275471</td>\n",
       "      <td>1.272369</td>\n",
       "      <td>1.266997</td>\n",
       "      <td>1.256940</td>\n",
       "      <td>2.341925</td>\n",
       "      <td>1.863118</td>\n",
       "      <td>12.065304</td>\n",
       "      <td>15.506162</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181105</td>\n",
       "      <td>1.267159</td>\n",
       "      <td>1.283259</td>\n",
       "      <td>1.270806</td>\n",
       "      <td>1.272010</td>\n",
       "      <td>1.290815</td>\n",
       "      <td>-0.368285</td>\n",
       "      <td>-0.285309</td>\n",
       "      <td>10.388866</td>\n",
       "      <td>12.576481</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181106</td>\n",
       "      <td>1.264272</td>\n",
       "      <td>1.280993</td>\n",
       "      <td>1.264946</td>\n",
       "      <td>1.270106</td>\n",
       "      <td>1.285488</td>\n",
       "      <td>-0.202527</td>\n",
       "      <td>-0.157553</td>\n",
       "      <td>8.775492</td>\n",
       "      <td>10.354342</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181107</td>\n",
       "      <td>1.255708</td>\n",
       "      <td>1.280572</td>\n",
       "      <td>1.271989</td>\n",
       "      <td>1.271998</td>\n",
       "      <td>1.282559</td>\n",
       "      <td>-0.600640</td>\n",
       "      <td>-0.468303</td>\n",
       "      <td>9.286598</td>\n",
       "      <td>10.985510</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181108</td>\n",
       "      <td>1.252993</td>\n",
       "      <td>1.280688</td>\n",
       "      <td>1.265662</td>\n",
       "      <td>1.268651</td>\n",
       "      <td>1.273871</td>\n",
       "      <td>-0.190377</td>\n",
       "      <td>-0.149464</td>\n",
       "      <td>8.559201</td>\n",
       "      <td>9.598501</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>000001.SH</td>\n",
       "      <td>20181109</td>\n",
       "      <td>1.235517</td>\n",
       "      <td>1.261985</td>\n",
       "      <td>1.246109</td>\n",
       "      <td>1.252197</td>\n",
       "      <td>1.271117</td>\n",
       "      <td>-1.225684</td>\n",
       "      <td>-0.964259</td>\n",
       "      <td>8.173738</td>\n",
       "      <td>9.045851</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4148 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ts_code trade_date   close_1    open_1 ...      vol_1   amount_1  y1  y2\n",
       "1263  000001.SH   20011009  0.829411  0.830096 ...   0.145891   0.235083   0   0\n",
       "1262  000001.SH   20011010  0.801824  0.839662 ...   0.210788   0.348804   0   0\n",
       "1261  000001.SH   20011011  0.778869  0.809945 ...   0.234323   0.368340   0   0\n",
       "1260  000001.SH   20011012  0.804065  0.788488 ...   0.388148   0.605245   0   0\n",
       "1259  000001.SH   20011015  0.802142  0.814690 ...   0.246036   0.384142   0   0\n",
       "1258  000001.SH   20011016  0.787224  0.813252 ...   0.181316   0.281191   0   0\n",
       "1257  000001.SH   20011017  0.781122  0.796759 ...   0.147104   0.229858   0   0\n",
       "1256  000001.SH   20011018  0.768055  0.790540 ...   0.176715   0.263443   0   0\n",
       "1255  000001.SH   20011019  0.747552  0.775342 ...   0.317957   0.459681   0   0\n",
       "1254  000001.SH   20011022  0.722934  0.750318 ...   0.226833   0.319451   1   1\n",
       "1253  000001.SH   20011023  0.794194  0.790640 ...   0.762403   1.127082   0   0\n",
       "1252  000001.SH   20011024  0.816775  0.834653 ...   1.090954   1.746596   0   0\n",
       "1251  000001.SH   20011025  0.797016  0.822323 ...   0.521445   0.890591   0   0\n",
       "1250  000001.SH   20011026  0.797672  0.803609 ...   0.347903   0.555807   0   0\n",
       "1249  000001.SH   20011029  0.808416  0.808215 ...   0.294291   0.471984   0   0\n",
       "1248  000001.SH   20011030  0.799997  0.819836 ...   0.214293   0.340980   0   0\n",
       "1247  000001.SH   20011031  0.803040  0.808312 ...   0.217223   0.337326   0   0\n",
       "1246  000001.SH   20011101  0.811875  0.814776 ...   0.338852   0.518484   0   0\n",
       "1245  000001.SH   20011102  0.804077  0.820347 ...   0.230290   0.345025   0   0\n",
       "1244  000001.SH   20011105  0.798477  0.812604 ...   0.186302   0.279762   0   0\n",
       "1243  000001.SH   20011106  0.794534  0.807950 ...   0.205229   0.288812   0   0\n",
       "1242  000001.SH   20011107  0.757820  0.803739 ...   0.318802   0.466018   1   1\n",
       "1241  000001.SH   20011108  0.763439  0.754195 ...   0.280917   0.386775   1   0\n",
       "1240  000001.SH   20011109  0.775081  0.772757 ...   0.310850   0.449318   0   0\n",
       "1239  000001.SH   20011112  0.770724  0.785623 ...   0.216537   0.318237   0   0\n",
       "1238  000001.SH   20011113  0.768222  0.775105 ...   0.184470   0.261682   1   1\n",
       "1237  000001.SH   20011114  0.768779  0.777897 ...   0.148733   0.220050   1   0\n",
       "1236  000001.SH   20011115  0.770795  0.779588 ...   0.135029   0.203414   1   0\n",
       "1235  000001.SH   20011116  0.782880  0.830710 ...   0.591455   0.949532   1   0\n",
       "1234  000001.SH   20011119  0.793876  0.792023 ...   0.245270   0.377600   1   0\n",
       "...         ...        ...       ...       ... ...        ...        ...  ..  ..\n",
       "44    000001.SH   20180921  1.329939  1.316213 ...   8.472403  11.113413   0   0\n",
       "43    000001.SH   20180925  1.322168  1.336045 ...   6.004220   8.096030   0   0\n",
       "42    000001.SH   20180926  1.334374  1.340980 ...   7.483224  10.821167   0   0\n",
       "41    000001.SH   20180927  1.327224  1.350838 ...   6.648404   9.209014   0   0\n",
       "40    000001.SH   20180928  1.341284  1.345288 ...   7.200273   9.352414   0   0\n",
       "39    000001.SH   20181008  1.291443  1.332743 ...   8.015840  10.558042   0   0\n",
       "38    000001.SH   20181009  1.293584  1.306515 ...   6.260977   8.227652   0   0\n",
       "37    000001.SH   20181010  1.295877  1.311326 ...   6.084783   8.303743   0   0\n",
       "36    000001.SH   20181011  1.228189  1.272497 ...  10.570660  12.686055   0   0\n",
       "35    000001.SH   20181012  1.239340  1.239262 ...   9.119294  10.652906   0   0\n",
       "34    000001.SH   20181015  1.220887  1.254606 ...   6.357719   7.848862   0   0\n",
       "33    000001.SH   20181016  1.210538  1.236240 ...   6.402915   7.966542   0   0\n",
       "32    000001.SH   20181017  1.217804  1.239393 ...   6.967764   8.756996   0   0\n",
       "31    000001.SH   20181018  1.182056  1.225237 ...   6.725120   7.950467   1   1\n",
       "30    000001.SH   20181019  1.212504  1.184396 ...   7.899071   9.704961   0   0\n",
       "29    000001.SH   20181022  1.262142  1.235219 ...  11.360857  14.719017   0   0\n",
       "28    000001.SH   20181023  1.233593  1.277107 ...   9.562274  12.451938   0   0\n",
       "27    000001.SH   20181024  1.237620  1.242117 ...   8.581984  10.599117   0   0\n",
       "26    000001.SH   20181025  1.237860  1.223323 ...   8.687748  10.267005   0   0\n",
       "25    000001.SH   20181026  1.235505  1.257007 ...   8.521703   9.952596   0   0\n",
       "24    000001.SH   20181029  1.208529  1.248674 ...   7.197670   9.138320   1   1\n",
       "23    000001.SH   20181030  1.220863  1.222186 ...   8.937032  11.367763   1   0\n",
       "22    000001.SH   20181031  1.237377  1.238768 ...   9.680597  11.554915   0   0\n",
       "21    000001.SH   20181101  1.239019  1.259960 ...  10.771898  13.446664   0   0\n",
       "20    000001.SH   20181102  1.272411  1.275471 ...  12.065304  15.506162   0   0\n",
       "19    000001.SH   20181105  1.267159  1.283259 ...  10.388866  12.576481   0   0\n",
       "18    000001.SH   20181106  1.264272  1.280993 ...   8.775492  10.354342   0   0\n",
       "17    000001.SH   20181107  1.255708  1.280572 ...   9.286598  10.985510   0   0\n",
       "16    000001.SH   20181108  1.252993  1.280688 ...   8.559201   9.598501   0   0\n",
       "15    000001.SH   20181109  1.235517  1.261985 ...   8.173738   9.045851   0   0\n",
       "\n",
       "[4148 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 由于数据不均衡，买点的数据很少。买点数据:非买点数据≈1:7.所以，额外复制6份买点数据，使得数据均衡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(0, 0): 3424, (1, 0): 2096, (1, 1): 1696})\n"
     ]
    }
   ],
   "source": [
    "x,y1,y2 = t.preprocess(ll[:-250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = t.splitData(x,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5051, 2165)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train),len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 打乱数据\n",
    "# np.random.seed(14)\n",
    "# shuffle_indices = np.random.permutation(np.arange(len(y1)))\n",
    "# x_shuffled = x[shuffle_indices]\n",
    "# y1_shuffled = y1[shuffle_indices]\n",
    "# y2_shuffled = y2[shuffle_indices]\n",
    "\n",
    "# # x1 = x.reshape(x.shape[0],-1)\n",
    "# x_train, x_test = x_shuffled[:3000], x_shuffled[3000:]\n",
    "\n",
    "# y11 = np_utils.to_categorical(y1_shuffled,num_classes=2)\n",
    "# y_train, y_test = y11[:3000], y11[3000:]\n",
    "\n",
    "# a = [y_train[i,1] for i in range(y_train.shape[0])]\n",
    "# Counter(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 256)               204288    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               51400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 256,090\n",
      "Trainable params: 256,090\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = t.buildModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "5051/5051 [==============================] - 28s 5ms/step - loss: 0.6920 - acc: 0.5173\n",
      "Epoch 2/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.6794 - acc: 0.5597\n",
      "Epoch 3/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.6748 - acc: 0.5714\n",
      "Epoch 4/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.6689 - acc: 0.5848\n",
      "Epoch 5/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.6608 - acc: 0.6094\n",
      "Epoch 6/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.6505 - acc: 0.6234\n",
      "Epoch 7/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.6417 - acc: 0.6322\n",
      "Epoch 8/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.6318 - acc: 0.6474\n",
      "Epoch 9/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.6256 - acc: 0.6577\n",
      "Epoch 10/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.6167 - acc: 0.6634\n",
      "Epoch 11/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.6134 - acc: 0.6672\n",
      "Epoch 12/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.6139 - acc: 0.6611\n",
      "Epoch 13/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.6108 - acc: 0.6605\n",
      "Epoch 14/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.6008 - acc: 0.6826\n",
      "Epoch 15/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.5975 - acc: 0.6805\n",
      "Epoch 16/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.5947 - acc: 0.6785\n",
      "Epoch 17/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.5903 - acc: 0.6757\n",
      "Epoch 18/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.5860 - acc: 0.6791\n",
      "Epoch 19/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.5846 - acc: 0.6816\n",
      "Epoch 20/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.5778 - acc: 0.6850\n",
      "Epoch 21/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.5811 - acc: 0.6846\n",
      "Epoch 22/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.5797 - acc: 0.6872\n",
      "Epoch 23/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.5667 - acc: 0.6917\n",
      "Epoch 24/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.5619 - acc: 0.6937\n",
      "Epoch 25/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.5577 - acc: 0.6915\n",
      "Epoch 26/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.5526 - acc: 0.7032\n",
      "Epoch 27/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.5594 - acc: 0.7001\n",
      "Epoch 28/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.5417 - acc: 0.7074\n",
      "Epoch 29/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.5347 - acc: 0.7127\n",
      "Epoch 30/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.5303 - acc: 0.7159\n",
      "Epoch 31/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.5247 - acc: 0.7189\n",
      "Epoch 32/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.5214 - acc: 0.7214\n",
      "Epoch 33/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.5157 - acc: 0.7242\n",
      "Epoch 34/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.5080 - acc: 0.7323\n",
      "Epoch 35/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.5042 - acc: 0.7355\n",
      "Epoch 36/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.4982 - acc: 0.7454\n",
      "Epoch 37/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.4915 - acc: 0.7535\n",
      "Epoch 38/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.4888 - acc: 0.7533\n",
      "Epoch 39/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.4822 - acc: 0.7547\n",
      "Epoch 40/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.4729 - acc: 0.7612\n",
      "Epoch 41/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.4860 - acc: 0.7583\n",
      "Epoch 42/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.4649 - acc: 0.7640\n",
      "Epoch 43/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.4596 - acc: 0.7733\n",
      "Epoch 44/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.4587 - acc: 0.7715\n",
      "Epoch 45/300\n",
      "5051/5051 [==============================] - 28s 5ms/step - loss: 0.4466 - acc: 0.7816\n",
      "Epoch 46/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.4512 - acc: 0.7808\n",
      "Epoch 47/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.4381 - acc: 0.7868\n",
      "Epoch 48/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.4381 - acc: 0.7850\n",
      "Epoch 49/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.4372 - acc: 0.7862\n",
      "Epoch 50/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.4345 - acc: 0.7872\n",
      "Epoch 51/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.4207 - acc: 0.7967\n",
      "Epoch 52/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.4256 - acc: 0.7901\n",
      "Epoch 53/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.4373 - acc: 0.7870\n",
      "Epoch 54/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.4210 - acc: 0.7961\n",
      "Epoch 55/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.4142 - acc: 0.8024\n",
      "Epoch 56/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.4183 - acc: 0.7977\n",
      "Epoch 57/300\n",
      "5051/5051 [==============================] - 28s 5ms/step - loss: 0.4060 - acc: 0.8052\n",
      "Epoch 58/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.4011 - acc: 0.8082\n",
      "Epoch 59/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3987 - acc: 0.8060\n",
      "Epoch 60/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3958 - acc: 0.8103\n",
      "Epoch 61/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3970 - acc: 0.8155\n",
      "Epoch 62/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3984 - acc: 0.8091\n",
      "Epoch 63/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3926 - acc: 0.8103\n",
      "Epoch 64/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3851 - acc: 0.8171\n",
      "Epoch 65/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3917 - acc: 0.8099\n",
      "Epoch 66/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3781 - acc: 0.8183\n",
      "Epoch 67/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3776 - acc: 0.8224\n",
      "Epoch 68/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3798 - acc: 0.8190\n",
      "Epoch 69/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3745 - acc: 0.8218\n",
      "Epoch 70/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3719 - acc: 0.8220\n",
      "Epoch 71/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3760 - acc: 0.8167\n",
      "Epoch 72/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3661 - acc: 0.8270\n",
      "Epoch 73/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3607 - acc: 0.8274\n",
      "Epoch 74/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3614 - acc: 0.8311\n",
      "Epoch 75/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3618 - acc: 0.8256\n",
      "Epoch 76/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3595 - acc: 0.8282\n",
      "Epoch 77/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3639 - acc: 0.8266\n",
      "Epoch 78/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3608 - acc: 0.8260\n",
      "Epoch 79/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3551 - acc: 0.8325\n",
      "Epoch 80/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3524 - acc: 0.8311\n",
      "Epoch 81/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3437 - acc: 0.8375\n",
      "Epoch 82/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3425 - acc: 0.8355\n",
      "Epoch 83/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3380 - acc: 0.8392\n",
      "Epoch 84/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3438 - acc: 0.8402\n",
      "Epoch 85/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3374 - acc: 0.8388\n",
      "Epoch 86/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3390 - acc: 0.8384\n",
      "Epoch 87/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3528 - acc: 0.8307\n",
      "Epoch 88/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3372 - acc: 0.8377\n",
      "Epoch 89/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3321 - acc: 0.8434\n",
      "Epoch 90/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3328 - acc: 0.8361\n",
      "Epoch 91/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3403 - acc: 0.8414\n",
      "Epoch 92/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3294 - acc: 0.8438\n",
      "Epoch 93/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3474 - acc: 0.8341\n",
      "Epoch 94/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3442 - acc: 0.8365\n",
      "Epoch 95/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3255 - acc: 0.8462\n",
      "Epoch 96/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3308 - acc: 0.8430\n",
      "Epoch 97/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3281 - acc: 0.8390\n",
      "Epoch 98/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3380 - acc: 0.8398\n",
      "Epoch 99/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3209 - acc: 0.8468\n",
      "Epoch 100/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3173 - acc: 0.8507\n",
      "Epoch 101/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3135 - acc: 0.8527\n",
      "Epoch 102/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3168 - acc: 0.8515\n",
      "Epoch 103/300\n",
      "5051/5051 [==============================] - 28s 5ms/step - loss: 0.3155 - acc: 0.8489\n",
      "Epoch 104/300\n",
      "5051/5051 [==============================] - 27s 5ms/step - loss: 0.3102 - acc: 0.8525\n",
      "Epoch 105/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.3113 - acc: 0.8533\n",
      "Epoch 106/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.3149 - acc: 0.8527\n",
      "Epoch 107/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.3104 - acc: 0.8553\n",
      "Epoch 108/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.3102 - acc: 0.8582\n",
      "Epoch 109/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.3133 - acc: 0.8545\n",
      "Epoch 110/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.3086 - acc: 0.8531\n",
      "Epoch 111/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.3167 - acc: 0.8541\n",
      "Epoch 112/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.3030 - acc: 0.8610\n",
      "Epoch 113/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2987 - acc: 0.8596\n",
      "Epoch 114/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.3069 - acc: 0.8618\n",
      "Epoch 115/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.3160 - acc: 0.8539\n",
      "Epoch 116/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.3012 - acc: 0.8630\n",
      "Epoch 117/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2998 - acc: 0.8614\n",
      "Epoch 118/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.2954 - acc: 0.8610\n",
      "Epoch 119/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.3046 - acc: 0.8654\n",
      "Epoch 120/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2973 - acc: 0.8642\n",
      "Epoch 121/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.2985 - acc: 0.8685\n",
      "Epoch 122/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2954 - acc: 0.8679\n",
      "Epoch 123/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2907 - acc: 0.8656\n",
      "Epoch 124/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.3076 - acc: 0.8569\n",
      "Epoch 125/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2992 - acc: 0.8584\n",
      "Epoch 126/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2899 - acc: 0.8699\n",
      "Epoch 127/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2890 - acc: 0.8695\n",
      "Epoch 128/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.2871 - acc: 0.8721\n",
      "Epoch 129/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2849 - acc: 0.8739\n",
      "Epoch 130/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.2913 - acc: 0.8683\n",
      "Epoch 131/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.2844 - acc: 0.8741\n",
      "Epoch 132/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2813 - acc: 0.8751\n",
      "Epoch 133/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2750 - acc: 0.8776\n",
      "Epoch 134/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2802 - acc: 0.8725\n",
      "Epoch 135/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2740 - acc: 0.8755\n",
      "Epoch 136/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2989 - acc: 0.8670\n",
      "Epoch 137/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2810 - acc: 0.8775\n",
      "Epoch 138/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.2683 - acc: 0.8814\n",
      "Epoch 139/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2719 - acc: 0.8796\n",
      "Epoch 140/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.2724 - acc: 0.8794\n",
      "Epoch 141/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2748 - acc: 0.8739\n",
      "Epoch 142/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2685 - acc: 0.8826\n",
      "Epoch 143/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2685 - acc: 0.8830\n",
      "Epoch 144/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2633 - acc: 0.8858\n",
      "Epoch 145/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2661 - acc: 0.8836\n",
      "Epoch 146/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2661 - acc: 0.8840\n",
      "Epoch 147/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.2564 - acc: 0.8887\n",
      "Epoch 148/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.2629 - acc: 0.8854\n",
      "Epoch 149/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2576 - acc: 0.8943\n",
      "Epoch 150/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2586 - acc: 0.8901\n",
      "Epoch 151/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2699 - acc: 0.8866\n",
      "Epoch 152/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2612 - acc: 0.8872\n",
      "Epoch 153/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.2602 - acc: 0.8877\n",
      "Epoch 154/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2513 - acc: 0.8984\n",
      "Epoch 155/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2495 - acc: 0.8921\n",
      "Epoch 156/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.2510 - acc: 0.8931\n",
      "Epoch 157/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2461 - acc: 0.9004\n",
      "Epoch 158/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2537 - acc: 0.8959\n",
      "Epoch 159/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2434 - acc: 0.9032\n",
      "Epoch 160/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2423 - acc: 0.9020\n",
      "Epoch 161/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2495 - acc: 0.8951\n",
      "Epoch 162/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.2348 - acc: 0.9020\n",
      "Epoch 163/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.2360 - acc: 0.9058\n",
      "Epoch 164/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2460 - acc: 0.8996\n",
      "Epoch 165/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2439 - acc: 0.9030\n",
      "Epoch 166/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2329 - acc: 0.9050\n",
      "Epoch 167/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2305 - acc: 0.9052\n",
      "Epoch 168/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2304 - acc: 0.9075\n",
      "Epoch 169/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.2303 - acc: 0.9071\n",
      "Epoch 170/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2231 - acc: 0.9131\n",
      "Epoch 171/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.2292 - acc: 0.9087\n",
      "Epoch 172/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2293 - acc: 0.9075\n",
      "Epoch 173/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2644 - acc: 0.8889\n",
      "Epoch 174/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2264 - acc: 0.9113\n",
      "Epoch 175/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2207 - acc: 0.9149\n",
      "Epoch 176/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2236 - acc: 0.9089\n",
      "Epoch 177/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2198 - acc: 0.9129\n",
      "Epoch 178/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2169 - acc: 0.9151\n",
      "Epoch 179/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2162 - acc: 0.9123\n",
      "Epoch 180/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2177 - acc: 0.9149\n",
      "Epoch 181/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2115 - acc: 0.9174\n",
      "Epoch 182/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.2025 - acc: 0.9236\n",
      "Epoch 183/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2065 - acc: 0.9206\n",
      "Epoch 184/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.2242 - acc: 0.9135\n",
      "Epoch 185/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.2038 - acc: 0.9202\n",
      "Epoch 186/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2071 - acc: 0.9192\n",
      "Epoch 187/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2090 - acc: 0.9145\n",
      "Epoch 188/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1988 - acc: 0.9220\n",
      "Epoch 189/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1986 - acc: 0.9244\n",
      "Epoch 190/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2013 - acc: 0.9230\n",
      "Epoch 191/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.2035 - acc: 0.9167\n",
      "Epoch 192/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1986 - acc: 0.9250\n",
      "Epoch 193/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1996 - acc: 0.9232\n",
      "Epoch 194/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1997 - acc: 0.9236\n",
      "Epoch 195/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1909 - acc: 0.9277\n",
      "Epoch 196/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1954 - acc: 0.9262\n",
      "Epoch 197/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1919 - acc: 0.9273\n",
      "Epoch 198/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1957 - acc: 0.9258\n",
      "Epoch 199/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1969 - acc: 0.9234\n",
      "Epoch 200/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2015 - acc: 0.9206\n",
      "Epoch 201/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1920 - acc: 0.9299\n",
      "Epoch 202/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1814 - acc: 0.9335\n",
      "Epoch 203/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1891 - acc: 0.9269\n",
      "Epoch 204/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1894 - acc: 0.9329\n",
      "Epoch 205/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1807 - acc: 0.9323\n",
      "Epoch 206/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1839 - acc: 0.9339\n",
      "Epoch 207/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1817 - acc: 0.9337\n",
      "Epoch 208/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1823 - acc: 0.9315\n",
      "Epoch 209/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1831 - acc: 0.9345\n",
      "Epoch 210/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1937 - acc: 0.9277\n",
      "Epoch 211/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1850 - acc: 0.9293\n",
      "Epoch 212/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1775 - acc: 0.9390\n",
      "Epoch 213/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1839 - acc: 0.9339\n",
      "Epoch 214/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1811 - acc: 0.9361\n",
      "Epoch 215/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.2010 - acc: 0.9273\n",
      "Epoch 216/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1784 - acc: 0.9370\n",
      "Epoch 217/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1745 - acc: 0.9378\n",
      "Epoch 218/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1793 - acc: 0.9329\n",
      "Epoch 219/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1725 - acc: 0.9402\n",
      "Epoch 220/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1724 - acc: 0.9361\n",
      "Epoch 221/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1743 - acc: 0.9374\n",
      "Epoch 222/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1732 - acc: 0.9374\n",
      "Epoch 223/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1816 - acc: 0.9323\n",
      "Epoch 224/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1734 - acc: 0.9368\n",
      "Epoch 225/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1830 - acc: 0.9370\n",
      "Epoch 226/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1708 - acc: 0.9404\n",
      "Epoch 227/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1681 - acc: 0.9428\n",
      "Epoch 228/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1624 - acc: 0.9450\n",
      "Epoch 229/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1666 - acc: 0.9422\n",
      "Epoch 230/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1641 - acc: 0.9412\n",
      "Epoch 231/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1660 - acc: 0.9408\n",
      "Epoch 232/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1764 - acc: 0.9380\n",
      "Epoch 233/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1670 - acc: 0.9430\n",
      "Epoch 234/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1608 - acc: 0.9432\n",
      "Epoch 235/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1760 - acc: 0.9351\n",
      "Epoch 236/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1647 - acc: 0.9432\n",
      "Epoch 237/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1646 - acc: 0.9424\n",
      "Epoch 238/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1594 - acc: 0.9458\n",
      "Epoch 239/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1636 - acc: 0.9422\n",
      "Epoch 240/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1546 - acc: 0.9469\n",
      "Epoch 241/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1525 - acc: 0.9463\n",
      "Epoch 242/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1544 - acc: 0.9469\n",
      "Epoch 243/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1564 - acc: 0.9467\n",
      "Epoch 244/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1565 - acc: 0.9473\n",
      "Epoch 245/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1750 - acc: 0.9382\n",
      "Epoch 246/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1599 - acc: 0.9408\n",
      "Epoch 247/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1587 - acc: 0.9450\n",
      "Epoch 248/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1604 - acc: 0.9446\n",
      "Epoch 249/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1598 - acc: 0.9452\n",
      "Epoch 250/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1551 - acc: 0.9456\n",
      "Epoch 251/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1559 - acc: 0.9481\n",
      "Epoch 252/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1499 - acc: 0.9463\n",
      "Epoch 253/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1498 - acc: 0.9529\n",
      "Epoch 254/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1549 - acc: 0.9469\n",
      "Epoch 255/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1777 - acc: 0.9327\n",
      "Epoch 256/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1621 - acc: 0.9440\n",
      "Epoch 257/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1475 - acc: 0.9479\n",
      "Epoch 258/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1391 - acc: 0.9535\n",
      "Epoch 259/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1511 - acc: 0.9458\n",
      "Epoch 260/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1716 - acc: 0.9380\n",
      "Epoch 261/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1671 - acc: 0.9368\n",
      "Epoch 262/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1451 - acc: 0.9497\n",
      "Epoch 263/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1425 - acc: 0.9519\n",
      "Epoch 264/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1425 - acc: 0.9489\n",
      "Epoch 265/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1428 - acc: 0.9521\n",
      "Epoch 266/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1476 - acc: 0.9511\n",
      "Epoch 267/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1383 - acc: 0.9545\n",
      "Epoch 268/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1359 - acc: 0.9545\n",
      "Epoch 269/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1395 - acc: 0.9531\n",
      "Epoch 270/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1494 - acc: 0.9497\n",
      "Epoch 271/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1353 - acc: 0.9543\n",
      "Epoch 272/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1327 - acc: 0.9555\n",
      "Epoch 273/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1494 - acc: 0.9489\n",
      "Epoch 274/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1497 - acc: 0.9491\n",
      "Epoch 275/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1454 - acc: 0.9444\n",
      "Epoch 276/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1373 - acc: 0.9543\n",
      "Epoch 277/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1299 - acc: 0.9566\n",
      "Epoch 278/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1396 - acc: 0.9533\n",
      "Epoch 279/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1351 - acc: 0.9549\n",
      "Epoch 280/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1329 - acc: 0.9549\n",
      "Epoch 281/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1293 - acc: 0.9560\n",
      "Epoch 282/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1333 - acc: 0.9566\n",
      "Epoch 283/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1368 - acc: 0.9529\n",
      "Epoch 284/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1300 - acc: 0.9560\n",
      "Epoch 285/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1336 - acc: 0.9557\n",
      "Epoch 286/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1294 - acc: 0.9559\n",
      "Epoch 287/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1345 - acc: 0.9519\n",
      "Epoch 288/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1542 - acc: 0.9452\n",
      "Epoch 289/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1388 - acc: 0.9515\n",
      "Epoch 290/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1245 - acc: 0.9606\n",
      "Epoch 291/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1250 - acc: 0.9604\n",
      "Epoch 292/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1532 - acc: 0.9487\n",
      "Epoch 293/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1256 - acc: 0.9596\n",
      "Epoch 294/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1250 - acc: 0.9578\n",
      "Epoch 295/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1210 - acc: 0.9584\n",
      "Epoch 296/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1182 - acc: 0.9618\n",
      "Epoch 297/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1183 - acc: 0.9594\n",
      "Epoch 298/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1148 - acc: 0.9622\n",
      "Epoch 299/300\n",
      "5051/5051 [==============================] - 31s 6ms/step - loss: 0.1209 - acc: 0.9592\n",
      "Epoch 300/300\n",
      "5051/5051 [==============================] - 32s 6ms/step - loss: 0.1149 - acc: 0.9646\n",
      "@ Total Time Spent: 8968.76 seconds\n"
     ]
    }
   ],
   "source": [
    "t.train(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2165/2165 [==============================] - 7s 3ms/step\n",
      "test loss 0.25568237271134076\n",
      "test accuracy 0.9274826789838337\n"
     ]
    }
   ],
   "source": [
    "# 评估模型\n",
    "loss,accuracy = t._model.evaluate(x_test,y_test)\n",
    "\n",
    "print('test loss',loss)\n",
    "print('test accuracy',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t._model.save('model1201_9_GRU.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(0, 0): 238, (1, 0): 147, (1, 1): 105})\n"
     ]
    }
   ],
   "source": [
    "x_p,y1_p,y2_p = t.preprocess(ll[-250:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490/490 [==============================] - 2s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.857459967476981, 0.46938775510204084]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t._model.evaluate(x_p,y1_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = t._model.predict(x_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateScore(y_test, y_predict):\n",
    "    score = accuracy_score(np.argmax(y_test,axis=1), np.argmax(y_predict,axis=1))\n",
    "    c  = classification_report(np.argmax(y_test,axis=1), np.argmax(y_predict,axis=1))\n",
    "    print(score)\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(490, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(490, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 482, 1: 8})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(np.argmax(y_predict,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46938775510204084\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.97      0.64       238\n",
      "          1       0.00      0.00      0.00       252\n",
      "\n",
      "avg / total       0.23      0.47      0.31       490\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateScore(y1_p, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用BP做预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[i[1]] for i in ll])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 100)               9100      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 500)               50500     \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 2)                 1002      \n",
      "=================================================================\n",
      "Total params: 311,102\n",
      "Trainable params: 311,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 创建模型，输入784个神经元，输出10个神经元\n",
    "model = Sequential()\n",
    "model.add(Dense(units=100,input_dim=90,bias_initializer='one',activation='tanh'))\n",
    "model.add(Dense(units=500,bias_initializer='one',activation='tanh'))\n",
    "model.add(Dense(units=500,bias_initializer='one',activation='tanh'))\n",
    "model.add(Dense(units=2,activation='tanh'))\n",
    "\n",
    "# 定义优化器\n",
    "sgd = SGD(lr=0.2)\n",
    "\n",
    "# 定义优化器，loss function，训练过程中计算准确率\n",
    "model.compile(\n",
    "    optimizer = sgd,\n",
    "    loss = 'mse',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3200/3200 [==============================] - 0s 132us/step - loss: 0.5184 - acc: 0.8687\n",
      "Epoch 2/100\n",
      "3200/3200 [==============================] - 0s 49us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 3/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 4/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 5/100\n",
      "3200/3200 [==============================] - 0s 40us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 6/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 7/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 8/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 9/100\n",
      "3200/3200 [==============================] - 0s 49us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 10/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 11/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 12/100\n",
      "3200/3200 [==============================] - 0s 50us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 13/100\n",
      "3200/3200 [==============================] - 0s 52us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 14/100\n",
      "3200/3200 [==============================] - 0s 39us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 15/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 16/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 17/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 18/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 19/100\n",
      "3200/3200 [==============================] - 0s 51us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 20/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 21/100\n",
      "3200/3200 [==============================] - 0s 49us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 22/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 23/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 24/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 25/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 26/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 27/100\n",
      "3200/3200 [==============================] - 0s 49us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 28/100\n",
      "3200/3200 [==============================] - 0s 49us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 29/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 30/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 31/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 32/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 33/100\n",
      "3200/3200 [==============================] - 0s 44us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 34/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 35/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 36/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 37/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 38/100\n",
      "3200/3200 [==============================] - 0s 45us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 39/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 40/100\n",
      "3200/3200 [==============================] - 0s 50us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 41/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 42/100\n",
      "3200/3200 [==============================] - 0s 49us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 43/100\n",
      "3200/3200 [==============================] - 0s 51us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 44/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 45/100\n",
      "3200/3200 [==============================] - 0s 50us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 46/100\n",
      "3200/3200 [==============================] - 0s 49us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 47/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 48/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 49/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 50/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 51/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 52/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 53/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 54/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 55/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 56/100\n",
      "3200/3200 [==============================] - 0s 51us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 57/100\n",
      "3200/3200 [==============================] - 0s 49us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 58/100\n",
      "3200/3200 [==============================] - 0s 51us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 59/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 60/100\n",
      "3200/3200 [==============================] - 0s 57us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 61/100\n",
      "3200/3200 [==============================] - 0s 37us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 62/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 63/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 64/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 65/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 66/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 67/100\n",
      "3200/3200 [==============================] - 0s 49us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 68/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 69/100\n",
      "3200/3200 [==============================] - 0s 49us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 70/100\n",
      "3200/3200 [==============================] - 0s 45us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 71/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 72/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 73/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 74/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 75/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 76/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 77/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 78/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 79/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 80/100\n",
      "3200/3200 [==============================] - 0s 38us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 81/100\n",
      "3200/3200 [==============================] - 0s 52us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 82/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 84/100\n",
      "3200/3200 [==============================] - 0s 51us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 85/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 86/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 87/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 88/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 89/100\n",
      "3200/3200 [==============================] - 0s 43us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 90/100\n",
      "3200/3200 [==============================] - 0s 44us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 91/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 92/100\n",
      "3200/3200 [==============================] - 0s 49us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 93/100\n",
      "3200/3200 [==============================] - 0s 49us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 94/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 95/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 96/100\n",
      "3200/3200 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 97/100\n",
      "3200/3200 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 98/100\n",
      "3200/3200 [==============================] - 0s 55us/step - loss: 0.5000 - acc: 0.8687: 0s - loss: 0.5000 - acc: 0.870\n",
      "Epoch 99/100\n",
      "3200/3200 [==============================] - 0s 39us/step - loss: 0.5000 - acc: 0.8687\n",
      "Epoch 100/100\n",
      "3200/3200 [==============================] - 0s 48us/step - loss: 0.5000 - acc: 0.8687\n",
      "355/355 [==============================] - 0s 252us/step\n",
      "\n",
      "test loss 0.5\n",
      "accuracy 0.9661971830985916\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "model.fit(x_train,y_train,batch_size=128,epochs=100)\n",
    "\n",
    "# 评估模型\n",
    "loss,accuracy = model.evaluate(x_test,y_test)\n",
    "\n",
    "print('\\ntest loss',loss)\n",
    "print('accuracy',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense,Dropout,Convolution2D,MaxPooling2D,Flatten\n",
    "from keras.optimizers import Adam\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 10, 9, 64)         1664      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 5, 5, 126)         201726    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 126)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 256)         806656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 2,061,696\n",
      "Trainable params: 2,061,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 定义顺序模型\n",
    "model = Sequential()\n",
    "\n",
    "# 第一个卷积层\n",
    "# input_shape 输入平面\n",
    "# filters 卷积核/滤波器个数\n",
    "# kernel_size 卷积窗口大小\n",
    "# strides 步长\n",
    "# padding padding方式 same/valid\n",
    "# activation 激活函数\n",
    "model.add(Convolution2D(\n",
    "    input_shape = (10,9,1),\n",
    "    filters = 64,\n",
    "    kernel_size = 5,\n",
    "    strides = 1,\n",
    "    padding = 'same',\n",
    "    activation = 'relu'\n",
    "))\n",
    "# 第一个池化层\n",
    "model.add(MaxPooling2D(\n",
    "    pool_size = 2,\n",
    "    strides = 2,\n",
    "    padding = 'same',\n",
    "))\n",
    "# 第二个卷积层\n",
    "model.add(Convolution2D(126,5,strides=1,padding='same',activation = 'relu'))\n",
    "# 第二个池化层\n",
    "model.add(MaxPooling2D(2,2,'same'))\n",
    "\n",
    "# 第三个卷积层\n",
    "model.add(Convolution2D(256,5,strides=1,padding='same',activation = 'relu'))\n",
    "# 第三个池化层\n",
    "model.add(MaxPooling2D(2,2,'same'))\n",
    "\n",
    "# 把第三个池化层的输出扁平化为1维\n",
    "model.add(Flatten())\n",
    "# 第一个全连接层\n",
    "model.add(Dense(1024,activation = 'relu'))\n",
    "# Dropout\n",
    "model.add(Dropout(0.5))\n",
    "# 第二个全连接层\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "# 定义优化器\n",
    "adam = Adam(lr=1e-4)\n",
    "\n",
    "# 定义优化器，loss function，训练过程中计算准确率\n",
    "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/266\n",
      "3200/3200 [==============================] - 2s 739us/step - loss: 0.5231 - acc: 0.8419\n",
      "Epoch 2/266\n",
      "3200/3200 [==============================] - 2s 534us/step - loss: 0.4764 - acc: 0.8687\n",
      "Epoch 3/266\n",
      "3200/3200 [==============================] - 2s 588us/step - loss: 0.4567 - acc: 0.8687\n",
      "Epoch 4/266\n",
      "3200/3200 [==============================] - 2s 545us/step - loss: 0.4319 - acc: 0.8694\n",
      "Epoch 5/266\n",
      "3200/3200 [==============================] - 2s 539us/step - loss: 0.4317 - acc: 0.8663\n",
      "Epoch 6/266\n",
      "3200/3200 [==============================] - 2s 545us/step - loss: 0.4137 - acc: 0.8684\n",
      "Epoch 7/266\n",
      "3200/3200 [==============================] - 2s 529us/step - loss: 0.4128 - acc: 0.8675\n",
      "Epoch 8/266\n",
      "3200/3200 [==============================] - 2s 522us/step - loss: 0.4165 - acc: 0.8678\n",
      "Epoch 9/266\n",
      "3200/3200 [==============================] - 2s 543us/step - loss: 0.4117 - acc: 0.8669\n",
      "Epoch 10/266\n",
      "3200/3200 [==============================] - 2s 527us/step - loss: 0.4056 - acc: 0.8687\n",
      "Epoch 11/266\n",
      "3200/3200 [==============================] - 2s 539us/step - loss: 0.3955 - acc: 0.8684\n",
      "Epoch 12/266\n",
      "3200/3200 [==============================] - 2s 545us/step - loss: 0.3922 - acc: 0.8678\n",
      "Epoch 13/266\n",
      "3200/3200 [==============================] - 2s 538us/step - loss: 0.3978 - acc: 0.8687\n",
      "Epoch 14/266\n",
      "3200/3200 [==============================] - 2s 529us/step - loss: 0.3887 - acc: 0.8687\n",
      "Epoch 15/266\n",
      "3200/3200 [==============================] - 2s 535us/step - loss: 0.3894 - acc: 0.8678\n",
      "Epoch 16/266\n",
      "3200/3200 [==============================] - 2s 525us/step - loss: 0.3837 - acc: 0.8687\n",
      "Epoch 17/266\n",
      "3200/3200 [==============================] - 2s 532us/step - loss: 0.3848 - acc: 0.8678\n",
      "Epoch 18/266\n",
      "3200/3200 [==============================] - 2s 537us/step - loss: 0.3762 - acc: 0.8684\n",
      "Epoch 19/266\n",
      "3200/3200 [==============================] - 2s 530us/step - loss: 0.3815 - acc: 0.8678\n",
      "Epoch 20/266\n",
      "3200/3200 [==============================] - 2s 544us/step - loss: 0.3796 - acc: 0.8703\n",
      "Epoch 21/266\n",
      "3200/3200 [==============================] - 2s 540us/step - loss: 0.3734 - acc: 0.8678\n",
      "Epoch 22/266\n",
      "3200/3200 [==============================] - 2s 547us/step - loss: 0.3757 - acc: 0.8694\n",
      "Epoch 23/266\n",
      "3200/3200 [==============================] - 2s 541us/step - loss: 0.3713 - acc: 0.8697\n",
      "Epoch 24/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.3705 - acc: 0.8703\n",
      "Epoch 25/266\n",
      "3200/3200 [==============================] - 2s 534us/step - loss: 0.3679 - acc: 0.8700\n",
      "Epoch 26/266\n",
      "3200/3200 [==============================] - 2s 555us/step - loss: 0.3664 - acc: 0.8691\n",
      "Epoch 27/266\n",
      "3200/3200 [==============================] - 2s 526us/step - loss: 0.3700 - acc: 0.8706\n",
      "Epoch 28/266\n",
      "3200/3200 [==============================] - 2s 557us/step - loss: 0.3674 - acc: 0.8712\n",
      "Epoch 29/266\n",
      "3200/3200 [==============================] - 2s 544us/step - loss: 0.3649 - acc: 0.8703\n",
      "Epoch 30/266\n",
      "3200/3200 [==============================] - 2s 517us/step - loss: 0.3616 - acc: 0.8716\n",
      "Epoch 31/266\n",
      "3200/3200 [==============================] - 2s 546us/step - loss: 0.3580 - acc: 0.8697\n",
      "Epoch 32/266\n",
      "3200/3200 [==============================] - 2s 540us/step - loss: 0.3485 - acc: 0.8719\n",
      "Epoch 33/266\n",
      "3200/3200 [==============================] - 2s 521us/step - loss: 0.3503 - acc: 0.8719\n",
      "Epoch 34/266\n",
      "3200/3200 [==============================] - 2s 555us/step - loss: 0.3474 - acc: 0.8731\n",
      "Epoch 35/266\n",
      "3200/3200 [==============================] - 2s 528us/step - loss: 0.3410 - acc: 0.8747\n",
      "Epoch 36/266\n",
      "3200/3200 [==============================] - 2s 518us/step - loss: 0.3347 - acc: 0.8756\n",
      "Epoch 37/266\n",
      "3200/3200 [==============================] - 2s 536us/step - loss: 0.3342 - acc: 0.8775\n",
      "Epoch 38/266\n",
      "3200/3200 [==============================] - 2s 532us/step - loss: 0.3268 - acc: 0.8797\n",
      "Epoch 39/266\n",
      "3200/3200 [==============================] - 2s 522us/step - loss: 0.3279 - acc: 0.8772\n",
      "Epoch 40/266\n",
      "3200/3200 [==============================] - 2s 529us/step - loss: 0.3300 - acc: 0.8778\n",
      "Epoch 41/266\n",
      "3200/3200 [==============================] - 2s 522us/step - loss: 0.3151 - acc: 0.8819\n",
      "Epoch 42/266\n",
      "3200/3200 [==============================] - 2s 524us/step - loss: 0.3125 - acc: 0.8822\n",
      "Epoch 43/266\n",
      "3200/3200 [==============================] - 2s 529us/step - loss: 0.3055 - acc: 0.8850\n",
      "Epoch 44/266\n",
      "3200/3200 [==============================] - 2s 538us/step - loss: 0.3042 - acc: 0.8866\n",
      "Epoch 45/266\n",
      "3200/3200 [==============================] - 2s 548us/step - loss: 0.2938 - acc: 0.8856\n",
      "Epoch 46/266\n",
      "3200/3200 [==============================] - 2s 536us/step - loss: 0.2926 - acc: 0.8891\n",
      "Epoch 47/266\n",
      "3200/3200 [==============================] - 2s 518us/step - loss: 0.2883 - acc: 0.8900\n",
      "Epoch 48/266\n",
      "3200/3200 [==============================] - 2s 544us/step - loss: 0.2813 - acc: 0.8922\n",
      "Epoch 49/266\n",
      "3200/3200 [==============================] - 2s 548us/step - loss: 0.2893 - acc: 0.8894\n",
      "Epoch 50/266\n",
      "3200/3200 [==============================] - 2s 542us/step - loss: 0.2834 - acc: 0.8903\n",
      "Epoch 51/266\n",
      "3200/3200 [==============================] - 2s 515us/step - loss: 0.2791 - acc: 0.8903\n",
      "Epoch 52/266\n",
      "3200/3200 [==============================] - 2s 513us/step - loss: 0.2753 - acc: 0.8959\n",
      "Epoch 53/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.2718 - acc: 0.8909\n",
      "Epoch 54/266\n",
      "3200/3200 [==============================] - 2s 528us/step - loss: 0.2486 - acc: 0.9044\n",
      "Epoch 55/266\n",
      "3200/3200 [==============================] - 2s 516us/step - loss: 0.2490 - acc: 0.9031\n",
      "Epoch 56/266\n",
      "3200/3200 [==============================] - 2s 547us/step - loss: 0.2425 - acc: 0.9103\n",
      "Epoch 57/266\n",
      "3200/3200 [==============================] - 2s 549us/step - loss: 0.2475 - acc: 0.9037\n",
      "Epoch 58/266\n",
      "3200/3200 [==============================] - 2s 525us/step - loss: 0.2367 - acc: 0.9081\n",
      "Epoch 59/266\n",
      "3200/3200 [==============================] - 2s 524us/step - loss: 0.2318 - acc: 0.9138\n",
      "Epoch 60/266\n",
      "3200/3200 [==============================] - 2s 524us/step - loss: 0.2296 - acc: 0.9097\n",
      "Epoch 61/266\n",
      "3200/3200 [==============================] - 2s 540us/step - loss: 0.2170 - acc: 0.9169\n",
      "Epoch 62/266\n",
      "3200/3200 [==============================] - 2s 552us/step - loss: 0.2070 - acc: 0.9228\n",
      "Epoch 63/266\n",
      "3200/3200 [==============================] - 2s 534us/step - loss: 0.2139 - acc: 0.9153\n",
      "Epoch 64/266\n",
      "3200/3200 [==============================] - 2s 536us/step - loss: 0.2085 - acc: 0.9209\n",
      "Epoch 65/266\n",
      "3200/3200 [==============================] - 2s 545us/step - loss: 0.2185 - acc: 0.9175\n",
      "Epoch 66/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.2062 - acc: 0.9166\n",
      "Epoch 67/266\n",
      "3200/3200 [==============================] - 2s 536us/step - loss: 0.1908 - acc: 0.9259\n",
      "Epoch 68/266\n",
      "3200/3200 [==============================] - 2s 529us/step - loss: 0.1796 - acc: 0.9303\n",
      "Epoch 69/266\n",
      "3200/3200 [==============================] - 2s 552us/step - loss: 0.1725 - acc: 0.9328\n",
      "Epoch 70/266\n",
      "3200/3200 [==============================] - 2s 547us/step - loss: 0.1907 - acc: 0.9241\n",
      "Epoch 71/266\n",
      "3200/3200 [==============================] - 2s 525us/step - loss: 0.2246 - acc: 0.9119\n",
      "Epoch 72/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.1753 - acc: 0.9331\n",
      "Epoch 73/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.1616 - acc: 0.9384\n",
      "Epoch 74/266\n",
      "3200/3200 [==============================] - 2s 513us/step - loss: 0.1580 - acc: 0.9428\n",
      "Epoch 75/266\n",
      "3200/3200 [==============================] - 2s 536us/step - loss: 0.1553 - acc: 0.9413\n",
      "Epoch 76/266\n",
      "3200/3200 [==============================] - 2s 539us/step - loss: 0.1511 - acc: 0.9434\n",
      "Epoch 77/266\n",
      "3200/3200 [==============================] - 2s 532us/step - loss: 0.1452 - acc: 0.9500\n",
      "Epoch 78/266\n",
      "3200/3200 [==============================] - 2s 542us/step - loss: 0.1487 - acc: 0.9409\n",
      "Epoch 79/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.1496 - acc: 0.9441\n",
      "Epoch 80/266\n",
      "3200/3200 [==============================] - 2s 526us/step - loss: 0.1361 - acc: 0.9491\n",
      "Epoch 81/266\n",
      "3200/3200 [==============================] - 2s 510us/step - loss: 0.1268 - acc: 0.9534\n",
      "Epoch 82/266\n",
      "3200/3200 [==============================] - 2s 532us/step - loss: 0.1221 - acc: 0.9550\n",
      "Epoch 83/266\n",
      "3200/3200 [==============================] - 2s 516us/step - loss: 0.1384 - acc: 0.9484\n",
      "Epoch 84/266\n",
      "3200/3200 [==============================] - 2s 535us/step - loss: 0.1285 - acc: 0.9528\n",
      "Epoch 85/266\n",
      "3200/3200 [==============================] - 2s 526us/step - loss: 0.1586 - acc: 0.9391\n",
      "Epoch 86/266\n",
      "3200/3200 [==============================] - 2s 533us/step - loss: 0.1285 - acc: 0.9522\n",
      "Epoch 87/266\n",
      "3200/3200 [==============================] - 2s 534us/step - loss: 0.1350 - acc: 0.9441\n",
      "Epoch 88/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.1317 - acc: 0.9528\n",
      "Epoch 89/266\n",
      "3200/3200 [==============================] - 2s 534us/step - loss: 0.1188 - acc: 0.9569\n",
      "Epoch 90/266\n",
      "3200/3200 [==============================] - 2s 524us/step - loss: 0.1049 - acc: 0.9631\n",
      "Epoch 91/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.1142 - acc: 0.9581\n",
      "Epoch 92/266\n",
      "3200/3200 [==============================] - 2s 521us/step - loss: 0.0964 - acc: 0.9672\n",
      "Epoch 93/266\n",
      "3200/3200 [==============================] - 2s 538us/step - loss: 0.0992 - acc: 0.9706\n",
      "Epoch 94/266\n",
      "3200/3200 [==============================] - 2s 532us/step - loss: 0.1057 - acc: 0.9622\n",
      "Epoch 95/266\n",
      "3200/3200 [==============================] - 2s 532us/step - loss: 0.1278 - acc: 0.9469\n",
      "Epoch 96/266\n",
      "3200/3200 [==============================] - 2s 554us/step - loss: 0.1040 - acc: 0.9631\n",
      "Epoch 97/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.0952 - acc: 0.9669\n",
      "Epoch 98/266\n",
      "3200/3200 [==============================] - 2s 547us/step - loss: 0.0929 - acc: 0.9666\n",
      "Epoch 99/266\n",
      "3200/3200 [==============================] - 2s 539us/step - loss: 0.0833 - acc: 0.9738\n",
      "Epoch 100/266\n",
      "3200/3200 [==============================] - 2s 568us/step - loss: 0.0742 - acc: 0.9791\n",
      "Epoch 101/266\n",
      "3200/3200 [==============================] - 2s 518us/step - loss: 0.0722 - acc: 0.9788\n",
      "Epoch 102/266\n",
      "3200/3200 [==============================] - 2s 547us/step - loss: 0.0692 - acc: 0.9794\n",
      "Epoch 103/266\n",
      "3200/3200 [==============================] - 2s 548us/step - loss: 0.0710 - acc: 0.9784\n",
      "Epoch 104/266\n",
      "3200/3200 [==============================] - 2s 515us/step - loss: 0.0638 - acc: 0.9819\n",
      "Epoch 105/266\n",
      "3200/3200 [==============================] - 2s 527us/step - loss: 0.0627 - acc: 0.9812\n",
      "Epoch 106/266\n",
      "3200/3200 [==============================] - 2s 532us/step - loss: 0.0603 - acc: 0.9828\n",
      "Epoch 107/266\n",
      "3200/3200 [==============================] - 2s 535us/step - loss: 0.0612 - acc: 0.9850\n",
      "Epoch 108/266\n",
      "3200/3200 [==============================] - 2s 555us/step - loss: 0.0631 - acc: 0.9816\n",
      "Epoch 109/266\n",
      "3200/3200 [==============================] - 2s 527us/step - loss: 0.0557 - acc: 0.9850\n",
      "Epoch 110/266\n",
      "3200/3200 [==============================] - 2s 519us/step - loss: 0.0559 - acc: 0.9856\n",
      "Epoch 111/266\n",
      "3200/3200 [==============================] - 2s 549us/step - loss: 0.0528 - acc: 0.9862\n",
      "Epoch 112/266\n",
      "3200/3200 [==============================] - 2s 538us/step - loss: 0.0557 - acc: 0.9847\n",
      "Epoch 113/266\n",
      "3200/3200 [==============================] - 2s 535us/step - loss: 0.0510 - acc: 0.9884\n",
      "Epoch 114/266\n",
      "3200/3200 [==============================] - 2s 558us/step - loss: 0.0506 - acc: 0.9862\n",
      "Epoch 115/266\n",
      "3200/3200 [==============================] - 2s 543us/step - loss: 0.0466 - acc: 0.9881\n",
      "Epoch 116/266\n",
      "3200/3200 [==============================] - 2s 545us/step - loss: 0.0469 - acc: 0.9881\n",
      "Epoch 117/266\n",
      "3200/3200 [==============================] - 2s 523us/step - loss: 0.0471 - acc: 0.9856\n",
      "Epoch 118/266\n",
      "3200/3200 [==============================] - 2s 527us/step - loss: 0.0776 - acc: 0.9766\n",
      "Epoch 119/266\n",
      "3200/3200 [==============================] - 2s 548us/step - loss: 0.6634 - acc: 0.8369\n",
      "Epoch 120/266\n",
      "3200/3200 [==============================] - 2s 528us/step - loss: 0.3135 - acc: 0.8766\n",
      "Epoch 121/266\n",
      "3200/3200 [==============================] - 2s 527us/step - loss: 0.2586 - acc: 0.8912\n",
      "Epoch 122/266\n",
      "3200/3200 [==============================] - 2s 522us/step - loss: 0.1858 - acc: 0.9213\n",
      "Epoch 123/266\n",
      "3200/3200 [==============================] - 2s 530us/step - loss: 0.1340 - acc: 0.9553\n",
      "Epoch 124/266\n",
      "3200/3200 [==============================] - 2s 539us/step - loss: 0.1134 - acc: 0.9647\n",
      "Epoch 125/266\n",
      "3200/3200 [==============================] - 2s 516us/step - loss: 0.0995 - acc: 0.9622\n",
      "Epoch 126/266\n",
      "3200/3200 [==============================] - 2s 547us/step - loss: 0.1399 - acc: 0.9484\n",
      "Epoch 127/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.1339 - acc: 0.9469\n",
      "Epoch 128/266\n",
      "3200/3200 [==============================] - 2s 535us/step - loss: 0.0894 - acc: 0.9697\n",
      "Epoch 129/266\n",
      "3200/3200 [==============================] - 2s 532us/step - loss: 0.0690 - acc: 0.9812\n",
      "Epoch 130/266\n",
      "3200/3200 [==============================] - 2s 530us/step - loss: 0.0616 - acc: 0.9844\n",
      "Epoch 131/266\n",
      "3200/3200 [==============================] - 2s 535us/step - loss: 0.0570 - acc: 0.9819\n",
      "Epoch 132/266\n",
      "3200/3200 [==============================] - 2s 522us/step - loss: 0.0514 - acc: 0.9884\n",
      "Epoch 133/266\n",
      "3200/3200 [==============================] - 2s 516us/step - loss: 0.0488 - acc: 0.9872\n",
      "Epoch 134/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.0443 - acc: 0.9900\n",
      "Epoch 135/266\n",
      "3200/3200 [==============================] - 2s 525us/step - loss: 0.0431 - acc: 0.9916\n",
      "Epoch 136/266\n",
      "3200/3200 [==============================] - 2s 525us/step - loss: 0.0406 - acc: 0.9916\n",
      "Epoch 137/266\n",
      "3200/3200 [==============================] - 2s 521us/step - loss: 0.0395 - acc: 0.9916\n",
      "Epoch 138/266\n",
      "3200/3200 [==============================] - 2s 552us/step - loss: 0.0390 - acc: 0.9919\n",
      "Epoch 139/266\n",
      "3200/3200 [==============================] - 2s 534us/step - loss: 0.0377 - acc: 0.9928\n",
      "Epoch 140/266\n",
      "3200/3200 [==============================] - 2s 518us/step - loss: 0.0374 - acc: 0.9913\n",
      "Epoch 141/266\n",
      "3200/3200 [==============================] - 2s 526us/step - loss: 0.0351 - acc: 0.9934\n",
      "Epoch 142/266\n",
      "3200/3200 [==============================] - 2s 539us/step - loss: 0.0327 - acc: 0.9934\n",
      "Epoch 143/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.0369 - acc: 0.9903\n",
      "Epoch 144/266\n",
      "3200/3200 [==============================] - 2s 527us/step - loss: 0.0303 - acc: 0.9947\n",
      "Epoch 145/266\n",
      "3200/3200 [==============================] - 2s 524us/step - loss: 0.0298 - acc: 0.9944\n",
      "Epoch 146/266\n",
      "3200/3200 [==============================] - 2s 522us/step - loss: 0.0296 - acc: 0.9944\n",
      "Epoch 147/266\n",
      "3200/3200 [==============================] - 2s 536us/step - loss: 0.0273 - acc: 0.9953\n",
      "Epoch 148/266\n",
      "3200/3200 [==============================] - 2s 535us/step - loss: 0.0262 - acc: 0.9959\n",
      "Epoch 149/266\n",
      "3200/3200 [==============================] - 2s 534us/step - loss: 0.0257 - acc: 0.9959\n",
      "Epoch 150/266\n",
      "3200/3200 [==============================] - 2s 523us/step - loss: 0.0273 - acc: 0.9956\n",
      "Epoch 151/266\n",
      "3200/3200 [==============================] - 2s 547us/step - loss: 0.0245 - acc: 0.9959\n",
      "Epoch 152/266\n",
      "3200/3200 [==============================] - 2s 530us/step - loss: 0.0255 - acc: 0.9950\n",
      "Epoch 153/266\n",
      "3200/3200 [==============================] - 2s 523us/step - loss: 0.0262 - acc: 0.9941\n",
      "Epoch 154/266\n",
      "3200/3200 [==============================] - 2s 536us/step - loss: 0.0240 - acc: 0.9944\n",
      "Epoch 155/266\n",
      "3200/3200 [==============================] - 2s 556us/step - loss: 0.0238 - acc: 0.9962\n",
      "Epoch 156/266\n",
      "3200/3200 [==============================] - 2s 543us/step - loss: 0.0232 - acc: 0.9959\n",
      "Epoch 157/266\n",
      "3200/3200 [==============================] - 2s 545us/step - loss: 0.0218 - acc: 0.9975\n",
      "Epoch 158/266\n",
      "3200/3200 [==============================] - 2s 546us/step - loss: 0.0207 - acc: 0.9969\n",
      "Epoch 159/266\n",
      "3200/3200 [==============================] - 2s 513us/step - loss: 0.0199 - acc: 0.9972\n",
      "Epoch 160/266\n",
      "3200/3200 [==============================] - 2s 545us/step - loss: 0.0193 - acc: 0.9972\n",
      "Epoch 161/266\n",
      "3200/3200 [==============================] - 2s 522us/step - loss: 0.0198 - acc: 0.9969\n",
      "Epoch 162/266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - 2s 544us/step - loss: 0.0197 - acc: 0.9969\n",
      "Epoch 163/266\n",
      "3200/3200 [==============================] - 2s 544us/step - loss: 0.0221 - acc: 0.9966\n",
      "Epoch 164/266\n",
      "3200/3200 [==============================] - 2s 540us/step - loss: 0.0205 - acc: 0.9966\n",
      "Epoch 165/266\n",
      "3200/3200 [==============================] - 2s 544us/step - loss: 0.0184 - acc: 0.9972\n",
      "Epoch 166/266\n",
      "3200/3200 [==============================] - 2s 547us/step - loss: 0.0168 - acc: 0.9969\n",
      "Epoch 167/266\n",
      "3200/3200 [==============================] - 2s 527us/step - loss: 0.0171 - acc: 0.9975\n",
      "Epoch 168/266\n",
      "3200/3200 [==============================] - 2s 514us/step - loss: 0.0161 - acc: 0.9981\n",
      "Epoch 169/266\n",
      "3200/3200 [==============================] - 2s 533us/step - loss: 0.0163 - acc: 0.9969\n",
      "Epoch 170/266\n",
      "3200/3200 [==============================] - 2s 555us/step - loss: 0.0162 - acc: 0.9975\n",
      "Epoch 171/266\n",
      "3200/3200 [==============================] - 2s 525us/step - loss: 0.0161 - acc: 0.9978\n",
      "Epoch 172/266\n",
      "3200/3200 [==============================] - 2s 544us/step - loss: 0.0159 - acc: 0.9981\n",
      "Epoch 173/266\n",
      "3200/3200 [==============================] - 2s 534us/step - loss: 0.0148 - acc: 0.9981\n",
      "Epoch 174/266\n",
      "3200/3200 [==============================] - 2s 530us/step - loss: 0.0148 - acc: 0.9975\n",
      "Epoch 175/266\n",
      "3200/3200 [==============================] - 2s 530us/step - loss: 0.0139 - acc: 0.9972\n",
      "Epoch 176/266\n",
      "3200/3200 [==============================] - 2s 547us/step - loss: 0.0145 - acc: 0.9988\n",
      "Epoch 177/266\n",
      "3200/3200 [==============================] - 2s 536us/step - loss: 0.0136 - acc: 0.9972\n",
      "Epoch 178/266\n",
      "3200/3200 [==============================] - 2s 526us/step - loss: 0.0132 - acc: 0.9984\n",
      "Epoch 179/266\n",
      "3200/3200 [==============================] - 2s 550us/step - loss: 0.0128 - acc: 0.9984\n",
      "Epoch 180/266\n",
      "3200/3200 [==============================] - 2s 522us/step - loss: 0.0117 - acc: 0.9981\n",
      "Epoch 181/266\n",
      "3200/3200 [==============================] - 2s 490us/step - loss: 0.0126 - acc: 0.9984\n",
      "Epoch 182/266\n",
      "3200/3200 [==============================] - 2s 519us/step - loss: 0.0119 - acc: 0.9988\n",
      "Epoch 183/266\n",
      "3200/3200 [==============================] - 2s 541us/step - loss: 0.0119 - acc: 0.9978\n",
      "Epoch 184/266\n",
      "3200/3200 [==============================] - 2s 524us/step - loss: 0.0118 - acc: 0.9984\n",
      "Epoch 185/266\n",
      "3200/3200 [==============================] - 2s 527us/step - loss: 0.0116 - acc: 0.9994\n",
      "Epoch 186/266\n",
      "3200/3200 [==============================] - 2s 525us/step - loss: 0.0104 - acc: 0.9984\n",
      "Epoch 187/266\n",
      "3200/3200 [==============================] - 2s 538us/step - loss: 0.0105 - acc: 0.9988\n",
      "Epoch 188/266\n",
      "3200/3200 [==============================] - 2s 520us/step - loss: 0.0097 - acc: 0.9994\n",
      "Epoch 189/266\n",
      "3200/3200 [==============================] - 2s 509us/step - loss: 0.0096 - acc: 0.9984\n",
      "Epoch 190/266\n",
      "3200/3200 [==============================] - 2s 544us/step - loss: 0.0091 - acc: 0.9994\n",
      "Epoch 191/266\n",
      "3200/3200 [==============================] - 2s 541us/step - loss: 0.0090 - acc: 0.9991\n",
      "Epoch 192/266\n",
      "3200/3200 [==============================] - 2s 549us/step - loss: 0.0094 - acc: 0.9991\n",
      "Epoch 193/266\n",
      "3200/3200 [==============================] - 2s 555us/step - loss: 0.0087 - acc: 0.9991\n",
      "Epoch 194/266\n",
      "3200/3200 [==============================] - 2s 540us/step - loss: 0.0094 - acc: 0.9988\n",
      "Epoch 195/266\n",
      "3200/3200 [==============================] - 2s 549us/step - loss: 0.0091 - acc: 0.9991\n",
      "Epoch 196/266\n",
      "3200/3200 [==============================] - 2s 529us/step - loss: 0.0079 - acc: 0.9997\n",
      "Epoch 197/266\n",
      "3200/3200 [==============================] - 2s 550us/step - loss: 0.0087 - acc: 0.9994\n",
      "Epoch 198/266\n",
      "3200/3200 [==============================] - 2s 537us/step - loss: 0.0083 - acc: 0.9991\n",
      "Epoch 199/266\n",
      "3200/3200 [==============================] - 2s 538us/step - loss: 0.0073 - acc: 0.9994\n",
      "Epoch 200/266\n",
      "3200/3200 [==============================] - 2s 550us/step - loss: 0.0079 - acc: 0.9991\n",
      "Epoch 201/266\n",
      "3200/3200 [==============================] - 2s 527us/step - loss: 0.0069 - acc: 0.9994\n",
      "Epoch 202/266\n",
      "3200/3200 [==============================] - 2s 547us/step - loss: 0.0068 - acc: 0.9994\n",
      "Epoch 203/266\n",
      "3200/3200 [==============================] - 2s 531us/step - loss: 0.0060 - acc: 0.9997\n",
      "Epoch 204/266\n",
      "3200/3200 [==============================] - 2s 539us/step - loss: 0.0061 - acc: 0.9994\n",
      "Epoch 205/266\n",
      "3200/3200 [==============================] - 2s 547us/step - loss: 0.0063 - acc: 0.9994\n",
      "Epoch 206/266\n",
      "3200/3200 [==============================] - 2s 541us/step - loss: 0.0071 - acc: 0.9991\n",
      "Epoch 207/266\n",
      "3200/3200 [==============================] - 2s 514us/step - loss: 0.0098 - acc: 0.9988\n",
      "Epoch 208/266\n",
      "3200/3200 [==============================] - 2s 533us/step - loss: 0.0136 - acc: 0.9966\n",
      "Epoch 209/266\n",
      "3200/3200 [==============================] - 2s 527us/step - loss: 0.0686 - acc: 0.9834\n",
      "Epoch 210/266\n",
      "3200/3200 [==============================] - 2s 544us/step - loss: 1.6539 - acc: 0.8603\n",
      "Epoch 211/266\n",
      "3200/3200 [==============================] - 2s 528us/step - loss: 1.3773 - acc: 0.8141\n",
      "Epoch 212/266\n",
      "3200/3200 [==============================] - 2s 546us/step - loss: 0.4386 - acc: 0.8663\n",
      "Epoch 213/266\n",
      "3200/3200 [==============================] - 2s 550us/step - loss: 0.3825 - acc: 0.8684\n",
      "Epoch 214/266\n",
      "3200/3200 [==============================] - 2s 526us/step - loss: 0.3691 - acc: 0.8659\n",
      "Epoch 215/266\n",
      "3200/3200 [==============================] - 2s 526us/step - loss: 0.3598 - acc: 0.8691\n",
      "Epoch 216/266\n",
      "3200/3200 [==============================] - 2s 535us/step - loss: 0.3434 - acc: 0.8716\n",
      "Epoch 217/266\n",
      "3200/3200 [==============================] - 2s 536us/step - loss: 0.3305 - acc: 0.8706\n",
      "Epoch 218/266\n",
      "3200/3200 [==============================] - 2s 557us/step - loss: 0.3044 - acc: 0.8756\n",
      "Epoch 219/266\n",
      "3200/3200 [==============================] - 2s 527us/step - loss: 0.2909 - acc: 0.8791\n",
      "Epoch 220/266\n",
      "3200/3200 [==============================] - 2s 525us/step - loss: 0.2781 - acc: 0.8844\n",
      "Epoch 221/266\n",
      "3200/3200 [==============================] - 2s 510us/step - loss: 0.2246 - acc: 0.8972\n",
      "Epoch 222/266\n",
      "3200/3200 [==============================] - 2s 525us/step - loss: 0.2048 - acc: 0.9091\n",
      "Epoch 223/266\n",
      "3200/3200 [==============================] - 2s 519us/step - loss: 0.3149 - acc: 0.8916\n",
      "Epoch 224/266\n",
      "3200/3200 [==============================] - 2s 519us/step - loss: 0.2417 - acc: 0.8944\n",
      "Epoch 225/266\n",
      "3200/3200 [==============================] - 2s 559us/step - loss: 0.1688 - acc: 0.9266\n",
      "Epoch 226/266\n",
      "3200/3200 [==============================] - 2s 507us/step - loss: 0.1256 - acc: 0.9563\n",
      "Epoch 227/266\n",
      "3200/3200 [==============================] - 2s 542us/step - loss: 0.0897 - acc: 0.9728\n",
      "Epoch 228/266\n",
      "3200/3200 [==============================] - 2s 520us/step - loss: 0.1315 - acc: 0.9528\n",
      "Epoch 229/266\n",
      "3200/3200 [==============================] - 2s 541us/step - loss: 0.1745 - acc: 0.9400\n",
      "Epoch 230/266\n",
      "3200/3200 [==============================] - 2s 524us/step - loss: 0.0964 - acc: 0.9681\n",
      "Epoch 231/266\n",
      "3200/3200 [==============================] - 2s 538us/step - loss: 0.0719 - acc: 0.9791\n",
      "Epoch 232/266\n",
      "3200/3200 [==============================] - 2s 518us/step - loss: 0.0561 - acc: 0.9850\n",
      "Epoch 233/266\n",
      "3200/3200 [==============================] - 2s 546us/step - loss: 0.0563 - acc: 0.9828\n",
      "Epoch 234/266\n",
      "3200/3200 [==============================] - 2s 519us/step - loss: 0.0394 - acc: 0.9903\n",
      "Epoch 235/266\n",
      "3200/3200 [==============================] - 2s 545us/step - loss: 0.0354 - acc: 0.9928\n",
      "Epoch 236/266\n",
      "3200/3200 [==============================] - 2s 528us/step - loss: 0.0324 - acc: 0.9925\n",
      "Epoch 237/266\n",
      "3200/3200 [==============================] - 2s 544us/step - loss: 0.0296 - acc: 0.9953\n",
      "Epoch 238/266\n",
      "3200/3200 [==============================] - 2s 532us/step - loss: 0.0249 - acc: 0.9962\n",
      "Epoch 239/266\n",
      "3200/3200 [==============================] - 2s 539us/step - loss: 0.0343 - acc: 0.9906\n",
      "Epoch 240/266\n",
      "3200/3200 [==============================] - 2s 538us/step - loss: 0.0364 - acc: 0.9909\n",
      "Epoch 241/266\n",
      "3200/3200 [==============================] - 2s 524us/step - loss: 0.0534 - acc: 0.9825\n",
      "Epoch 242/266\n",
      "3200/3200 [==============================] - 2s 527us/step - loss: 0.1391 - acc: 0.9519\n",
      "Epoch 243/266\n",
      "3200/3200 [==============================] - 2s 539us/step - loss: 0.1439 - acc: 0.9409\n",
      "Epoch 244/266\n",
      "3200/3200 [==============================] - 2s 514us/step - loss: 0.0880 - acc: 0.9731\n",
      "Epoch 245/266\n",
      "3200/3200 [==============================] - 2s 530us/step - loss: 0.0376 - acc: 0.9875\n",
      "Epoch 246/266\n",
      "3200/3200 [==============================] - 2s 524us/step - loss: 0.0225 - acc: 0.9969\n",
      "Epoch 247/266\n",
      "3200/3200 [==============================] - 2s 544us/step - loss: 0.0236 - acc: 0.9953\n",
      "Epoch 248/266\n",
      "3200/3200 [==============================] - 2s 535us/step - loss: 0.0189 - acc: 0.9972\n",
      "Epoch 249/266\n",
      "3200/3200 [==============================] - 2s 536us/step - loss: 0.0133 - acc: 0.9991\n",
      "Epoch 250/266\n",
      "3200/3200 [==============================] - 2s 544us/step - loss: 0.0105 - acc: 0.9997\n",
      "Epoch 251/266\n",
      "3200/3200 [==============================] - 2s 526us/step - loss: 0.0102 - acc: 0.9991\n",
      "Epoch 252/266\n",
      "3200/3200 [==============================] - 2s 533us/step - loss: 0.0107 - acc: 0.9988\n",
      "Epoch 253/266\n",
      "3200/3200 [==============================] - 2s 560us/step - loss: 0.0101 - acc: 0.9997\n",
      "Epoch 254/266\n",
      "3200/3200 [==============================] - 2s 539us/step - loss: 0.0103 - acc: 0.9994\n",
      "Epoch 255/266\n",
      "3200/3200 [==============================] - 2s 547us/step - loss: 0.0088 - acc: 0.9997\n",
      "Epoch 256/266\n",
      "3200/3200 [==============================] - 2s 539us/step - loss: 0.0078 - acc: 0.9997\n",
      "Epoch 257/266\n",
      "3200/3200 [==============================] - 2s 538us/step - loss: 0.0079 - acc: 0.9991\n",
      "Epoch 258/266\n",
      "3200/3200 [==============================] - 2s 535us/step - loss: 0.0072 - acc: 0.9997\n",
      "Epoch 259/266\n",
      "3200/3200 [==============================] - 2s 529us/step - loss: 0.0071 - acc: 0.9997\n",
      "Epoch 260/266\n",
      "3200/3200 [==============================] - 2s 542us/step - loss: 0.0062 - acc: 0.9997\n",
      "Epoch 261/266\n",
      "3200/3200 [==============================] - 2s 512us/step - loss: 0.0059 - acc: 0.9997\n",
      "Epoch 262/266\n",
      "3200/3200 [==============================] - 2s 532us/step - loss: 0.0061 - acc: 0.9997\n",
      "Epoch 263/266\n",
      "3200/3200 [==============================] - 2s 529us/step - loss: 0.0058 - acc: 0.9997\n",
      "Epoch 264/266\n",
      "3200/3200 [==============================] - 2s 551us/step - loss: 0.0055 - acc: 0.9997\n",
      "Epoch 265/266\n",
      "3200/3200 [==============================] - 2s 546us/step - loss: 0.0050 - acc: 1.0000\n",
      "Epoch 266/266\n",
      "3200/3200 [==============================] - 2s 539us/step - loss: 0.0050 - acc: 0.9997\n",
      "@ Total Time Spent: 456.21 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# 训练模型\n",
    "model.fit(x_train.reshape((x_train.shape[0],10,9,1)),y_train,batch_size=256,epochs=266)\n",
    "print('@ Total Time Spent: %.2f seconds' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict(x_test.reshape((x_test.shape[0],10,9,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9998593e-01, 1.4124693e-05],\n",
       "       [1.0000000e+00, 4.3917570e-10],\n",
       "       [1.0000000e+00, 8.7278396e-10],\n",
       "       [9.9999309e-01, 6.8770369e-06],\n",
       "       [9.9999988e-01, 7.6666971e-08],\n",
       "       [1.0000000e+00, 5.8134241e-08],\n",
       "       [9.9999738e-01, 2.6630350e-06],\n",
       "       [9.9998093e-01, 1.9049217e-05],\n",
       "       [1.0000000e+00, 2.8458558e-08],\n",
       "       [9.9926537e-01, 7.3465967e-04]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/355 [==============================] - 0s 386us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.582860738884114, 0.9492957746478873]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test.reshape((x_test.shape[0],10,9,1)),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用SimpleRNN预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 50)                2700      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 2,802\n",
      "Trainable params: 2,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 创建模型\n",
    "model = Sequential()\n",
    "\n",
    "# 循环神经网络\n",
    "model.add(SimpleRNN(\n",
    "    units = 50, # 输出\n",
    "    input_shape = (30,3), #输入\n",
    "))\n",
    "\n",
    "# 输出层\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "# 定义优化器\n",
    "adam = Adam(lr=1e-4)\n",
    "\n",
    "# 定义优化器，loss function，训练过程中计算准确率\n",
    "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 1s 263us/step - loss: 0.4818 - acc: 0.8672\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.4001 - acc: 0.8688\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.3906 - acc: 0.8688\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.3893 - acc: 0.8688\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.3888 - acc: 0.8688\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.3884 - acc: 0.8688\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.3878 - acc: 0.8688\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 1s 183us/step - loss: 0.3874 - acc: 0.8688\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.3869 - acc: 0.8688\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.3864 - acc: 0.8688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd140392160>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "model.fit(x[:3200],y_train[:3200],batch_size=64,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "3200/3200 [==============================] - 0s 150us/step - loss: 0.3267 - acc: 0.8759\n",
      "Epoch 2/200\n",
      "3200/3200 [==============================] - 0s 155us/step - loss: 0.3344 - acc: 0.8709\n",
      "Epoch 3/200\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.3281 - acc: 0.8744\n",
      "Epoch 4/200\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.3265 - acc: 0.8744\n",
      "Epoch 5/200\n",
      "3200/3200 [==============================] - 1s 161us/step - loss: 0.3263 - acc: 0.8747\n",
      "Epoch 6/200\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.3314 - acc: 0.8728\n",
      "Epoch 7/200\n",
      "3200/3200 [==============================] - 0s 151us/step - loss: 0.3254 - acc: 0.8769\n",
      "Epoch 8/200\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.3251 - acc: 0.8759\n",
      "Epoch 9/200\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.3254 - acc: 0.8769\n",
      "Epoch 10/200\n",
      "3200/3200 [==============================] - 1s 180us/step - loss: 0.3298 - acc: 0.8762\n",
      "Epoch 11/200\n",
      "3200/3200 [==============================] - 1s 176us/step - loss: 0.3274 - acc: 0.8784\n",
      "Epoch 12/200\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.3310 - acc: 0.8759\n",
      "Epoch 13/200\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.3288 - acc: 0.8731\n",
      "Epoch 14/200\n",
      "3200/3200 [==============================] - 1s 159us/step - loss: 0.3266 - acc: 0.8741\n",
      "Epoch 15/200\n",
      "3200/3200 [==============================] - 1s 160us/step - loss: 0.3270 - acc: 0.8769\n",
      "Epoch 16/200\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.3312 - acc: 0.8738\n",
      "Epoch 17/200\n",
      "3200/3200 [==============================] - 1s 157us/step - loss: 0.3260 - acc: 0.8756\n",
      "Epoch 18/200\n",
      "3200/3200 [==============================] - 1s 179us/step - loss: 0.3301 - acc: 0.8744\n",
      "Epoch 19/200\n",
      "3200/3200 [==============================] - 0s 154us/step - loss: 0.3260 - acc: 0.8766\n",
      "Epoch 20/200\n",
      "3200/3200 [==============================] - 0s 154us/step - loss: 0.3251 - acc: 0.8759\n",
      "Epoch 21/200\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.3258 - acc: 0.8769\n",
      "Epoch 22/200\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.3284 - acc: 0.8731\n",
      "Epoch 23/200\n",
      "3200/3200 [==============================] - 1s 161us/step - loss: 0.3270 - acc: 0.8722\n",
      "Epoch 24/200\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.3246 - acc: 0.8756\n",
      "Epoch 25/200\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.3261 - acc: 0.8747\n",
      "Epoch 26/200\n",
      "3200/3200 [==============================] - 1s 159us/step - loss: 0.3285 - acc: 0.8706\n",
      "Epoch 27/200\n",
      "3200/3200 [==============================] - 0s 156us/step - loss: 0.3251 - acc: 0.8766\n",
      "Epoch 28/200\n",
      "3200/3200 [==============================] - 0s 150us/step - loss: 0.3236 - acc: 0.8759\n",
      "Epoch 29/200\n",
      "3200/3200 [==============================] - 1s 158us/step - loss: 0.3243 - acc: 0.8762\n",
      "Epoch 30/200\n",
      "3200/3200 [==============================] - 1s 157us/step - loss: 0.3230 - acc: 0.8797\n",
      "Epoch 31/200\n",
      "3200/3200 [==============================] - 0s 153us/step - loss: 0.3282 - acc: 0.8787\n",
      "Epoch 32/200\n",
      "3200/3200 [==============================] - 1s 160us/step - loss: 0.3294 - acc: 0.8728\n",
      "Epoch 33/200\n",
      "3200/3200 [==============================] - 1s 158us/step - loss: 0.3278 - acc: 0.8731\n",
      "Epoch 34/200\n",
      "3200/3200 [==============================] - 0s 153us/step - loss: 0.3268 - acc: 0.8759\n",
      "Epoch 35/200\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.3258 - acc: 0.8716\n",
      "Epoch 36/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3249 - acc: 0.8775\n",
      "Epoch 37/200\n",
      "3200/3200 [==============================] - 1s 160us/step - loss: 0.3239 - acc: 0.8769\n",
      "Epoch 38/200\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.3271 - acc: 0.8738\n",
      "Epoch 39/200\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.3260 - acc: 0.8738\n",
      "Epoch 40/200\n",
      "3200/3200 [==============================] - 1s 168us/step - loss: 0.3257 - acc: 0.8750\n",
      "Epoch 41/200\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.3240 - acc: 0.8766\n",
      "Epoch 42/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3272 - acc: 0.8775\n",
      "Epoch 43/200\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.3276 - acc: 0.8738\n",
      "Epoch 44/200\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.3252 - acc: 0.8738\n",
      "Epoch 45/200\n",
      "3200/3200 [==============================] - 0s 150us/step - loss: 0.3248 - acc: 0.8769\n",
      "Epoch 46/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3227 - acc: 0.8747\n",
      "Epoch 47/200\n",
      "3200/3200 [==============================] - 0s 153us/step - loss: 0.3242 - acc: 0.8756\n",
      "Epoch 48/200\n",
      "3200/3200 [==============================] - 0s 140us/step - loss: 0.3270 - acc: 0.8741\n",
      "Epoch 49/200\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.3251 - acc: 0.8747\n",
      "Epoch 50/200\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.3246 - acc: 0.8775\n",
      "Epoch 51/200\n",
      "3200/3200 [==============================] - 0s 142us/step - loss: 0.3250 - acc: 0.8759\n",
      "Epoch 52/200\n",
      "3200/3200 [==============================] - 1s 158us/step - loss: 0.3236 - acc: 0.8772\n",
      "Epoch 53/200\n",
      "3200/3200 [==============================] - 0s 156us/step - loss: 0.3223 - acc: 0.8778\n",
      "Epoch 54/200\n",
      "3200/3200 [==============================] - 0s 148us/step - loss: 0.3224 - acc: 0.8772\n",
      "Epoch 55/200\n",
      "3200/3200 [==============================] - 0s 155us/step - loss: 0.3219 - acc: 0.8769\n",
      "Epoch 56/200\n",
      "3200/3200 [==============================] - 0s 156us/step - loss: 0.3256 - acc: 0.8753\n",
      "Epoch 57/200\n",
      "3200/3200 [==============================] - 0s 143us/step - loss: 0.3234 - acc: 0.8775\n",
      "Epoch 58/200\n",
      "3200/3200 [==============================] - 0s 156us/step - loss: 0.3249 - acc: 0.8762\n",
      "Epoch 59/200\n",
      "3200/3200 [==============================] - 0s 154us/step - loss: 0.3223 - acc: 0.8762\n",
      "Epoch 60/200\n",
      "3200/3200 [==============================] - 0s 150us/step - loss: 0.3242 - acc: 0.8756\n",
      "Epoch 61/200\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.3236 - acc: 0.8778\n",
      "Epoch 62/200\n",
      "3200/3200 [==============================] - 1s 159us/step - loss: 0.3262 - acc: 0.8759\n",
      "Epoch 63/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3254 - acc: 0.8734\n",
      "Epoch 64/200\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.3218 - acc: 0.8762\n",
      "Epoch 65/200\n",
      "3200/3200 [==============================] - 0s 147us/step - loss: 0.3213 - acc: 0.8803\n",
      "Epoch 66/200\n",
      "3200/3200 [==============================] - 0s 151us/step - loss: 0.3205 - acc: 0.8787\n",
      "Epoch 67/200\n",
      "3200/3200 [==============================] - 0s 149us/step - loss: 0.3203 - acc: 0.8781\n",
      "Epoch 68/200\n",
      "3200/3200 [==============================] - 0s 146us/step - loss: 0.3256 - acc: 0.8775\n",
      "Epoch 69/200\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.3220 - acc: 0.8800\n",
      "Epoch 70/200\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.3221 - acc: 0.8781\n",
      "Epoch 71/200\n",
      "3200/3200 [==============================] - 0s 156us/step - loss: 0.3225 - acc: 0.8759\n",
      "Epoch 72/200\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.3286 - acc: 0.8756\n",
      "Epoch 73/200\n",
      "3200/3200 [==============================] - 1s 178us/step - loss: 0.3301 - acc: 0.8756\n",
      "Epoch 74/200\n",
      "3200/3200 [==============================] - 0s 152us/step - loss: 0.3245 - acc: 0.8750\n",
      "Epoch 75/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3321 - acc: 0.8703\n",
      "Epoch 76/200\n",
      "3200/3200 [==============================] - 1s 160us/step - loss: 0.3285 - acc: 0.8762\n",
      "Epoch 77/200\n",
      "3200/3200 [==============================] - 0s 151us/step - loss: 0.3261 - acc: 0.8741\n",
      "Epoch 78/200\n",
      "3200/3200 [==============================] - 0s 144us/step - loss: 0.3247 - acc: 0.8769\n",
      "Epoch 79/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3299 - acc: 0.8759\n",
      "Epoch 80/200\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.3294 - acc: 0.8747\n",
      "Epoch 81/200\n",
      "3200/3200 [==============================] - 0s 141us/step - loss: 0.3220 - acc: 0.8781\n",
      "Epoch 82/200\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.3234 - acc: 0.8738\n",
      "Epoch 83/200\n",
      "3200/3200 [==============================] - 0s 155us/step - loss: 0.3224 - acc: 0.8756\n",
      "Epoch 84/200\n",
      "3200/3200 [==============================] - 0s 149us/step - loss: 0.3280 - acc: 0.8738\n",
      "Epoch 85/200\n",
      "3200/3200 [==============================] - 0s 155us/step - loss: 0.3217 - acc: 0.8784\n",
      "Epoch 86/200\n",
      "3200/3200 [==============================] - 0s 153us/step - loss: 0.3245 - acc: 0.8775\n",
      "Epoch 87/200\n",
      "3200/3200 [==============================] - 0s 150us/step - loss: 0.3239 - acc: 0.8772\n",
      "Epoch 88/200\n",
      "3200/3200 [==============================] - 0s 151us/step - loss: 0.3207 - acc: 0.8766\n",
      "Epoch 89/200\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.3215 - acc: 0.8794\n",
      "Epoch 90/200\n",
      "3200/3200 [==============================] - 0s 152us/step - loss: 0.3229 - acc: 0.8784\n",
      "Epoch 91/200\n",
      "3200/3200 [==============================] - 1s 177us/step - loss: 0.3239 - acc: 0.8778\n",
      "Epoch 92/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3219 - acc: 0.8784\n",
      "Epoch 93/200\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.3221 - acc: 0.8769\n",
      "Epoch 94/200\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.3216 - acc: 0.8800\n",
      "Epoch 95/200\n",
      "3200/3200 [==============================] - 0s 150us/step - loss: 0.3315 - acc: 0.8741\n",
      "Epoch 96/200\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.3216 - acc: 0.8769\n",
      "Epoch 97/200\n",
      "3200/3200 [==============================] - 1s 161us/step - loss: 0.3252 - acc: 0.8738\n",
      "Epoch 98/200\n",
      "3200/3200 [==============================] - 0s 145us/step - loss: 0.3224 - acc: 0.8769\n",
      "Epoch 99/200\n",
      "3200/3200 [==============================] - 1s 158us/step - loss: 0.3234 - acc: 0.8753\n",
      "Epoch 100/200\n",
      "3200/3200 [==============================] - 0s 155us/step - loss: 0.3256 - acc: 0.8766\n",
      "Epoch 101/200\n",
      "3200/3200 [==============================] - 0s 152us/step - loss: 0.3237 - acc: 0.8784\n",
      "Epoch 102/200\n",
      "3200/3200 [==============================] - 0s 155us/step - loss: 0.3212 - acc: 0.8775\n",
      "Epoch 103/200\n",
      "3200/3200 [==============================] - 1s 158us/step - loss: 0.3252 - acc: 0.8741\n",
      "Epoch 104/200\n",
      "3200/3200 [==============================] - 0s 150us/step - loss: 0.3279 - acc: 0.8725\n",
      "Epoch 105/200\n",
      "3200/3200 [==============================] - 0s 153us/step - loss: 0.3231 - acc: 0.8769\n",
      "Epoch 106/200\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.3200 - acc: 0.8762\n",
      "Epoch 107/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3213 - acc: 0.8775\n",
      "Epoch 108/200\n",
      "3200/3200 [==============================] - 1s 194us/step - loss: 0.3230 - acc: 0.8769\n",
      "Epoch 109/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3210 - acc: 0.8772\n",
      "Epoch 110/200\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.3208 - acc: 0.8803\n",
      "Epoch 111/200\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.3224 - acc: 0.8753\n",
      "Epoch 112/200\n",
      "3200/3200 [==============================] - 0s 155us/step - loss: 0.3254 - acc: 0.8741\n",
      "Epoch 113/200\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.3258 - acc: 0.8781\n",
      "Epoch 114/200\n",
      "3200/3200 [==============================] - 1s 160us/step - loss: 0.3202 - acc: 0.8787\n",
      "Epoch 115/200\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.3315 - acc: 0.8744\n",
      "Epoch 116/200\n",
      "3200/3200 [==============================] - 0s 155us/step - loss: 0.3233 - acc: 0.8759\n",
      "Epoch 117/200\n",
      "3200/3200 [==============================] - 1s 160us/step - loss: 0.3208 - acc: 0.8772\n",
      "Epoch 118/200\n",
      "3200/3200 [==============================] - 0s 151us/step - loss: 0.3191 - acc: 0.8769\n",
      "Epoch 119/200\n",
      "3200/3200 [==============================] - 1s 161us/step - loss: 0.3200 - acc: 0.8772\n",
      "Epoch 120/200\n",
      "3200/3200 [==============================] - 0s 152us/step - loss: 0.3226 - acc: 0.8762\n",
      "Epoch 121/200\n",
      "3200/3200 [==============================] - 1s 159us/step - loss: 0.3239 - acc: 0.8741\n",
      "Epoch 122/200\n",
      "3200/3200 [==============================] - 1s 158us/step - loss: 0.3217 - acc: 0.8781\n",
      "Epoch 123/200\n",
      "3200/3200 [==============================] - 0s 151us/step - loss: 0.3206 - acc: 0.8769\n",
      "Epoch 124/200\n",
      "3200/3200 [==============================] - 1s 158us/step - loss: 0.3206 - acc: 0.8759\n",
      "Epoch 125/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3286 - acc: 0.8744\n",
      "Epoch 126/200\n",
      "3200/3200 [==============================] - 0s 152us/step - loss: 0.3216 - acc: 0.8769\n",
      "Epoch 127/200\n",
      "3200/3200 [==============================] - 1s 161us/step - loss: 0.3196 - acc: 0.8784\n",
      "Epoch 128/200\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.3193 - acc: 0.8769\n",
      "Epoch 129/200\n",
      "3200/3200 [==============================] - 0s 141us/step - loss: 0.3260 - acc: 0.8750\n",
      "Epoch 130/200\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.3215 - acc: 0.8766\n",
      "Epoch 131/200\n",
      "3200/3200 [==============================] - 0s 153us/step - loss: 0.3192 - acc: 0.8775\n",
      "Epoch 132/200\n",
      "3200/3200 [==============================] - 1s 157us/step - loss: 0.3196 - acc: 0.8800\n",
      "Epoch 133/200\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.3231 - acc: 0.8797\n",
      "Epoch 134/200\n",
      "3200/3200 [==============================] - 1s 159us/step - loss: 0.3208 - acc: 0.8762\n",
      "Epoch 135/200\n",
      "3200/3200 [==============================] - 1s 159us/step - loss: 0.3229 - acc: 0.8747\n",
      "Epoch 136/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3246 - acc: 0.8731\n",
      "Epoch 137/200\n",
      "3200/3200 [==============================] - 1s 161us/step - loss: 0.3298 - acc: 0.8738\n",
      "Epoch 138/200\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.3211 - acc: 0.8769\n",
      "Epoch 139/200\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.3193 - acc: 0.8762\n",
      "Epoch 140/200\n",
      "3200/3200 [==============================] - 0s 148us/step - loss: 0.3205 - acc: 0.8778\n",
      "Epoch 141/200\n",
      "3200/3200 [==============================] - 0s 155us/step - loss: 0.3215 - acc: 0.8784\n",
      "Epoch 142/200\n",
      "3200/3200 [==============================] - 0s 151us/step - loss: 0.3245 - acc: 0.8759\n",
      "Epoch 143/200\n",
      "3200/3200 [==============================] - 0s 147us/step - loss: 0.3206 - acc: 0.8769\n",
      "Epoch 144/200\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.3210 - acc: 0.8759\n",
      "Epoch 145/200\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.3194 - acc: 0.8769\n",
      "Epoch 146/200\n",
      "3200/3200 [==============================] - 1s 160us/step - loss: 0.3175 - acc: 0.8797\n",
      "Epoch 147/200\n",
      "3200/3200 [==============================] - 1s 174us/step - loss: 0.3185 - acc: 0.8791\n",
      "Epoch 148/200\n",
      "3200/3200 [==============================] - 0s 156us/step - loss: 0.3198 - acc: 0.8794\n",
      "Epoch 149/200\n",
      "3200/3200 [==============================] - 0s 153us/step - loss: 0.3208 - acc: 0.8800\n",
      "Epoch 150/200\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.3265 - acc: 0.8744\n",
      "Epoch 151/200\n",
      "3200/3200 [==============================] - 1s 161us/step - loss: 0.3234 - acc: 0.8772\n",
      "Epoch 152/200\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.3244 - acc: 0.8794\n",
      "Epoch 153/200\n",
      "3200/3200 [==============================] - 1s 163us/step - loss: 0.3176 - acc: 0.8775\n",
      "Epoch 154/200\n",
      "3200/3200 [==============================] - 0s 153us/step - loss: 0.3182 - acc: 0.8787\n",
      "Epoch 155/200\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.3184 - acc: 0.8772\n",
      "Epoch 156/200\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.3184 - acc: 0.8800\n",
      "Epoch 157/200\n",
      "3200/3200 [==============================] - 1s 161us/step - loss: 0.3222 - acc: 0.8759\n",
      "Epoch 158/200\n",
      "3200/3200 [==============================] - 1s 171us/step - loss: 0.3224 - acc: 0.8738\n",
      "Epoch 159/200\n",
      "3200/3200 [==============================] - 0s 152us/step - loss: 0.3312 - acc: 0.8753\n",
      "Epoch 160/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3265 - acc: 0.8762\n",
      "Epoch 161/200\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.3201 - acc: 0.8787\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - 0s 153us/step - loss: 0.3223 - acc: 0.8803\n",
      "Epoch 163/200\n",
      "3200/3200 [==============================] - 1s 170us/step - loss: 0.3325 - acc: 0.8694\n",
      "Epoch 164/200\n",
      "3200/3200 [==============================] - 1s 158us/step - loss: 0.3228 - acc: 0.8778\n",
      "Epoch 165/200\n",
      "3200/3200 [==============================] - 0s 146us/step - loss: 0.3192 - acc: 0.8787\n",
      "Epoch 166/200\n",
      "3200/3200 [==============================] - 1s 191us/step - loss: 0.3177 - acc: 0.8784\n",
      "Epoch 167/200\n",
      "3200/3200 [==============================] - 1s 167us/step - loss: 0.3185 - acc: 0.8781\n",
      "Epoch 168/200\n",
      "3200/3200 [==============================] - 0s 154us/step - loss: 0.3177 - acc: 0.8791\n",
      "Epoch 169/200\n",
      "3200/3200 [==============================] - 1s 157us/step - loss: 0.3210 - acc: 0.8791\n",
      "Epoch 170/200\n",
      "3200/3200 [==============================] - 0s 155us/step - loss: 0.3232 - acc: 0.8778\n",
      "Epoch 171/200\n",
      "3200/3200 [==============================] - 1s 161us/step - loss: 0.3222 - acc: 0.8756\n",
      "Epoch 172/200\n",
      "3200/3200 [==============================] - 0s 153us/step - loss: 0.3198 - acc: 0.8778\n",
      "Epoch 173/200\n",
      "3200/3200 [==============================] - 0s 152us/step - loss: 0.3253 - acc: 0.8791\n",
      "Epoch 174/200\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.3222 - acc: 0.8750\n",
      "Epoch 175/200\n",
      "3200/3200 [==============================] - 1s 169us/step - loss: 0.3195 - acc: 0.8775\n",
      "Epoch 176/200\n",
      "3200/3200 [==============================] - 1s 157us/step - loss: 0.3184 - acc: 0.8784\n",
      "Epoch 177/200\n",
      "3200/3200 [==============================] - 1s 156us/step - loss: 0.3198 - acc: 0.8784\n",
      "Epoch 178/200\n",
      "3200/3200 [==============================] - 1s 165us/step - loss: 0.3197 - acc: 0.8781\n",
      "Epoch 179/200\n",
      "3200/3200 [==============================] - 1s 160us/step - loss: 0.3174 - acc: 0.8797\n",
      "Epoch 180/200\n",
      "3200/3200 [==============================] - 1s 160us/step - loss: 0.3215 - acc: 0.8766\n",
      "Epoch 181/200\n",
      "3200/3200 [==============================] - 0s 154us/step - loss: 0.3234 - acc: 0.8781\n",
      "Epoch 182/200\n",
      "3200/3200 [==============================] - 0s 147us/step - loss: 0.3196 - acc: 0.8781\n",
      "Epoch 183/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3181 - acc: 0.8775\n",
      "Epoch 184/200\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.3193 - acc: 0.8787\n",
      "Epoch 185/200\n",
      "3200/3200 [==============================] - 1s 158us/step - loss: 0.3265 - acc: 0.8766\n",
      "Epoch 186/200\n",
      "3200/3200 [==============================] - 1s 161us/step - loss: 0.3222 - acc: 0.8762\n",
      "Epoch 187/200\n",
      "3200/3200 [==============================] - 0s 140us/step - loss: 0.3173 - acc: 0.8803\n",
      "Epoch 188/200\n",
      "3200/3200 [==============================] - 0s 155us/step - loss: 0.3179 - acc: 0.8787\n",
      "Epoch 189/200\n",
      "3200/3200 [==============================] - 1s 173us/step - loss: 0.3208 - acc: 0.8766\n",
      "Epoch 190/200\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.3269 - acc: 0.8713\n",
      "Epoch 191/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3170 - acc: 0.8797\n",
      "Epoch 192/200\n",
      "3200/3200 [==============================] - 1s 175us/step - loss: 0.3256 - acc: 0.8713\n",
      "Epoch 193/200\n",
      "3200/3200 [==============================] - 1s 166us/step - loss: 0.3197 - acc: 0.8769\n",
      "Epoch 194/200\n",
      "3200/3200 [==============================] - 1s 181us/step - loss: 0.3234 - acc: 0.8759\n",
      "Epoch 195/200\n",
      "3200/3200 [==============================] - 0s 154us/step - loss: 0.3206 - acc: 0.8738\n",
      "Epoch 196/200\n",
      "3200/3200 [==============================] - 0s 154us/step - loss: 0.3196 - acc: 0.8766\n",
      "Epoch 197/200\n",
      "3200/3200 [==============================] - 1s 172us/step - loss: 0.3168 - acc: 0.8775\n",
      "Epoch 198/200\n",
      "3200/3200 [==============================] - 1s 162us/step - loss: 0.3210 - acc: 0.8766\n",
      "Epoch 199/200\n",
      "3200/3200 [==============================] - 1s 164us/step - loss: 0.3238 - acc: 0.8744\n",
      "Epoch 200/200\n",
      "3200/3200 [==============================] - 1s 158us/step - loss: 0.3179 - acc: 0.8787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd140388c50>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "model.fit(x[:3200],y_train[:3200],batch_size=64,epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/355 [==============================] - 0s 337us/step\n",
      "test loss 0.2047892114226247\n",
      "test accuracy 0.9183098591549296\n"
     ]
    }
   ],
   "source": [
    "# 评估模型\n",
    "loss,accuracy = model.evaluate(x[3200:],y_test)\n",
    "\n",
    "print('test loss',loss)\n",
    "print('test accuracy',accuracy)\n",
    "\n",
    "p = model.predict(x[3200:])\n",
    "\n",
    "p[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 256)               266240    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 200)               51400     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 318,042\n",
      "Trainable params: 318,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.6923 - acc: 0.5437\n",
      "Epoch 2/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.6766 - acc: 0.5727\n",
      "Epoch 3/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.6729 - acc: 0.5717\n",
      "Epoch 4/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.6595 - acc: 0.5937\n",
      "Epoch 5/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.6479 - acc: 0.6037\n",
      "Epoch 6/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.6299 - acc: 0.6253\n",
      "Epoch 7/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.6149 - acc: 0.6533\n",
      "Epoch 8/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.6237 - acc: 0.6487\n",
      "Epoch 9/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.6083 - acc: 0.6653\n",
      "Epoch 10/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.6081 - acc: 0.6547\n",
      "Epoch 11/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.6044 - acc: 0.6707\n",
      "Epoch 12/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.6153 - acc: 0.6533\n",
      "Epoch 13/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.6130 - acc: 0.6610\n",
      "Epoch 14/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.5965 - acc: 0.6743\n",
      "Epoch 15/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.5922 - acc: 0.6747\n",
      "Epoch 16/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.5912 - acc: 0.6767\n",
      "Epoch 17/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.6037 - acc: 0.6653\n",
      "Epoch 18/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.5942 - acc: 0.6773\n",
      "Epoch 19/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.5801 - acc: 0.6850\n",
      "Epoch 20/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.5740 - acc: 0.6937\n",
      "Epoch 21/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.5768 - acc: 0.6843\n",
      "Epoch 22/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.5685 - acc: 0.6970\n",
      "Epoch 23/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.5541 - acc: 0.7047\n",
      "Epoch 24/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.5441 - acc: 0.7083\n",
      "Epoch 25/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.5555 - acc: 0.7000\n",
      "Epoch 26/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.5455 - acc: 0.7017\n",
      "Epoch 27/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.5474 - acc: 0.7070\n",
      "Epoch 28/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.5319 - acc: 0.7143\n",
      "Epoch 29/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.5199 - acc: 0.7293\n",
      "Epoch 30/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.5172 - acc: 0.7297\n",
      "Epoch 31/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.5113 - acc: 0.7370\n",
      "Epoch 32/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.5094 - acc: 0.7447\n",
      "Epoch 33/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4990 - acc: 0.7433\n",
      "Epoch 34/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4946 - acc: 0.7547\n",
      "Epoch 35/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4947 - acc: 0.7537\n",
      "Epoch 36/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4971 - acc: 0.7447\n",
      "Epoch 37/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.5038 - acc: 0.7473\n",
      "Epoch 38/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.5031 - acc: 0.7487\n",
      "Epoch 39/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4906 - acc: 0.7590\n",
      "Epoch 40/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4979 - acc: 0.7623\n",
      "Epoch 41/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4852 - acc: 0.7540\n",
      "Epoch 42/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4794 - acc: 0.7687\n",
      "Epoch 43/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4846 - acc: 0.7603\n",
      "Epoch 44/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4923 - acc: 0.7590\n",
      "Epoch 45/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4782 - acc: 0.7627\n",
      "Epoch 46/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4649 - acc: 0.7743\n",
      "Epoch 47/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4660 - acc: 0.7677\n",
      "Epoch 48/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4811 - acc: 0.7573\n",
      "Epoch 49/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4599 - acc: 0.7683\n",
      "Epoch 50/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4580 - acc: 0.7727\n",
      "Epoch 51/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4612 - acc: 0.7673\n",
      "Epoch 52/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4720 - acc: 0.7647\n",
      "Epoch 53/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4612 - acc: 0.7703\n",
      "Epoch 54/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4558 - acc: 0.7690\n",
      "Epoch 55/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4580 - acc: 0.7740\n",
      "Epoch 56/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4655 - acc: 0.7683\n",
      "Epoch 57/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4592 - acc: 0.7673\n",
      "Epoch 58/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4528 - acc: 0.7767\n",
      "Epoch 59/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4667 - acc: 0.7653\n",
      "Epoch 60/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4597 - acc: 0.7760\n",
      "Epoch 61/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4520 - acc: 0.7783\n",
      "Epoch 62/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4438 - acc: 0.7797\n",
      "Epoch 63/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4573 - acc: 0.7633\n",
      "Epoch 64/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4491 - acc: 0.7793\n",
      "Epoch 65/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4440 - acc: 0.7890\n",
      "Epoch 66/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4407 - acc: 0.7823\n",
      "Epoch 67/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4317 - acc: 0.7903\n",
      "Epoch 68/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4360 - acc: 0.7893\n",
      "Epoch 69/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4332 - acc: 0.7837\n",
      "Epoch 70/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4320 - acc: 0.7877\n",
      "Epoch 71/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4378 - acc: 0.7823\n",
      "Epoch 72/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4436 - acc: 0.7793\n",
      "Epoch 73/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4346 - acc: 0.7817\n",
      "Epoch 74/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4299 - acc: 0.7897\n",
      "Epoch 75/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4211 - acc: 0.7947\n",
      "Epoch 76/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4276 - acc: 0.7917\n",
      "Epoch 77/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4448 - acc: 0.7810\n",
      "Epoch 78/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4999 - acc: 0.7450\n",
      "Epoch 79/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4760 - acc: 0.7493\n",
      "Epoch 80/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4643 - acc: 0.7590\n",
      "Epoch 81/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4629 - acc: 0.7587\n",
      "Epoch 82/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4962 - acc: 0.7480\n",
      "Epoch 83/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.6706 - acc: 0.6453\n",
      "Epoch 84/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.5949 - acc: 0.6760\n",
      "Epoch 85/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.5712 - acc: 0.6920\n",
      "Epoch 86/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.5663 - acc: 0.7057\n",
      "Epoch 87/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.5418 - acc: 0.7163\n",
      "Epoch 88/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.5287 - acc: 0.7193\n",
      "Epoch 89/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.5016 - acc: 0.7397\n",
      "Epoch 90/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.5086 - acc: 0.7370\n",
      "Epoch 91/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.5028 - acc: 0.7410\n",
      "Epoch 92/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4885 - acc: 0.7540\n",
      "Epoch 93/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.5309 - acc: 0.7353\n",
      "Epoch 94/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.5277 - acc: 0.7257\n",
      "Epoch 95/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4942 - acc: 0.7500\n",
      "Epoch 96/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4791 - acc: 0.7593\n",
      "Epoch 97/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4696 - acc: 0.7657\n",
      "Epoch 98/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4760 - acc: 0.7543\n",
      "Epoch 99/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4682 - acc: 0.7660\n",
      "Epoch 100/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4594 - acc: 0.7673\n",
      "Epoch 101/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4649 - acc: 0.7770\n",
      "Epoch 102/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4551 - acc: 0.7803\n",
      "Epoch 103/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4584 - acc: 0.7790\n",
      "Epoch 104/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4458 - acc: 0.7863\n",
      "Epoch 105/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4443 - acc: 0.7827\n",
      "Epoch 106/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4483 - acc: 0.7763\n",
      "Epoch 107/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4403 - acc: 0.7883\n",
      "Epoch 108/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4335 - acc: 0.7917\n",
      "Epoch 109/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4317 - acc: 0.7893\n",
      "Epoch 110/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4768 - acc: 0.7727\n",
      "Epoch 111/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4666 - acc: 0.7700\n",
      "Epoch 112/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4508 - acc: 0.7763\n",
      "Epoch 113/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4389 - acc: 0.7867\n",
      "Epoch 114/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4330 - acc: 0.7927\n",
      "Epoch 115/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4313 - acc: 0.7947\n",
      "Epoch 116/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4230 - acc: 0.7987\n",
      "Epoch 117/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4174 - acc: 0.8010\n",
      "Epoch 118/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4154 - acc: 0.8023\n",
      "Epoch 119/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4218 - acc: 0.8020\n",
      "Epoch 120/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4286 - acc: 0.7880\n",
      "Epoch 121/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4226 - acc: 0.7937\n",
      "Epoch 122/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4136 - acc: 0.8020\n",
      "Epoch 123/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4018 - acc: 0.8043\n",
      "Epoch 124/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4007 - acc: 0.8087\n",
      "Epoch 125/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4071 - acc: 0.8060\n",
      "Epoch 126/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4023 - acc: 0.8083\n",
      "Epoch 127/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3965 - acc: 0.8117\n",
      "Epoch 128/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3992 - acc: 0.8147\n",
      "Epoch 129/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3928 - acc: 0.8097\n",
      "Epoch 130/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3911 - acc: 0.8143\n",
      "Epoch 131/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3873 - acc: 0.8177\n",
      "Epoch 132/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3877 - acc: 0.8103\n",
      "Epoch 133/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3990 - acc: 0.8070\n",
      "Epoch 134/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3853 - acc: 0.8153\n",
      "Epoch 135/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3831 - acc: 0.8180\n",
      "Epoch 136/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3802 - acc: 0.8197\n",
      "Epoch 137/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3841 - acc: 0.8187\n",
      "Epoch 138/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3791 - acc: 0.8213\n",
      "Epoch 139/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3800 - acc: 0.8257\n",
      "Epoch 140/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3684 - acc: 0.8273\n",
      "Epoch 141/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3739 - acc: 0.8223\n",
      "Epoch 142/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3761 - acc: 0.8183\n",
      "Epoch 143/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3698 - acc: 0.8227\n",
      "Epoch 144/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3737 - acc: 0.8283\n",
      "Epoch 145/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3749 - acc: 0.8190\n",
      "Epoch 146/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3825 - acc: 0.8283\n",
      "Epoch 147/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3878 - acc: 0.8177\n",
      "Epoch 148/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3831 - acc: 0.8157\n",
      "Epoch 149/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3691 - acc: 0.8253\n",
      "Epoch 150/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3730 - acc: 0.8217\n",
      "Epoch 151/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3786 - acc: 0.8147\n",
      "Epoch 152/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3647 - acc: 0.8277\n",
      "Epoch 153/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3686 - acc: 0.8200\n",
      "Epoch 154/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3637 - acc: 0.8253\n",
      "Epoch 155/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3576 - acc: 0.8287\n",
      "Epoch 156/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3650 - acc: 0.8277\n",
      "Epoch 157/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3695 - acc: 0.8263\n",
      "Epoch 158/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3910 - acc: 0.8187\n",
      "Epoch 159/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3804 - acc: 0.8227\n",
      "Epoch 160/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3722 - acc: 0.8207\n",
      "Epoch 161/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3626 - acc: 0.8230\n",
      "Epoch 162/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3573 - acc: 0.8290\n",
      "Epoch 163/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3490 - acc: 0.8350\n",
      "Epoch 164/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3479 - acc: 0.8343\n",
      "Epoch 165/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3492 - acc: 0.8317\n",
      "Epoch 166/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3442 - acc: 0.8380\n",
      "Epoch 167/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3463 - acc: 0.8407\n",
      "Epoch 168/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3490 - acc: 0.8320\n",
      "Epoch 169/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3440 - acc: 0.8397\n",
      "Epoch 170/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3429 - acc: 0.8387\n",
      "Epoch 171/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3525 - acc: 0.8280\n",
      "Epoch 172/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3468 - acc: 0.8380\n",
      "Epoch 173/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3372 - acc: 0.8403\n",
      "Epoch 174/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3421 - acc: 0.8430\n",
      "Epoch 175/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3416 - acc: 0.8407\n",
      "Epoch 176/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3360 - acc: 0.8427\n",
      "Epoch 177/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3583 - acc: 0.8437\n",
      "Epoch 178/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3710 - acc: 0.8273\n",
      "Epoch 179/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3592 - acc: 0.8297\n",
      "Epoch 180/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3647 - acc: 0.8280\n",
      "Epoch 181/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3609 - acc: 0.8387\n",
      "Epoch 182/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3562 - acc: 0.8330\n",
      "Epoch 183/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3427 - acc: 0.8453\n",
      "Epoch 184/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3400 - acc: 0.8460\n",
      "Epoch 185/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3311 - acc: 0.8490\n",
      "Epoch 186/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3290 - acc: 0.8500\n",
      "Epoch 187/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3360 - acc: 0.8487\n",
      "Epoch 188/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3322 - acc: 0.8537\n",
      "Epoch 189/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3386 - acc: 0.8483\n",
      "Epoch 190/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3290 - acc: 0.8543\n",
      "Epoch 191/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3249 - acc: 0.8577\n",
      "Epoch 192/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3248 - acc: 0.8580\n",
      "Epoch 193/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3290 - acc: 0.8500\n",
      "Epoch 194/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3254 - acc: 0.8587\n",
      "Epoch 195/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3352 - acc: 0.8507\n",
      "Epoch 196/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3172 - acc: 0.8600\n",
      "Epoch 197/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3152 - acc: 0.8653\n",
      "Epoch 198/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3135 - acc: 0.8607\n",
      "Epoch 199/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3179 - acc: 0.8597\n",
      "Epoch 200/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3312 - acc: 0.8550\n",
      "Epoch 201/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3274 - acc: 0.8527\n",
      "Epoch 202/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3124 - acc: 0.8617\n",
      "Epoch 203/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3135 - acc: 0.8633\n",
      "Epoch 204/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3186 - acc: 0.8560\n",
      "Epoch 205/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3178 - acc: 0.8500\n",
      "Epoch 206/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3154 - acc: 0.8533\n",
      "Epoch 207/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3057 - acc: 0.8660\n",
      "Epoch 208/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3051 - acc: 0.8603\n",
      "Epoch 209/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3200 - acc: 0.8563\n",
      "Epoch 210/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3130 - acc: 0.8610\n",
      "Epoch 211/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2973 - acc: 0.8690\n",
      "Epoch 212/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3018 - acc: 0.8667\n",
      "Epoch 213/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3192 - acc: 0.8523\n",
      "Epoch 214/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3212 - acc: 0.8540\n",
      "Epoch 215/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3069 - acc: 0.8597\n",
      "Epoch 216/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3034 - acc: 0.8707\n",
      "Epoch 217/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2979 - acc: 0.8653\n",
      "Epoch 218/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2917 - acc: 0.8727\n",
      "Epoch 219/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2922 - acc: 0.8683\n",
      "Epoch 220/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2882 - acc: 0.8693\n",
      "Epoch 221/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2887 - acc: 0.8727\n",
      "Epoch 222/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3127 - acc: 0.8563\n",
      "Epoch 223/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2991 - acc: 0.8690\n",
      "Epoch 224/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3074 - acc: 0.8657\n",
      "Epoch 225/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3017 - acc: 0.8677\n",
      "Epoch 226/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3019 - acc: 0.8733\n",
      "Epoch 227/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3172 - acc: 0.8620\n",
      "Epoch 228/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3155 - acc: 0.8603\n",
      "Epoch 229/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2933 - acc: 0.8697\n",
      "Epoch 230/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2956 - acc: 0.8740\n",
      "Epoch 231/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2885 - acc: 0.8743\n",
      "Epoch 232/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2903 - acc: 0.8713\n",
      "Epoch 233/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2920 - acc: 0.8737\n",
      "Epoch 234/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2933 - acc: 0.8677\n",
      "Epoch 235/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3163 - acc: 0.8613\n",
      "Epoch 236/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4673 - acc: 0.8193\n",
      "Epoch 237/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.6073 - acc: 0.7293\n",
      "Epoch 238/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4835 - acc: 0.7730\n",
      "Epoch 239/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.4129 - acc: 0.8177\n",
      "Epoch 240/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3908 - acc: 0.8287\n",
      "Epoch 241/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.4043 - acc: 0.8163\n",
      "Epoch 242/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3698 - acc: 0.8310\n",
      "Epoch 243/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3563 - acc: 0.8430\n",
      "Epoch 244/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3378 - acc: 0.8570\n",
      "Epoch 245/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3455 - acc: 0.8470\n",
      "Epoch 246/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3411 - acc: 0.8463\n",
      "Epoch 247/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3282 - acc: 0.8520\n",
      "Epoch 248/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3169 - acc: 0.8653\n",
      "Epoch 249/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3125 - acc: 0.8640\n",
      "Epoch 250/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3159 - acc: 0.8563\n",
      "Epoch 251/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3225 - acc: 0.8553\n",
      "Epoch 252/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3220 - acc: 0.8670\n",
      "Epoch 253/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3162 - acc: 0.8600\n",
      "Epoch 254/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3053 - acc: 0.8660\n",
      "Epoch 255/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2941 - acc: 0.8680\n",
      "Epoch 256/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2981 - acc: 0.8697\n",
      "Epoch 257/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3022 - acc: 0.8673\n",
      "Epoch 258/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2988 - acc: 0.8700\n",
      "Epoch 259/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2875 - acc: 0.8790\n",
      "Epoch 260/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2876 - acc: 0.8783\n",
      "Epoch 261/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2910 - acc: 0.8733\n",
      "Epoch 262/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2866 - acc: 0.8740\n",
      "Epoch 263/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2866 - acc: 0.8700\n",
      "Epoch 264/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2855 - acc: 0.8720\n",
      "Epoch 265/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2761 - acc: 0.8783\n",
      "Epoch 266/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2780 - acc: 0.8840\n",
      "Epoch 267/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2750 - acc: 0.8863\n",
      "Epoch 268/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2797 - acc: 0.8827\n",
      "Epoch 269/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2877 - acc: 0.8793\n",
      "Epoch 270/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2799 - acc: 0.8830\n",
      "Epoch 271/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2802 - acc: 0.8853\n",
      "Epoch 272/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2706 - acc: 0.8800\n",
      "Epoch 273/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2677 - acc: 0.8837\n",
      "Epoch 274/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2712 - acc: 0.8820\n",
      "Epoch 275/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2746 - acc: 0.8847\n",
      "Epoch 276/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2690 - acc: 0.8803\n",
      "Epoch 277/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2689 - acc: 0.8863\n",
      "Epoch 278/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2628 - acc: 0.8910\n",
      "Epoch 279/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2618 - acc: 0.8893\n",
      "Epoch 280/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2636 - acc: 0.8880\n",
      "Epoch 281/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2573 - acc: 0.8930\n",
      "Epoch 282/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2525 - acc: 0.8983\n",
      "Epoch 283/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2614 - acc: 0.8890\n",
      "Epoch 284/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2594 - acc: 0.8887\n",
      "Epoch 285/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2606 - acc: 0.8913\n",
      "Epoch 286/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2662 - acc: 0.8877\n",
      "Epoch 287/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2540 - acc: 0.8887\n",
      "Epoch 288/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.3690 - acc: 0.8453\n",
      "Epoch 289/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3117 - acc: 0.8607\n",
      "Epoch 290/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.3014 - acc: 0.8733\n",
      "Epoch 291/300\n",
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.2892 - acc: 0.8760\n",
      "Epoch 292/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2697 - acc: 0.8847\n",
      "Epoch 293/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2686 - acc: 0.8870\n",
      "Epoch 294/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2697 - acc: 0.8810\n",
      "Epoch 295/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2669 - acc: 0.8880\n",
      "Epoch 296/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2639 - acc: 0.8850\n",
      "Epoch 297/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2655 - acc: 0.8817\n",
      "Epoch 298/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2680 - acc: 0.8860\n",
      "Epoch 299/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2657 - acc: 0.8880\n",
      "Epoch 300/300\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2565 - acc: 0.8937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcd3fe4a630>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "model.fit(x_train,y_train,batch_size=128,epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2961/2961 [==============================] - 8s 3ms/step\n",
      "test loss 0.30742023430817517\n",
      "test accuracy 0.8571428572233768\n"
     ]
    }
   ],
   "source": [
    "# 评估模型\n",
    "loss,accuracy = model.evaluate(x_test,y_test)\n",
    "\n",
    "print('test loss',loss)\n",
    "print('test accuracy',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0176864 , 0.9823136 ],\n",
       "       [0.03465068, 0.9653494 ],\n",
       "       [0.16076513, 0.83923495],\n",
       "       [0.45331347, 0.54668653],\n",
       "       [0.41238803, 0.5876119 ],\n",
       "       [0.5698003 , 0.43019968],\n",
       "       [0.61668897, 0.383311  ],\n",
       "       [0.3237785 , 0.67622155],\n",
       "       [0.46554664, 0.5344534 ],\n",
       "       [0.5422835 , 0.45771652]], dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = model.predict(x[3000:])\n",
    "\n",
    "p[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_test,axis=1)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 1, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(x_test[-10:]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.68243315,  0.58925604,  0.35573441],\n",
       "        [ 0.68677302,  0.60780638,  0.37112146],\n",
       "        [ 0.67700302,  0.59209266,  0.36469325],\n",
       "        ...,\n",
       "        [ 0.71984887,  1.20518682,  0.67997824],\n",
       "        [ 0.70889313,  1.09930849,  0.61711824],\n",
       "        [ 0.71217514,  0.85800405,  0.48673185]],\n",
       "\n",
       "       [[ 1.6819057 ,  6.03897411,  7.58308694],\n",
       "        [ 1.69757553,  6.65576341,  7.79173837],\n",
       "        [ 1.70689292,  6.39256763,  7.62175808],\n",
       "        ...,\n",
       "        [ 1.79684022,  3.68024718,  5.08508901],\n",
       "        [ 1.7982281 ,  3.47404753,  4.95274987],\n",
       "        [ 1.77590998,  3.10637131,  4.34346348]],\n",
       "\n",
       "       [[ 1.27212026,  3.65098843,  3.56377684],\n",
       "        [ 1.26060288,  4.51145991,  4.53849977],\n",
       "        [ 1.25757134,  3.793343  ,  3.64790272],\n",
       "        ...,\n",
       "        [ 1.86877036,  9.48100705, 13.10308744],\n",
       "        [ 1.82473817,  6.47218655,  8.85794759],\n",
       "        [ 1.88310358,  6.03973828,  8.25170571]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.63147805,  5.79491248,  7.16018928],\n",
       "        [ 1.64300014,  5.7675158 ,  6.86734203],\n",
       "        [ 1.65971369,  6.318408  ,  7.42325629],\n",
       "        ...,\n",
       "        [ 1.79861766,  4.4058903 ,  6.25462516],\n",
       "        [ 1.8197742 ,  4.82964491,  6.76093757],\n",
       "        [ 1.81109094,  4.58123446,  6.41371873]],\n",
       "\n",
       "       [[ 1.19337505,  2.6884982 ,  2.49296804],\n",
       "        [ 1.19121454,  3.25032097,  3.06383263],\n",
       "        [ 1.19904327,  2.76141848,  2.65000931],\n",
       "        ...,\n",
       "        [ 1.84846835, 13.31555363, 17.93280386],\n",
       "        [ 1.81274049, 10.73239652, 13.72264044],\n",
       "        [ 1.82406574,  8.96470065, 12.39502838]],\n",
       "\n",
       "       [[ 1.84361753, 10.91124668, 13.74051703],\n",
       "        [ 1.88876474, 10.79513809, 14.59043612],\n",
       "        [ 1.87062441, 10.83454124, 14.94053743],\n",
       "        ...,\n",
       "        [ 1.98912211, 12.76681879, 18.62223605],\n",
       "        [ 1.99626444, 10.25074766, 15.1646935 ],\n",
       "        [ 1.99346262, 10.5951892 , 15.82164114]]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = np.array([i[0] for i in ll[-10:]])\n",
    "np.argmax(model.predict(xx),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.95026352, 11.3495938 , 15.20362192],\n",
       "        [ 1.92244136,  8.75392854, 11.44907751],\n",
       "        [ 1.84453147, 11.15166778, 14.01059662],\n",
       "        ...,\n",
       "        [ 1.57083227,  8.42761109,  8.68090376],\n",
       "        [ 1.56725252,  7.11881712,  7.14707477],\n",
       "        [ 1.556636  ,  7.53343428,  7.58273817]],\n",
       "\n",
       "       [[ 1.92244136,  8.75392854, 11.44907751],\n",
       "        [ 1.84453147, 11.15166778, 14.01059662],\n",
       "        [ 1.85883735,  6.66826689,  8.80521819],\n",
       "        ...,\n",
       "        [ 1.56725252,  7.11881712,  7.14707477],\n",
       "        [ 1.556636  ,  7.53343428,  7.58273817],\n",
       "        [ 1.55327102,  6.94335831,  6.62535627]],\n",
       "\n",
       "       [[ 1.84453147, 11.15166778, 14.01059662],\n",
       "        [ 1.85883735,  6.66826689,  8.80521819],\n",
       "        [ 1.8770085 ,  6.59070499,  8.85887366],\n",
       "        ...,\n",
       "        [ 1.556636  ,  7.53343428,  7.58273817],\n",
       "        [ 1.55327102,  6.94335831,  6.62535627],\n",
       "        [ 1.53160664,  6.63066435,  6.24389029]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.91800832,  6.56171404,  9.19654414],\n",
       "        [ 1.91942154,  6.29965292,  8.96718239],\n",
       "        [ 1.938702  ,  8.49711388, 11.55376519],\n",
       "        ...,\n",
       "        [ 1.56262559,  7.94204711,  7.69158994],\n",
       "        [ 1.5590475 ,  6.49420967,  6.46580236],\n",
       "        [ 1.52018037,  8.32820087,  7.72247774]],\n",
       "\n",
       "       [[ 1.91942154,  6.29965292,  8.96718239],\n",
       "        [ 1.938702  ,  8.49711388, 11.55376519],\n",
       "        [ 1.92810952,  7.33609853,  9.96306498],\n",
       "        ...,\n",
       "        [ 1.5590475 ,  6.49420967,  6.46580236],\n",
       "        [ 1.52018037,  8.32820087,  7.72247774],\n",
       "        [ 1.51801574,  5.84202169,  5.5692274 ]],\n",
       "\n",
       "       [[ 1.938702  ,  8.49711388, 11.55376519],\n",
       "        [ 1.92810952,  7.33609853,  9.96306498],\n",
       "        [ 1.93797335,  6.51675996,  9.05712559],\n",
       "        ...,\n",
       "        [ 1.52018037,  8.32820087,  7.72247774],\n",
       "        [ 1.51801574,  5.84202169,  5.5692274 ],\n",
       "        [ 1.51734926,  5.37853359,  5.26380367]]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 2.09771845, 11.0022761 , 15.84030392],\n",
       "        [ 2.09114247, 10.58728632, 14.82902633],\n",
       "        [ 2.0969308 ,  9.68628021, 13.29251755],\n",
       "        ...,\n",
       "        [ 1.56461218,  9.21610509, 10.15978727],\n",
       "        [ 1.52922219,  7.75706671,  8.59493856],\n",
       "        [ 1.53421362,  6.96184024,  7.3160305 ]],\n",
       "\n",
       "       [[ 2.09114247, 10.58728632, 14.82902633],\n",
       "        [ 2.0969308 ,  9.68628021, 13.29251755],\n",
       "        [ 2.07622857, 10.26601178, 14.69656275],\n",
       "        ...,\n",
       "        [ 1.52922219,  7.75706671,  8.59493856],\n",
       "        [ 1.53421362,  6.96184024,  7.3160305 ],\n",
       "        [ 1.53451088,  7.04763742,  7.08679076]],\n",
       "\n",
       "       [[ 2.0969308 ,  9.68628021, 13.29251755],\n",
       "        [ 2.07622857, 10.26601178, 14.69656275],\n",
       "        [ 2.05560673,  8.10617701, 11.56920766],\n",
       "        ...,\n",
       "        [ 1.53421362,  6.96184024,  7.3160305 ],\n",
       "        [ 1.53451088,  7.04763742,  7.08679076],\n",
       "        [ 1.53159208,  6.91293925,  6.86976996]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.04032633,  9.05222006, 12.16635837],\n",
       "        [ 2.05530499,  9.46773739, 12.98610448],\n",
       "        [ 1.98644411, 12.20278298, 16.41645955],\n",
       "        ...,\n",
       "        [ 1.53391194,  7.85305187,  7.97576936],\n",
       "        [ 1.53594751,  8.73833264,  9.28154709],\n",
       "        [ 1.57734183,  9.78756398, 10.7031135 ]],\n",
       "\n",
       "       [[ 2.05530499,  9.46773739, 12.98610448],\n",
       "        [ 1.98644411, 12.20278298, 16.41645955],\n",
       "        [ 1.95026352, 11.3495938 , 15.20362192],\n",
       "        ...,\n",
       "        [ 1.53594751,  8.73833264,  9.28154709],\n",
       "        [ 1.57734183,  9.78756398, 10.7031135 ],\n",
       "        [ 1.57083227,  8.42761109,  8.68090376]],\n",
       "\n",
       "       [[ 1.98644411, 12.20278298, 16.41645955],\n",
       "        [ 1.95026352, 11.3495938 , 15.20362192],\n",
       "        [ 1.92244136,  8.75392854, 11.44907751],\n",
       "        ...,\n",
       "        [ 1.57734183,  9.78756398, 10.7031135 ],\n",
       "        [ 1.57083227,  8.42761109,  8.68090376],\n",
       "        [ 1.56725252,  7.11881712,  7.14707477]]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(np.array(abc)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model_all.predict(np.array(abc)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = accuracy_score(np.argmax(y_test,axis=1), np.argmax(model.predict(x_test),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "c  = classification_report(np.argmax(y_test,axis=1), np.argmax(model.predict(x_test),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.85      0.85      1437\n",
      "          1       0.86      0.86      0.86      1524\n",
      "\n",
      "avg / total       0.86      0.86      0.86      2961\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_all1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_all = load_model('model_all.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
